{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-20T17:17:25.168179Z",
     "start_time": "2019-06-20T17:17:24.986130Z"
    },
    "colab_type": "text",
    "id": "Hjp__3bRh42K",
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "# Summer 2020 CX4641/CS7641 Homework 4\n",
    "\n",
    "## Instructor: Dr. Mahdi Roozbahani\n",
    "\n",
    "## Deadline: July 20th, 11:59 pm\n",
    "\n",
    "* No unapproved extension of the deadline is allowed. Late submission will lead to 0 credit. \n",
    "\n",
    "* Discussion is encouraged on Piazza as part of the Q/A. However, all assignments should be done individually.\n",
    "\n",
    "* You are allowed to resubmit your homework until July 26th 11:59 PM without any penalty.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wSJJRXYL2yOY"
   },
   "source": [
    "## Instructions for the assignment\n",
    "\n",
    "- In this assignment, we have programming and writing questions.\n",
    "- To switch between cell for code and for markdown, see the menu -> Cell -> Cell Type\n",
    "- You could directly type the Latex equations in the markdown cell.\n",
    "- Typing with Latex\\markdown is required for all the written questions. Handwritten answers would not be accepted. \n",
    "- If a question requires a picture, you could use this syntax $\"<img src=\"\" style=\"width: 300px;\"/>\"$ to include them within your ipython notebook.\n",
    "- Questions marked with <span style=\"color:blue\">**[P]**</span> are programming only and should be submitted to the autograder. Questions marked with <span style=\"color:green\">**[W]**</span> may required that you code a small function or generate plots, but should **NOT** be submitted to the autograder. It should be submitted on the writing portion of the assignment on gradescope\n",
    "- The outline of the assignment is as follows:\n",
    "    * Q1 [25 pts] > Decision Tree Utilities </span>| <span style=\"color:blue\">**[P]**</span>\n",
    "    * Q2 [20 pts] > Decision Tree </span>| <span style=\"color:blue\">**[P]**</span>\n",
    "    * Q3 [10 pts (bonus for Undergrad)] > Pruning </span>| <span style=\"color:blue\">**[P]**</span>\n",
    "    * Q4 [20 pts] > Random Forest </span>| <span style=\"color:blue\">**[P]** </span>\n",
    "    * Q5 [35 pts] > SVM <span style=\"color:green\">**[W]** items </span>\n",
    "    * Q6 [Bonus for all][30 pts] > Neural Network <span style=\"color:blue\">**[P]**</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nN-9x-Qm2yOY"
   },
   "source": [
    "## Using the autograder\n",
    "\n",
    "- You will find two assignments on Gradescope that correspond to HW4: \"HW4 - Programming\" and \"HW4 - Non-programming\".\n",
    "- You will submit your code for the autograder on \"HW4 - Programming\" in the following format:\n",
    "\n",
    "    * util.py\n",
    "    * decision_tree.py\n",
    "    * random_forest.py\n",
    "    * NN.py\n",
    "\n",
    "- All you will have to do is to copy your implementations of the classes \"DecisionTree\", \"RandomForest\", \"dlnet\" onto the respective files. We provided you different .py files and we added libraries in those files please DO NOT remove those lines and add your code after those lines. Note that these are the only allowed libraries that you can use for the homework.\n",
    "\n",
    "- You are allowed to make as many submissions until the deadline as you like. Additionally, note that the autograder tests each function separately, therefore it can serve as a useful tool to help you debug your code if you are not sure of what part of your implementation might have an issue. However, we encourage you to first debug locally since you will be able to solve most issues in the jupyter notebook.\n",
    "\n",
    "- **For the \"HW4 - Non-programming\" part, you will download your jupyter notbook as HTML, print it as a PDF from your browser and submit it on Gradescope. To download the notebook as html, click on \"File\" on the top left corner of this page and select \"Download as > HTML\". The non-programming part corresponds to Q5.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7U0WVt07tGRv"
   },
   "source": [
    "# Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "o0Ui6T2as9iI"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "from scipy import stats\n",
    "from math import log2, sqrt\n",
    "import pandas as pd\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HdkkppO-pNEn"
   },
   "source": [
    "# Part 1: Utility Functions (25 pts)\n",
    "\n",
    "### Part 1.1: Evaluation Utility Functions\n",
    "\n",
    "Here, we ask you to develop a few functions that will be the main building blocks of your decision tree and random forest algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7e_A6GBe11nA"
   },
   "source": [
    "### Entropy and information gain [10pts]\n",
    "\n",
    "First, we define and implement a function that computes entropy of the data.\\\n",
    "Then use this entropy function to compute the information gain for the partitioned data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jgbtf68v2yOe"
   },
   "source": [
    "## Part 1.2: Splitting Utility Functions\n",
    "\n",
    "Building a decision tree requires us to evaluate the best feature and value to split a node on. Now we will implement functions that help us determine these splits for the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rp1CKXFW2yOf"
   },
   "source": [
    "#### (1) partition_classes: [5pts]\n",
    "\n",
    "One of the basic operations is to split a tree on one attribute (features) with a specific value for that attribute.\n",
    "\n",
    "In partition_classes(), we split the data (X) and labels (y) based on the split feature and value - BINARY SPLIT.\n",
    "\n",
    "You will have to first check if the split attribute is numerical or categorical. If the split attribute is numeric, split_val should be a numerical value. For example, your split_val should go over all the values of attributes. If the split attribute is categorical, split_val should include all the categories one by one.   \n",
    "    \n",
    "You can perform the partition in the following way:\n",
    "   - Numeric Split Attribute:\n",
    "   \n",
    "       Split the data X into two lists(X_left and X_right) where the first list has all\n",
    "       the rows where the split attribute is less than or equal to the split value, and the \n",
    "       second list has all the rows where the split attribute is greater than the split \n",
    "       value. Also create two lists(y_left and y_right) with the corresponding y labels.\n",
    "    \n",
    "   - Categorical Split Attribute:\n",
    "   \n",
    "       Split the data X into two lists(X_left and X_right) where the first list has all \n",
    "       the rows where the split attribute is equal to the split value, and the second list\n",
    "       has all the rows where the split attribute is not equal to the split value.\n",
    "       Also create two lists(y_left and y_right) with the corresponding y labels.\n",
    "\n",
    "\n",
    "Hint: You could find out if the feature is categorical by checking if it is the instance of 'str'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "b70Z8KRR2yOf"
   },
   "source": [
    "#### (2) find_best_split [5pts]\n",
    "\n",
    "Given the data and labels, we need to find the order of splitting features, which is also the importance of the feature. For each attribute (feature), we need to calculate its optimal split value along with the corresponding information gain and then compare with all the features to find the optimal attribute to split.\n",
    "\n",
    "First, we specify an attribute. After computing the corresponding information gain of each value at this attribute list, we can get the optimal split value, which has the maximum information gain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9t8lY7dZ2yOf"
   },
   "source": [
    "#### (3)  find_best_feature [5pts]\n",
    "\n",
    "Based on the above functions, we can find the most important feature that we will split first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from math import log2, sqrt\n",
    "\n",
    "def entropy(class_y):\n",
    "    \"\"\" \n",
    "    Input: \n",
    "        - class_y: list of class labels (0's and 1's)\n",
    "\n",
    "    TODO: Compute the entropy for a list of classes\n",
    "    Example: entropy([0,0,0,1,1,1,1,1]) = 0.9544\n",
    "    \"\"\"\n",
    "    if np.count_nonzero(class_y) == 0 or np.count_nonzero(class_y) == len(class_y):\n",
    "        return 0\n",
    "    else:\n",
    "        num_ones = np.count_nonzero(class_y)\n",
    "        num_zeros = len(class_y)-num_ones\n",
    "        prob_ones = num_ones/(num_ones+num_zeros)\n",
    "        prob_zeros = num_zeros/(num_ones+num_zeros)\n",
    "        entropy = -prob_ones*np.log2(prob_ones)-prob_zeros*np.log2(prob_zeros)\n",
    "        return entropy\n",
    "\n",
    "def information_gain(previous_y, current_y):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "        - previous_y : the distribution of original labels (0's and 1's)\n",
    "        - current_y  : the distribution of labels after splitting based on a particular\n",
    "                     split attribute and split value\n",
    "    \n",
    "    TODO: Compute and return the information gain from partitioning the previous_y labels into the current_y labels.\n",
    "    \n",
    "    Reference: http://www.cs.cmu.edu/afs/cs.cmu.edu/academic/class/15381-s06/www/DTs.pdf \n",
    "\n",
    "    Example: previous_y = [0,0,0,1,1,1], current_y = [[0,0], [1,1,1,0]], info_gain = 0.4591\n",
    "    \"\"\" \n",
    "    prev_entropy = entropy(previous_y)\n",
    "    curr_entropy = 0\n",
    "\n",
    "    for elem in current_y:\n",
    "        curr_entropy += entropy(elem)*len(elem)/len(previous_y)\n",
    "    info_gain = prev_entropy - curr_entropy\n",
    "    return info_gain\n",
    "    \n",
    "\n",
    "def partition_classes(X, y, split_attribute, split_val):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "    - X               : (N,D) list containing all data attributes\n",
    "    - y               : a list of labels\n",
    "    - split_attribute : column index of the attribute to split on\n",
    "    - split_val       : either a numerical or categorical value to divide the split_attribute\n",
    "    \n",
    "    TODO: Partition the data(X) and labels(y) based on the split value - BINARY SPLIT.\n",
    "    \n",
    "    Example:\n",
    "    \n",
    "    X = [[3, 'aa', 10],                 y = [1,\n",
    "         [1, 'bb', 22],                      1,\n",
    "         [2, 'cc', 28],                      0,\n",
    "         [5, 'bb', 32],                      0,\n",
    "         [4, 'cc', 32]]                      1]\n",
    "    \n",
    "    Here, columns 0 and 2 represent numeric attributes, while column 1 is a categorical attribute.\n",
    "    \n",
    "    Consider the case where we call the function with split_attribute = 0 (the index of attribute) and split_val = 3 (the value of attribute).\n",
    "    Then we divide X into two lists - X_left, where column 0 is <= 3 and X_right, where column 0 is > 3.\n",
    "    \n",
    "    X_left = [[3, 'aa', 10],                 y_left = [1,\n",
    "              [1, 'bb', 22],                           1,\n",
    "              [2, 'cc', 28]]                           0]\n",
    "              \n",
    "    X_right = [[5, 'bb', 32],                y_right = [0,\n",
    "               [4, 'cc', 32]]                           1]\n",
    "\n",
    "    Consider another case where we call the function with split_attribute = 1 and split_val = 'bb'\n",
    "    Then we divide X into two lists, one where column 1 is 'bb', and the other where it is not 'bb'.\n",
    "        \n",
    "    X_left = [[1, 'bb', 22],                 y_left = [1,\n",
    "              [5, 'bb', 32]]                           0]\n",
    "              \n",
    "    X_right = [[3, 'aa', 10],                y_right = [1,\n",
    "               [2, 'cc', 28],                           0,\n",
    "               [4, 'cc', 32]]                           1]\n",
    "               \n",
    "               \n",
    "    Return in this order: X_left, X_right, y_left, y_right       \n",
    "    \"\"\"\n",
    "    \n",
    "    X = np.array(X, dtype = object)\n",
    "    y = np.array(y)\n",
    "    \n",
    "    split_col = X[:, split_attribute]\n",
    "    if type(split_val) == int or type(split_val) == float:\n",
    "        X_left = X[split_col <= split_val]\n",
    "        y_left = y[split_col <= split_val]\n",
    "        \n",
    "        X_right = X[split_col > split_val]        \n",
    "        y_right = y[split_col > split_val]       \n",
    "        \n",
    "    else:\n",
    "        X_left = X[split_col == split_val]\n",
    "        y_left = y[split_col == split_val]\n",
    "        \n",
    "        X_right = X[split_col != split_val]        \n",
    "        y_right = y[split_col != split_val]       \n",
    "    \n",
    "    return X_left, X_right, y_left, y_right\n",
    "\n",
    "def find_best_split(X, y, split_attribute):\n",
    "    \"\"\"Inputs:\n",
    "        - X               : (N,D) list containing all data attributes\n",
    "        - y               : a list array of labels\n",
    "        - split_attribute : Column of X on which to split\n",
    "    \n",
    "    TODO: Compute and return the optimal split value for a given attribute, along with the corresponding information gain\n",
    "    \n",
    "    Note: You will need the functions information_gain and partition_classes to write this function\n",
    "    \n",
    "    Example:\n",
    "    \n",
    "        X = [[3, 'aa', 10],                 y = [1,\n",
    "             [1, 'bb', 22],                      1,\n",
    "             [2, 'cc', 28],                      0,\n",
    "             [5, 'bb', 32],                      0,\n",
    "             [4, 'cc', 32]]                      1]\n",
    "    \n",
    "        split_attribute = 0\n",
    "        \n",
    "        Starting entropy: 0.971\n",
    "        \n",
    "        Calculate information gain at splits:\n",
    "           split_val = 1  -->  info_gain = 0.17\n",
    "           split_val = 2  -->  info_gain = 0.01997\n",
    "           split_val = 3  -->  info_gain = 0.01997\n",
    "           split_val = 4  -->  info_gain = 0.32\n",
    "           split_val = 5  -->  info_gain = 0\n",
    "        \n",
    "       best_split_val = 4; info_gain = .32; \n",
    "    \"\"\"\n",
    "    ig = 0\n",
    "    best_split_val = 0\n",
    "    column_vals = list(set([r[split_attribute] for r in X]))   \n",
    "\n",
    "    for val in column_vals:        \n",
    "        split_y = []        \n",
    "        X_left, X_right, y_left, y_right = partition_classes(X, y, split_attribute, val)        \n",
    "        split_y.append(y_left)\n",
    "        split_y.append(y_right)\n",
    "        ig_curr = information_gain(y,split_y)\n",
    "        if ig_curr>=ig:\n",
    "            ig = ig_curr\n",
    "            best_split_val = val\n",
    "    return best_split_val,ig\n",
    "    \n",
    "def find_best_feature(X, y):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "        - X: (N,D) list containing all data attributes\n",
    "        - y : a list of labels\n",
    "    \n",
    "    TODO: Compute and return the optimal attribute to split on and optimal splitting value\n",
    "    \n",
    "    Note: If two features tie, choose one of them at random\n",
    "    \n",
    "    Example:\n",
    "    \n",
    "        X = [[3, 'aa', 10],                 y = [1,\n",
    "             [1, 'bb', 22],                      1,\n",
    "             [2, 'cc', 28],                      0,\n",
    "             [5, 'bb', 32],                      0,\n",
    "             [4, 'cc', 32]]                      1]\n",
    "    \n",
    "        split_attribute = 0\n",
    "        \n",
    "        Starting entropy: 0.971\n",
    "        \n",
    "        Calculate information gain at splits:\n",
    "           feature 0:  -->  info_gain = 0.32\n",
    "           feature 1:  -->  info_gain = 0.17\n",
    "           feature 2:  -->  info_gain = 0.4199\n",
    "        \n",
    "       best_split_feature: 2 best_split_val: 22\n",
    "    \"\"\"\n",
    "    \n",
    "    ig = 0\n",
    "    best_split_val = 0\n",
    "    best_split_feature = 0\n",
    "    best_ig = 0   \n",
    "    for split_feature in range(len(X[0])):\n",
    "        split_val,ig = find_best_split(X,y,split_feature)        \n",
    "        if ig>best_ig:\n",
    "            best_ig = ig\n",
    "            best_split_val = split_val\n",
    "            best_split_feature = split_feature\n",
    "    return best_split_feature,best_split_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[3, 'aa', 10],[1, 'bb', 22],[2, 'cc', 28],[5, 'bb', 32],[4, 'cc', 32]],dtype=object)     \n",
    "y=[1,1,0,0,1]\n",
    "partition_classes(X,y,1,'bb')\n",
    "find_best_feature(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ayv_tqnyxMb0"
   },
   "source": [
    "# Part 2: Decision Tree (20 pts)\n",
    "## Please read the following instructions carefully before you dive into coding\n",
    "\n",
    "In this part, you will implement your own ID3 decision tree class and make it work on training and test set.\n",
    "\n",
    "You may use a recursive way to construct the tree and make use of helper functions in Part1. \n",
    "\n",
    "Please keep in mind that we use information gain to find the best feature and value to split the data for ID3 tree.\n",
    "\n",
    "To save your training time, we have added a ```max_depth``` parameter to control the maximum depth of the tree. You may adjust its value to pre-prune the tree. If set to None, it has no control of depth.\n",
    "\n",
    "You need to have a stop condition for splitting. The stopping condtion is reached when one of the two following conditions are met:\n",
    "1. If all data points in that node have the same label\n",
    "2. If the current node is at the maximum depth. In this case, you may assign the mode of the labels as the class label\n",
    "\n",
    "The MyDecisionTree class should have some member variables. We highly encourage you to use a dict in Python to store the tree information. For leaves nodes, this dict may have just one element representing the class label. For non-leaves node, the list should at least store the feature and value to split, and references to its left and right child.\n",
    "\n",
    "An example of the dict that you may use for non-leaf nodes:\n",
    "\n",
    "<pre> node = {\n",
    "            'isLeaf': False,\n",
    "            'split_attribute': split_attribute,\n",
    "            'split_value': split_val,\n",
    "            'is_categorical': is_categorical,\n",
    "            'leftTree': leftTree,\n",
    "            'rightTree': rightTree\n",
    "        };\n",
    "</pre>\n",
    "\n",
    "In the above example, the leftTree and rightTree are instances of MyDecisonTree itself.\n",
    "\n",
    "### If you use different ways to represent and store the information, please include clear comments or documentations with your code. If your result is not correct, partial credits can only be awarded if we are able to understand your code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "X2Cwef24xgtT"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "from scipy import stats\n",
    "\n",
    "\n",
    "class MyDecisionTree(object):\n",
    "    def __init__(self, max_depth=10):\n",
    "        \"\"\"\n",
    "        TODO: Initializing the tree as an empty dictionary, as preferred.\n",
    "        [5 points]\n",
    "        \n",
    "        For example: self.tree = {}\n",
    "        \n",
    "        Args:\n",
    "        \n",
    "        max_depth: maximum depth of the tree including the root node.\n",
    "        \"\"\"        \n",
    "        self.tree = {}\n",
    "        self.max_depth = max_depth\n",
    "\n",
    "        \n",
    "    def fit(self, X, y, depth):\n",
    "        \"\"\"\n",
    "        TODO: Train the decision tree (self.tree) using the the sample X and labels y.\n",
    "        [10 points]\n",
    "        \n",
    "        NOTE: You will have to make use of the utility functions to train the tree.\n",
    "        One possible way of implementing the tree: Each node in self.tree could be in the form of a dictionary:\n",
    "        https://docs.python.org/2/library/stdtypes.html#mapping-types-dict\n",
    "        \n",
    "        For example, a non-leaf node with two children can have a 'left' key and  a  'right' key. \n",
    "        You can add more keys which might help in classification (eg. split attribute and split value)\n",
    "        \n",
    "        \n",
    "        While fitting a tree to the data, you will need to check to see if the node is a leaf node(\n",
    "        based on the stopping condition explained above) or not. \n",
    "        If it is not a leaf node, find the best feature and attribute split:\n",
    "        X_left, X_right, y_left, y_right, for the data to build the left and\n",
    "        the right subtrees.\n",
    "        \n",
    "        Remember for building the left subtree, pass only X_left and y_left and for the right subtree,\n",
    "        pass only X_right and y_right.\n",
    "        \n",
    "        Args:\n",
    "        \n",
    "        X: N*D matrix corresponding to the data points\n",
    "        Y: N*1 array corresponding to the labels of the data points\n",
    "        depth: depth of node of the tree        \n",
    "        \"\"\"\n",
    "        if len(y) == 0:\n",
    "            return self\n",
    "        if depth>=self.max_depth:\n",
    "            self.tree = {'isLeaf': True,'mode':stats.mode(y).mode[0]}\n",
    "            return self\n",
    "        elif np.max(y) == np.min(y):\n",
    "            self.tree = {'isLeaf': True,'mode':stats.mode(y).mode[0]}\n",
    "            return self\n",
    "\n",
    "        most_common_y = stats.mode(y).mode[0]            \n",
    "        best_split_feature,best_split_val = find_best_feature(X,y)           \n",
    "\n",
    "        if type(best_split_val) == str:\n",
    "            is_categorical = True\n",
    "        else:\n",
    "            is_categorical = False\n",
    "\n",
    "        X_left, X_right, y_left, y_right = partition_classes(X,y,best_split_feature,best_split_val)\n",
    "\n",
    "\n",
    "        self.tree = {'isLeaf': False,'split_attribute': best_split_feature,\\\n",
    "                'split_value': best_split_val,'mode': most_common_y,'is_categorical':is_categorical,\\\n",
    "                     'left':MyDecisionTree(max_depth=self.max_depth).fit(X_left,y_left,depth+1),\\\n",
    "                    'right':MyDecisionTree(max_depth=self.max_depth).fit(X_right,y_right,depth+1)}\n",
    "        return self\n",
    "\n",
    "    def predict(self, record):\n",
    "        \"\"\"\n",
    "        TODO: classify a sample in test data set using self.tree and return the predicted label\n",
    "        [5 points]\n",
    "        Args:\n",
    "        \n",
    "        record: D*1, a single data point that should be classified\n",
    "        \n",
    "        Returns: True if the predicted class label is 1, False otherwise       \n",
    "        \"\"\"       \n",
    "        \n",
    "        curr_layer = self.tree        \n",
    "        while (curr_layer['isLeaf'] == False):\n",
    "            if curr_layer['is_categorical']:\n",
    "                if record[curr_layer['split_attribute']] == curr_layer['split_value']:\n",
    "                    curr_layer = curr_layer['left'].tree\n",
    "                elif record[curr_layer['split_attribute']] != curr_layer['split_value']:\n",
    "                    curr_layer = curr_layer['right'].tree\n",
    "            elif not(curr_layer['is_categorical']):\n",
    "                if record[curr_layer['split_attribute']] <= curr_layer['split_value']:\n",
    "                    curr_layer = curr_layer['left'].tree\n",
    "                elif record[curr_layer['split_attribute']] > curr_layer['split_value']:\n",
    "                    curr_layer = curr_layer['right'].tree\n",
    "        if curr_layer['mode'] == 1:\n",
    "            return True\n",
    "        else:\n",
    "            return False                    \n",
    "\n",
    "    # helper function. You don't have to modify it\n",
    "    def DecisionTreeEvalution(self,X,y, verbose=False):\n",
    "        # Make predictions\n",
    "        # For each test sample X, use our fitting dt classifer to predict\n",
    "        y_predicted = []\n",
    "        for record in X: \n",
    "            y_predicted.append(self.predict(record))\n",
    "\n",
    "        # Comparing predicted and true labels\n",
    "        results = [prediction == truth for prediction, truth in zip(y_predicted, y)]\n",
    "\n",
    "        # Accuracy\n",
    "        accuracy = float(results.count(True)) / float(len(results))\n",
    "        if verbose:\n",
    "            print(\"accuracy: %.4f\" % accuracy)\n",
    "        return accuracy\n",
    "\n",
    "    def DecisionTreeError(self, y):\n",
    "        # helper function for calculating the error of the entire subtree if converted to a leaf with majority class label.\n",
    "        # You don't have to modify it  \n",
    "        num_ones = np.sum(y)\n",
    "        num_zeros = len(y) - num_ones\n",
    "        return 1.0 - max(num_ones, num_zeros) / float(len(y))\n",
    "    \n",
    "    #  Define the post-pruning function\n",
    "    def pruning(self, X, y):\n",
    "        \"\"\"\n",
    "        TODO:\n",
    "        1. Prune the full grown decision trees recursively in a bottom up manner.  \n",
    "        2. Classify examples in validation set.\n",
    "        3. For each node: \n",
    "        3.1 Sum errors over the entire subtree. You may want to use the helper function \"DecisionTreeEvalution\".\n",
    "        3.2 Calculate the error on same example if converted to a leaf with majority class label. \n",
    "        You may want to use the helper function \"DecisionTreeError\".\n",
    "        4. If error rate in the subtree is greater than in the single leaf, replace the whole subtree by a leaf node.\n",
    "        5. Return the pruned decision tree.\n",
    "        \"\"\"\n",
    "        if self.tree['isLeaf'] or len(y)==0:\n",
    "            return self\n",
    "        else:\n",
    "            X_left, X_right, y_left, y_right = partition_classes(X, y, self.tree['split_attribute'], self.tree['split_value'])\n",
    "            self.tree['left'] = self.tree['left'].pruning(X_left,y_left)\n",
    "            self.tree['right'] = self.tree['right'].pruning(X_right,y_right)\n",
    "            most_common_y = stats.mode(y).mode[0]            \n",
    "            if (1-self.DecisionTreeEvalution(X,y))>self.DecisionTreeError(y):\n",
    "                self.tree = {'isLeaf':'yes','mode':most_common_y}\n",
    "        return self\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D5wmr3mt2yOu"
   },
   "source": [
    "### Dataset Objective\n",
    "\n",
    "We are the founders of a new e-commerce company that uses machine learning to optimize the user experience. We are tasked with the responsibility of coming up with a method for determining the likelihood of a shopping session ending in a purchase being made. We will then use this information to adjust pricing and services to encourage more purchasing.\n",
    "\n",
    "After much deliberation amongst the team, you come to a conclusion that we can use past online shopping data to predict the future occurence of revenue sessions. \n",
    "\n",
    "Our task is to use the decision tree algorithm to predict if a shopping session ends in a purchase.\n",
    "\n",
    "You can find more information on the dataset [here](https://archive.ics.uci.edu/ml/datasets/Online+Shoppers+Purchasing+Intention+Dataset#)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PxDuP6Yq2yOv"
   },
   "source": [
    "### Loading the dataset\n",
    "\n",
    "\n",
    "The dataset that the company has collected has the following features:\n",
    "\n",
    "1. Administrative : continuous variable\n",
    "2. Administrative_Duration\t: continuous variable\n",
    "3. Informational : continuous variable\n",
    "4. Informational_Duration : continuous variable\n",
    "5. ProductRelated : continuous variable\n",
    "6. ProductRelated_Duration : continuous variable\n",
    "7. BounceRates : continuous variable\n",
    "8. ExitRates : continuous variable\n",
    "9. PageValues : continuous variable\n",
    "10. SpecialDay : continuous variable\n",
    "11. Month\t: categorical variable\n",
    "12. OperatingSystems\t: continuous variable\n",
    "13. Browser : continuous variable\n",
    "14. Region : continuous variable\n",
    "14. TrafficType : continuous variable\n",
    "14. VisitorType : categorical variable\n",
    "14. Weekend : continuous variable\n",
    "14. Revenue : target variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7jQaNp6vlIQW"
   },
   "source": [
    "### Splitting the Dataset\n",
    "\n",
    "The original dataset explained above was split into four separate datasets. You are provided only three of these. The fourth is hidden and will be used to test your implementations via the gradescope autograder.\n",
    "\n",
    "**Training Data:** For training decision tree and random forest algorithms in parts 2 and 4.\n",
    "    \n",
    "**Validation Data:** For pruning your decision tree in part 3. (optional)\n",
    "    \n",
    "**Testing Data:** For testing your decision tree and random forest algorithms in parts 2 and 4\n",
    "\n",
    "**Hidden Data:** This data will be left out and will instead be used to grade your imlementations on gradescope."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Zv8fRYkp2yOv"
   },
   "outputs": [],
   "source": [
    "# helper function. You don't have to modify it\n",
    "data_test = pd.read_csv(\"hw4_summer2020_data_test.csv\")\n",
    "data_valid = pd.read_csv(\"hw4_summer2020_data_valid.csv\")\n",
    "data_train = pd.read_csv(\"hw4_summer2020_data_train.csv\")\n",
    "data_hidden = pd.read_csv(\"hw4_summer2020_data_hidden.csv\")\n",
    "\n",
    "categorical = ['Month']\n",
    "numerical = ['Administrative','Administrative_Duration','Informational',\n",
    "             'Informational_Duration','ProductRelated','ProductRelated_Duration','BounceRates','ExitRates','PageValues','SpecialDay','Month','OperatingSystems','Browser','Region','TrafficType','VisitorType','Weekend']\n",
    " \n",
    "X_train = data_train.drop(columns = 'Revenue')\n",
    "y_train = data_train['Revenue']\n",
    "X_test = data_test.drop(columns = 'Revenue')\n",
    "y_test = data_test['Revenue']\n",
    "X_train, y_train, X_test, y_test = np.array(X_train), np.array(y_train), np.array(X_test), np.array(y_test)\n",
    "\n",
    "X_valid = data_valid.drop(columns = 'Revenue')\n",
    "y_valid = data_valid['Revenue']\n",
    "X_valid, y_valid = np.array(X_valid), np.array(y_valid)\n",
    "\n",
    "X_hidden = data_hidden.drop(columns = 'Revenue')\n",
    "y_hidden = data_hidden['Revenue']\n",
    "X_hidden, y_hidden = np.array(X_hidden), np.array(y_hidden)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IropaXmi2yOy"
   },
   "source": [
    "Let us train and evaluate the performance of our decision tree on the test set. Note that it is trivially possible to achieve 84% accuracy because of the distribution of \"revenue\" shopping sessions in the dataset. You can use the provided test set to evaluate your implementation, however, your implementation will be tested using a left out hidden test set. You will need to obtain 87% on the hidden test set to receive full credit. Change the default parameters in your MyDecisionTree class to be the ones that you would like to be used in grading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NaUQz7x52yOy",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### Initializing a decision tree.\n",
    "t1 = time.time()\n",
    "dt = MyDecisionTree(max_depth=10)\n",
    "\n",
    "# Building a tree\n",
    "print(\"fitting the decision tree\")\n",
    "dt.fit(X_train, y_train, 0)\n",
    "\n",
    "# Evaluating the decision tree\n",
    "dt.DecisionTreeEvalution(X_test,y_test, True)\n",
    "t2 = time.time()\n",
    "print(t2-t1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6FxKE3QN2yO6"
   },
   "source": [
    "---------\n",
    "# Part 3\n",
    "## Pruning [10 Pts] [Bonus for undergrads]\n",
    "\n",
    "In order to avoid overfitting, you can:\n",
    "1. Acquire more training data; \n",
    "2. Remove irrelevant attributes; \n",
    "3. Grow full tree, then post-prune; \n",
    "4. Ensemble learning. \n",
    "\n",
    "In this part, you are going to apply reduced error post-pruning to prune the fully grown tree in a bottom-up manner.\n",
    "The idea is basically about, starting at the leaves, each node is replaced with its most popular class. If the prediction accuracy is not affected then the change is kept. You may also try recursive function to apply the post-pruning. Please notice we use validation set to get the accuracy for each node during the pruning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nebZxbdI2yO6"
   },
   "source": [
    "Now, you should make use of the decision tree you trained in part1. You can use the provided test set to evaluate your implementation, however, your implementation will be tested using a left out hidden test set. You will receive full credit if your decision tree is more accurate when pruned and you achieve at least 88% accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TMCsVv7U2yO7"
   },
   "outputs": [],
   "source": [
    "# helper function. You don't have to modify it.\n",
    "# pruning the full grown decision tree using validation set \n",
    "# dt should be a decision tree object that has been fully trained\n",
    "dt_pruned=dt.pruning(X_valid, y_valid)\n",
    "\n",
    "# Evaluate the decision tree using test set \n",
    "\n",
    "dt_pruned.DecisionTreeEvalution(X_test, y_test, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8aGIzP6dtY-Y"
   },
   "source": [
    "# Part 4: Random Forests [35pts]\n",
    "\n",
    "The decision boundaries drawn by decision trees are very sharp, and fitting a decision tree of unbounded depth to a list of examples almost inevitably leads to **overfitting**. In an attempt to decrease the variance of our classifier we're going to use a technique called 'Bootstrap Aggregating' (often abbreviated 'bagging'). This stems from the idea that a collection of weak learners can learn decision boundaries as well as a strong learner.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5zP6DnH62yO9"
   },
   "source": [
    "### Part 4.1 Random Forest Implementation (30 pts)\n",
    "\n",
    "\n",
    "A Random Forest is a collection of decision trees, built as follows:\n",
    "\n",
    "1) For every tree we're going to build:\n",
    "\n",
    "    a) Subsample the examples with replacement. Note that in this question, the size of the subsample data is equal to the original dataset. \n",
    "    \n",
    "    b) From the subsamples in a), choose attributes at random to learn on in accordance with a provided attribute subsampling rate. Based on what it was mentioned in the class, we randomly pick features in each split. We use a more general approach here to make the programming part easier. Let's randomly pick some features (70% percent of features) and grow the tree based on the pre-determined randomly selected features. Therefore, there is no need to find random features in each split.\n",
    "    \n",
    "    c) Fit a decision tree to the subsample of data we've chosen to a certain depth.\n",
    "    \n",
    "Classification for a random forest is then done by taking a majority vote of the classifications yielded by each tree in the forest after it classifies an example.\n",
    "\n",
    "We will be using the Out of Bag (OOB) score to test our random forest implementations. Since we are subsampling the datapoints with replacement, each individual decision tree in the random forests has some samples that were not used to train the tree. These samples are called \"out of bag.\" We first calculate the accuracy of each decision tree as the percentage of OOB samples that are correctly classified. Then we take the average of these accuracies to obtain the random forest's OOB score.\n",
    "\n",
    "In RandomForests Class, \n",
    "1. X is assumed to be a matrix with num_training rows and num_features columns where num_training is the\n",
    "number of total records and num_features is the number of features of each record. \n",
    "\n",
    "2. y is assumed to be a vector of labels of length num_training.\n",
    "\n",
    "**NOTE:** Lookout for TODOs for the parts that needs to be implemented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6n8GGVU7tYGh"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\"\"\"\n",
    "NOTE: For graduate student, you are required to use your own decision tree MyDecisionTree() to finish random forest.\n",
    "      Undergraduate students may use the decision tree library from sklearn.\n",
    "\"\"\"\n",
    "\n",
    "class RandomForest(object):\n",
    "    def __init__(self, n_estimators=25, max_depth=6, max_features=0.98):\n",
    "        # helper function. You don't have to modify it\n",
    "        # Initialization done here\n",
    "        self.n_estimators = n_estimators\n",
    "        self.max_depth = max_depth\n",
    "        self.max_features = max_features\n",
    "        self.bootstraps_row_indices = []\n",
    "        self.feature_indices = []\n",
    "        self.out_of_bag = []\n",
    "        self.decision_trees = [MyDecisionTree(max_depth=max_depth) for i in range(n_estimators)]\n",
    "        \n",
    "    def _bootstrapping(self, num_training, num_features, random_seed = None):\n",
    "        \"\"\"\n",
    "        TODO: \n",
    "        - Randomly select a sample dataset of size num_training with replacement from the original dataset. \n",
    "        - Randomly select certain number of features (num_features denotes the total number of features in X, \n",
    "          max_features denotes the percentage of features that are used to fit each decision tree) without replacement from the total number of features.\n",
    "        \n",
    "        Return:\n",
    "        - row_idx: the row indices corresponding to the row locations of the selected samples in the original dataset.\n",
    "        - col_idx: the column indices corresponding to the column locations of the selected features in the original feature list.\n",
    "        \n",
    "        Reference: https://en.wikipedia.org/wiki/Bootstrapping_(statistics)\n",
    "        \"\"\"\n",
    "        np.random.seed(seed=random_seed)\n",
    "        all_rows_ind = np.arange(num_training)\n",
    "        row_idx = np.random.choice(all_rows_ind,size=int(1*len(all_rows_ind)),replace=True)\n",
    "        \n",
    "        all_ft_ind = np.arange(num_features)\n",
    "        col_idx = np.random.choice(all_ft_ind,size=int(self.max_features*len(all_ft_ind)),replace=False)\n",
    "        \n",
    "        return row_idx,col_idx\n",
    "            \n",
    "    def bootstrapping(self, num_training, num_features):\n",
    "        # helper function. You don't have to modify it\n",
    "        # Initializing the bootstap datasets for each tree\n",
    "        for i in range(self.n_estimators):\n",
    "            total = set(list(range(num_training)))\n",
    "            row_idx, col_idx = self._bootstrapping(num_training, num_features,random_seed=5)\n",
    "            total = total - set(row_idx)\n",
    "            self.bootstraps_row_indices.append(row_idx)\n",
    "            self.feature_indices.append(col_idx)\n",
    "            self.out_of_bag.append(total)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        TODO:\n",
    "        Train decision trees using the bootstrapped datasets.\n",
    "        Note that you need to use the row indices and column indices.\n",
    "        \"\"\"\n",
    "        self.bootstrapping(len(X),len(X[0])) \n",
    "        for i in range(self.n_estimators):\n",
    "            print(i,' estimators fitted')\n",
    "            train_subset_X = X[self.bootstraps_row_indices[i]][:,self.feature_indices[i]]\n",
    "            train_subset_y = y[self.bootstraps_row_indices[i]]\n",
    "            self.decision_trees[i].fit(train_subset_X,train_subset_y,depth=0)\n",
    "    \n",
    "    def OOB_score(self, X, y):\n",
    "        # helper function. You don't have to modify it\n",
    "        accuracy = []\n",
    "        for i in range(len(X)):\n",
    "            predictions = []\n",
    "            for t in range(self.n_estimators):\n",
    "                if i in self.out_of_bag[t]:\n",
    "                    predictions.append(self.decision_trees[t].predict(X[i][self.feature_indices[t]]))\n",
    "            if len(predictions) > 0:\n",
    "                accuracy.append(np.sum(predictions == y[i]) / float(len(predictions)))\n",
    "        return np.mean(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BlxXUpNE2yPA"
   },
   "source": [
    "### Part 4.2 Hyperparameter tuning(5pts)\n",
    "\n",
    "Change the hyperparamters below to obtain atleast a 89% accuracy on the hidden test dataset. Change the default parameters in your RandomForest class to be the ones that you would like to be used in grading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AC1-lWuct2wj",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "TODO: \n",
    "n_estimators defines how many decision trees are fitted for the random forest (at least 10). \n",
    "max_depth defines a stop condition when the tree reaches to a certain depth.\n",
    "max_features controls the percentage of features that are used to fit each decision tree.\n",
    "Tune these three parameters to achieve a better accuracy. You will need to obtain 89% on the \n",
    "hidden test set to receive full credit. You can use the provided test set to evaluate your implementation.\n",
    "The random forest fitting may take 5 - 15 minutes. We will not take running time into account when grading\n",
    "this part, however, you need to make sure that the gradescope autograder does not time out.\n",
    "\"\"\"\n",
    "n_estimators = 25\n",
    "max_depth = 6\n",
    "max_features = 0.98\n",
    "\n",
    "start = time.time()\n",
    "random_forest = RandomForest(n_estimators, max_depth, max_features)\n",
    "\n",
    "random_forest.fit(X_train, y_train) \n",
    "    \n",
    "accuracy=random_forest.OOB_score(X_test, y_test)\n",
    "print(\"accuracy: %.4f\" % accuracy)\n",
    "\n",
    "end = time.time()\n",
    "print(end-start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "s1udwVq0PVFz"
   },
   "source": [
    "# Part 5: SVM (35 Pts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fBbKBsGIcvY0"
   },
   "source": [
    "\n",
    "### 5.1 Fitting an SVM classifier by hand (20 Pts)\n",
    "\n",
    "Consider a dataset with 2 points in 1-dimensional space: $(x_1 = -2, y_1 = −1)$ and $(x_2 = 3, y_2 = 1)$. Here $x$ are the point coordinates and $y$ are the classes.\n",
    "\n",
    "Consider mapping each point to 3-dimensional space using the feature vector $\\phi(x) = [1,2x, x^2]$. (This is equivalent to using a second order polynomial kernel.) The max margin classifier has the form\n",
    "\n",
    "$$min ||\\mathbf{\\theta}||^2 s.t.$$\n",
    "\n",
    "$$y_1(\\phi(x_1)\\mathbf{\\theta} + b) ≥ 1 $$\n",
    "\n",
    "$$y_2(\\phi(x_2)\\mathbf{\\theta}+ b) ≥ 1 $$\n",
    "\n",
    "**Hint:** $\\phi(x_1)$ and $\\phi(x_2)$ are the suppport vectors. We have already given you the solution for the suppport vectors and you need to calculate back the parameters. Margin is equal to $\\frac{1}{||\\mathbf{\\theta}||}$ and full margin is equal to $\\frac{2}{||\\mathbf{\\theta}||}$.\n",
    "\n",
    "(1) Find a vector parallel to the optimal vector $\\mathbf{\\theta}$. (4pts)\n",
    "\n",
    "(2) Calculate the value of the margin achieved by this $\\mathbf{\\theta}$? (4pts)\n",
    "\n",
    "(3) Solve for $\\mathbf{\\theta}$, given that the margin is equal to $1/||\\mathbf{\\theta}||$. (4pts)\n",
    "\n",
    "(4) Solve for $b$ using your value for $\\mathbf{\\theta}$. (4pts)\n",
    "\n",
    "(5) Write down the form of the discriminant function $f(x) = \\phi(x)\\mathbf{\\theta}+b$ as an explicit function of $x$. (4pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution:\n",
    "When mapped to the 3-dimensional space, the points become:\n",
    "\n",
    "$$p_{1} \\equiv [-2,-1] \\Rightarrow \\phi(x_{1}) = [1,-4,4] $$\n",
    "$$p_{2} \\equiv [3,1] \\Rightarrow \\phi(x_{2}) = [1,6,9] $$\n",
    "\n",
    "(1) Now, a vector parallel to the optimal vector $\\mathbf{\\theta}$ is parallel to the vector joining the two points. One such vector may be $\\phi(x_{2})-\\phi(x_{1}) = (0,10,5)$.\n",
    "\n",
    "(2) The value of the margin achieved by this $\\mathbf{\\theta}$ is given by\n",
    "$$\\gamma = \\frac{1}{2}||\\phi(x_{2})-\\phi(x_{1})|| = \\frac{5\\sqrt{5}}{2}$$\n",
    "\n",
    "(3) If the optimal vector is given by $[\\theta_{1},\\theta_{2},\\theta_{3}]$,\n",
    "$$\\theta_{1}^{2}+\\theta_{2}^{2}+\\theta_{3}^{2} = \\frac{1}{\\gamma} = \\frac{2}{5\\sqrt{5}}$$\n",
    "Given that this vector is parallel to the vector in (1), we get the following constraints:\n",
    "$$\\theta_{1} = 0$$\n",
    "$$\\theta_{2} = 2\\theta_{3}$$\n",
    "\n",
    "Plugging the constraints into the above equation, we get\n",
    "$$5\\theta_{3}^{2} = \\frac{4}{125} \\Rightarrow \\theta_{3} = 0.08,\\theta_{2} = 0.16$$\n",
    "So, we get the optimal vector set as\n",
    "$$[\\theta_{1},\\theta_{2},\\theta_{3}] \\equiv [0,0.16,0.08]$$\n",
    "\n",
    "(4) The constraint of the maximum margin classifier is given by\n",
    "$$y_{1}(\\phi(x_{1})\\mathbf{\\theta}+ b) \\geq 1$$\n",
    "$$y_{2}(\\phi(x_{2})\\mathbf{\\theta}+ b) \\geq 1$$\n",
    "\n",
    "Plugging in the results into the above inequalities, we get\n",
    "$$-1 \\times ([0,0.16,0.08] \\cdot [1,-4,4] + b) \\geq 1$$\n",
    "$$\\Rightarrow 0.32-b \\geq 1 $$\n",
    "$$1 \\times ([0,0.16,0.08] \\cdot [1,6,9] + b) \\geq 1$$\n",
    "$$\\Rightarrow 1.68+b \\geq 1$$\n",
    "Solving the above set of inequalities, we find that $b=\\frac{-17}{25}$.\n",
    "\n",
    "(5) Expressing the function in the required form, we get\n",
    "$$f(x)=[1,2x,x^{2}]\\cdot[0,0.16,0.08]-\\frac{17}{25}$$\n",
    "$$\\Rightarrow f(x)=0.32x+0.08x^{2}-\\frac{17}{25}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7dRN4oiDcvY1"
   },
   "source": [
    "### 5.2 SVM Kernel (15 Pts)\n",
    "\n",
    "(1) (5 points) We know that SVM can be used to classify linearly inseparable data by transforming it to a different feature space with a kernel $K(x, z) = \\phi(x)^T \\phi(z)$, where $\\phi(x)$ is a feature mapping. Let $K_1$ and $K_2$ be $R^n \\times R^n$ kernels, $c \\in R^+$ be a positive constant., and $\\phi_1,\\phi_2 : R^n → R^d$ be the respective feature mappings of $K_1$ and $K_2$. Explain how to use $\\phi_1,\\phi_2$ to obtain the following kernels:   \n",
    "a. $K(x, z) = cK_1(x, z)$   \n",
    "b. $K(x, z) = K_1(x, z)K_2(x, z)$  \n",
    "\n",
    "(2) (10 points)  \n",
    "a. Consider the polynomial kernel $$K(x,z) = (x^T z + 1)^d$$ with d=2. Let $x,z \\in R$ for simplicity. Define one calculation as one multiplication, addition or square operation. Assume that constants (like $\\sqrt{2}$) are already calculated and given. What is the number of calculations required to find $K(x,z)$ through direct computation?  \n",
    "b. Can you find the corresponding feature mapping $\\phi(x)$?  \n",
    "c. What is the number of calculations required for calculating the above feature map for a scalar $x$ ?  \n",
    "d. What is the number of calculations to find $K(x,z)$ using $\\phi(x)^T \\phi(z)$? Comment on this with respect to your answer in (a).  \n",
    "e. Consider the Radial Basis Kernel $$K(x,z) = exp ( - \\frac{||x - z||^2} {2\\sigma^2})$$. Is it possible to find a feature map in this case? Do you think it's necessary that an explicit feature map exists with all kernels?  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "clsJgbttmnE8"
   },
   "source": [
    "### Solution:\n",
    "(1) \n",
    "a. The kernel when multiplied by the constant as $K(x,z)=cK_1(x, z)$ is given by the expression $\\phi(x)=\\sqrt{c}\\phi_{1}(x)$.\n",
    "\n",
    "b. The kernel of the product $K_{1}(x,z)K_{2}(x,z)$ is given by the expression $\\phi(x)=\\phi_{1}(x)\\phi_{2}(x)$.\n",
    "\n",
    "(2)\n",
    "a. The kernel provided may be expressed as $K(x,z) = (x^{T}z+1)^{2}$. In the two-dimensional space, the kernel may be expressed as\n",
    "$$K(x,z) = (x_{1}z_{1}+1)^{2} = (x_{1}z_{1})^{2}+1+2x_{1}z_{1}$$\n",
    "Clearly, the above expression has 3 calculations in it.\n",
    "\n",
    "b. The corresponding feature map is given by:\n",
    "$$\\phi(x) = [x_{1}^{2},\\sqrt{2}x_{1},1]$$\n",
    "c. Clearly, the above feature mapping requires 3 calculations.\n",
    "\n",
    "d. Since this computation requires a dot product of two 3-dimensional vectors, 9 calculations are required to find $K(x,z)$ in terms of the product of the feature mappings.\n",
    "\n",
    "e. For the RBF kernel, the feature map is infinite-dimensional (i.e. may be expressed in the form of a Tayolr series). As such, an explicit finite-dimensional feature map need not exist for all kernels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zp610VW1RoUD"
   },
   "source": [
    "# Part 6: Two Layer Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TUr8kp36Rxnf"
   },
   "source": [
    "## Perceptron\n",
    "\n",
    "![Perceptron](https://drive.google.com/uc?id=1ziVTkcz30NZ_5dY7VZnpmKkecFMu4qXe)\n",
    "\n",
    "A single perceptron can be thought of as a linear hyperplane as in SVMs followed by a non-linear function. $$u_{i} = \\phi \\left( \\sum \\limits_{j=1}^{n} w_{ij}x_{j}+b_{i} \\right) = \\phi(w_{i}^{T}x+b_{i})$$ where $w_{i} \\in R^{n}$ is the weight vector, $x \\in R^{n}$ is ONE data point with $n$ features, $b_{i} \\in R$ is the bias element, and $\\phi(.)$ is any non linear function that will be described later. \n",
    "\n",
    "\n",
    "## Fully-connected Layer\n",
    "Typically, a modern neural network contains millions of perceprons as the one shown in the previous image. Preceptrons interact in different configurations. Such as cascaded or parallel. In this part we decribe a fully connected layer configuration in a neural network which comprises multiple parallel perceptrons forming one layer. \n",
    "\n",
    "We extend the previous notation to describe a fully connected layer as follows\n",
    "$$o=Wx+b$$ where $o \\in R^{m}$ is the output vector after appling linear operations, $W \\in R^{m \\times d}$ is the weight matrix, and $b \\in R^{m}$ is the bais vector. This is followed by element wise non-linear function $u^{[l]} = \\phi(o)$.\n",
    "The whole operation can be summarized as,\n",
    "$$u^{[l]} = \\phi(Wu^{[l-1]}+b) $$\n",
    "where $u^{[l-1]}$ is the output of the previous layer as shown in figure. \n",
    "![Fully connected layer](https://drive.google.com/uc?id=1ivyiXdhLPIPzh68RsMryrh0rPKBJTUPT)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Activation Function\n",
    "There are many activation functions in the literature, but for this question we are going to use Relu and Sigmoid only.\n",
    "### Relu\n",
    "The rectified linear unit (Relu) is one of the most commonly used activation function in deep learning models. The mathematical form is $\\phi(o)=max(0,o)$ while the graph form is \n",
    "\n",
    "![Relu](https://drive.google.com/uc?id=10g4b7WTbt9Y9QQQ9EJhiqXhQvfEpiXLe)\n",
    "\n",
    "### Sigmoid\n",
    "The sigmoid function is another non-linear function with S-shaped curve. This function is useful in the case of binary classification as its output is between 0 and 1. The mathematical form of the function is $\\phi(o)=\\frac{1}{1+e^{-o}}$ while the graphical form is shown in the above figure.\n",
    "\n",
    "\n",
    "\n",
    "## Cross Entropy Loss\n",
    "An essential piece in training a neural network, is the loss function. The whole purpose of gradient decent algorithm, is to find some network parameters that minimizes the loss function. In this excercise we minimize Cross Entropy (CE) loss, that represents on an intutive level, the distance between true data distribution and estimated distribution by neural network. So during training of the neural network, we will be looking for network parameters that minimizes the distance between true and estimated distribution. The mathematical form of the CE loss is given by \n",
    "$$CE(p,q) = -\\sum\\limits_{i} p(x_{i})\\log q(x_{i}) $$\n",
    "where $p(x)$ is the true distribution and $q(x)$ is the estimated distribution. \n",
    "### Implementation details\n",
    "For binary classification problems as in this exercise, we have probability distribution of a label $y_{i}$ is given by\n",
    "\\begin{equation}\n",
    "y_{i}= \n",
    "\\begin{cases}\n",
    "&1& \\text{ with probability } p(x_{i}) \\\\\n",
    "&0& \\text{ with probability } 1- p(x_{i})\n",
    "\\end{cases}\n",
    "\\end{equation}\n",
    "A frequentist estimate of $p(x_{i})$ can be written as $p(x_{i})= \\sum\\limits_{i=1}^{N} \\frac{y_{i}}{N}$, therefore the cross entropy for binary estimation can be written as \n",
    "$$CE(y_{i},\\hat{y_{i}}) = -\\frac{1}{N}\\sum\\limits_{i=1}^{N}y_{i} \\log (\\hat{y_{i}}) +(1-y_{i}) \\log (1-\\hat{y_{i}})$$\n",
    "where $y_{i} \\in \\{ 0,1\\}$ is the true label and $\\hat{y_{i}} \\in [0,1]$ is the estimated distribution.  \n",
    "\n",
    "## Forward Propagation\n",
    "We start by intializing the weights of the fully connected layer using Xavier initialization [Xavier initialization](http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf). During training we pass all the data points throught the network layer by layer.\n",
    "\\begin{eqnarray}\n",
    "u^{[0]} &=& x\\\\\n",
    "o^{[1]}&=& W^{[1]}u^{[0]}+b^{[1]} \\\\\n",
    "u^{[1]}&=& Relu(o^{[1]}) \\\\\n",
    "o^{[2]}&=& W^{[2]}u^{[1]}+b^{[2]} \\\\\n",
    "\\hat{y}=u^{[2]}&=& Sigmoid(o^{[2]}) \\\\\n",
    "\\end{eqnarray}\n",
    "\n",
    "\n",
    "Then we get the output and compute the loss \n",
    "$$l = y^{T}. \\log(\\hat{y})$$\n",
    "\n",
    "## Backward propagation\n",
    "After the forward pass, we do back propagation to update the weights and biases in the direction of the negative gradient of the loss function. So we update the weights and biases using the following formulas\n",
    "\\begin{equation}\n",
    "W^{[2]} := W^{[2]} - lr \\times \\frac{\\partial l}{\\partial W^{[2]}} \\\\\n",
    "b^{[2]} := b^{[2]} - lr \\times \\frac{\\partial l}{\\partial b^{[2]}} \\\\\n",
    "W^{[1]} := W^{[1]} - lr \\times \\frac{\\partial l}{\\partial W^{[1]}} \\\\\n",
    "b^{[1]} := b^{[1]} - lr \\times \\frac{\\partial l}{\\partial b^{[1]}}\n",
    "\\end{equation}\n",
    "where $lr$ is the learning rate. \n",
    "\n",
    "\n",
    "\n",
    "To compute the terms $\\frac{\\partial l}{\\partial W^{[i]}}$ and $ \\frac{\\partial l}{\\partial b^{[i]}}$ we use chain rule for differentiation as follows\n",
    "\n",
    "\\begin{eqnarray}\n",
    "\\frac{\\partial l}{\\partial W^{[2]}}&=&\\frac{\\partial l}{\\partial u^{[2]}}\\frac{\\partial u^{[2]}}{\\partial o^{[2]}}\\frac{\\partial o^{[2]}}{\\partial W^{[2]}} \\\\\n",
    "\\frac{\\partial l}{\\partial b^{[2]}}&=&\\frac{\\partial l}{\\partial u^{[2]}}\\frac{\\partial u^{[2]}}{\\partial o^{[2]}}\\frac{\\partial o^{[2]}}{\\partial b^{[2]}}\n",
    "\\end{eqnarray}\n",
    "\n",
    "So $\\frac{\\partial l}{\\partial u^{[2]}}$ is the differentiation of the cross entropy function at point $u^{[2]}$, $\\frac{\\partial u^{[2]}}{\\partial o^{[2]}}$ is the differentiation of the Sigmoid function at point $o^{[2]}$, $\\frac{\\partial o^{[2]}}{\\partial W^{[2]}}$ is equal to $u^{[1]}$, and $\\frac{\\partial o^{[2]}}{\\partial b^{[2]}}$ is equal to $1$. To compute $\\frac{\\partial l}{\\partial W^{[2]}}$, we need $u^{[2]}, o^{[2]} \\& u^{[1]}$ which are calculated during forward propagation. So we need to store these values in a cache variable during the forward propagation to be able to access them during bacward propagation. Also, the functional form of the CE differentiation and Sigmoid differentiation are given by \n",
    "\\begin{eqnarray}\n",
    "\\frac{\\partial l}{\\partial u^{[2]}} &=& \\frac{-1}{N}\\left(\\frac{y}{u^{[2]}}-\\frac{1-y}{1-u^{[2]}}\\right) \\\\\n",
    "\\frac{\\partial u^{[2]}}{\\partial o^{[2]}} &=& \\frac{1}{1+e^{-o^{[2]}}} \\left(1- \\frac{1}{1+e^{-o^{[2]}}} \\right) \\\\\n",
    "\\frac{\\partial o^{[2]}}{\\partial W^{[2]}} &=& u^{[1]} \\\\\n",
    "\\frac{\\partial o^{[2]}}{\\partial b^{[2]}} &=& 1\n",
    "\\end{eqnarray}\n",
    "\n",
    "While \n",
    "\\begin{eqnarray}\n",
    "\\frac{\\partial l}{\\partial W^{[1]}}&=&\\frac{\\partial l}{\\partial u^{[2]}}\\frac{\\partial u^{[2]}}{\\partial o^{[2]}}\\frac{\\partial o^{[2]}}{\\partial u^{[1]}} \\frac{u^{[1]}}{o^{[1]}} \\frac{o^{[1]}}{W^{[1]}}  \\\\\n",
    "\\frac{\\partial l}{\\partial b^{[1]}}&=&\\frac{\\partial l}{\\partial u^{[2]}}\\frac{\\partial u^{[2]}}{\\partial o^{[2]}}\\frac{\\partial o^{[2]}}{\\partial u^{[1]}} \\frac{u^{[1]}}{o^{[1]}} \\frac{o^{[1]}}{b^{[1]}}\n",
    "\\end{eqnarray}\n",
    "\n",
    "Where \n",
    "\\begin{eqnarray}\n",
    "\\frac{\\partial o^{[2]}}{\\partial u^{[1]}} &=& W^{[1]} \\\\\n",
    "\\frac{\\partial u^{[1]}}{\\partial o^{[1]}} &=&  \n",
    "\\begin{cases}\n",
    "&0& \\text{ if } o^{[1]} \\leq 0 \\\\\n",
    "&1& \\text{ if } o^{[1]} > 0 \n",
    "\\end{cases}\\\\\n",
    "\\frac{\\partial o^{[1]}}{\\partial W^{[1]}} &=& x\\\\\n",
    "\\frac{\\partial o^{[1]}}{\\partial b^{[1]}} &=& 1\n",
    "\\end{eqnarray}\n",
    "\n",
    "Note that $\\frac{\\partial u^{[1]}}{\\partial o^{[1]}}$ is the differentiation of the Relu function at $o^{[1]}$.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1ixCKWleyk1U"
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import plot_confusion_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PoOVpBL4R2V6"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "We are going to use Breast Cancer Wisconsin (Diagnostic) Data Set provided by sklearn\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_breast_cancer.html\n",
    "to train a 2 fully connected layer neural net. We are going to buld the neural network from scratch. \n",
    "'''\n",
    "\n",
    "class dlnet:\n",
    "\n",
    "    def __init__(self, x, y, lr=0.003):\n",
    "        '''\n",
    "        This method initializes the class, its implemented for you.\n",
    "        Args:\n",
    "            x: data\n",
    "            y: labels\n",
    "            Yh: predicted labels\n",
    "            dims: dimensions of different layers\n",
    "            param: dictionary of different layers parameters\n",
    "            ch: Cache dictionary to store forward parameters that are used in backpropagation\n",
    "            loss: list to store loss values\n",
    "            lr: learning rate\n",
    "            sam: number of training samples we have\n",
    "\n",
    "        '''\n",
    "        self.X = x  # features\n",
    "        self.Y = y  # ground truth labels\n",
    "\n",
    "        self.Yh = np.zeros((1, self.Y.shape[1]))  # estimated labels\n",
    "        self.dims = [30, 15, 1]  # dimensions of different layers\n",
    "\n",
    "        self.param = {}  # dictionary for different layer variables\n",
    "        self.ch = {}  # cache variable\n",
    "        self.loss = []\n",
    "\n",
    "        self.lr = lr  # learning rate\n",
    "        self.sam = self.Y.shape[1]  # number of training samples we have\n",
    "        self._estimator_type = 'classifier'\n",
    "\n",
    "    def nInit(self):\n",
    "        '''\n",
    "        This method initializes the neural network variables, its already implemented for you.\n",
    "        Check it and relate to mathematical the description above.\n",
    "        You are going to use these variables in forward and backward propagation.\n",
    "        '''\n",
    "        np.random.seed(1)\n",
    "        self.param['W1'] = np.random.randn(self.dims[1], self.dims[0]) / np.sqrt(self.dims[0])\n",
    "        self.param['b1'] = np.zeros((self.dims[1], 1))\n",
    "        self.param['W2'] = np.random.randn(self.dims[2], self.dims[1]) / np.sqrt(self.dims[1])\n",
    "        self.param['b2'] = np.zeros((self.dims[2], 1))\n",
    "        return\n",
    "\n",
    "    def Relu(self, x):\n",
    "        '''\n",
    "        In this method you are going to implement element wise Relu.\n",
    "        Make sure that all operations here are element wise and can be applied to an input of any dimension.\n",
    "        Input: Z of any dimension\n",
    "        return: Relu(Z)\n",
    "        '''\n",
    "        relu = np.maximum(0, x)\n",
    "        return(relu)\n",
    "\n",
    "\n",
    "    def Sigmoid(self, x):\n",
    "        '''\n",
    "        In this method you are going to implement element wise Sigmoid.\n",
    "        Make sure that all operations here are element wise and can be applied to an input of any dimension.\n",
    "        Input: Z of any dimension\n",
    "        return: Sigmoid(Z)\n",
    "        '''\n",
    "        sigmoid = 1/(1+np.exp(-Z))\n",
    "        return sigmoid\n",
    "\n",
    "\n",
    "    def dRelu(self, x):\n",
    "        '''\n",
    "        In this method you are going to implement element wise differentiation of Relu.\n",
    "        Make sure that all operations here are element wise and can be applied to an input of any dimension.\n",
    "        Input: Z of any dimension\n",
    "        return: dRelu(Z)\n",
    "        '''\n",
    "        x[x<=0] = 0\n",
    "        x[x>0] = 1\n",
    "        return x\n",
    "\n",
    "    def dSigmoid(self, x):\n",
    "        '''\n",
    "        In this method you are going to implement element wise differentiation of Sigmoid.\n",
    "        Make sure that all operations here are element wise and can be applied to an input of any dimension.\n",
    "        Input: Z of any dimension\n",
    "        return: dSigmoid(Z)\n",
    "        '''\n",
    "        sigm = self.Sigmoid(x)\n",
    "        dSig = sigm*(1-sigm)\n",
    "        return dSig\n",
    "\n",
    "    def nloss(self, y, yh):\n",
    "        '''\n",
    "        In this method you are going to implement Cross Entropy loss.\n",
    "        Refer to the description above and implement the appropriate mathematical equation.\n",
    "        Input: y 1xN: ground truth labels\n",
    "               yh 1xN: neural network output after Sigmoid\n",
    "\n",
    "        return: CE 1x1: loss value\n",
    "        '''\n",
    "        #  Delete this line when you implement the function\n",
    "        nloss = -np.sum(y*np.log(yh+1e-8))/y.shape[1]\n",
    "        return nloss\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        Fill in the missing code lines, please refer to the description for more details.\n",
    "        Check nInit method and use variables from there as well as other implemeted methods.\n",
    "        Refer to the description above and implement the appropriate mathematical equations.\n",
    "        donot change the lines followed by #keep.\n",
    "        '''\n",
    "        #Todo: uncomment the following 7 lines and complete the missing code\n",
    "        #u1 =\n",
    "        #o1 =\n",
    "        #self.ch['u1'], self.ch['o1'] = u1, o1  # keep\n",
    "        #u2 =\n",
    "        #o2 =\n",
    "        #self.ch['u2'], self.ch['o2'] = u2, o2  # keep\n",
    "        #return A2  # keep\n",
    "\n",
    "        #  Delete this line when you implement the function\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "\n",
    "    def backward(self, y, yh):\n",
    "        '''\n",
    "        Fill in the missing code lines, please refer to the description for more details\n",
    "        You will need to use cache variables, some of the implemeted methods, and other variables as well\n",
    "        Refer to the description above and implement the appropriate mathematical equations.\n",
    "        donot change the lines followed by #keep.\n",
    "        '''\n",
    "        #Todo: uncomment the following 13 lines and complete the missing code\n",
    "\n",
    "        # dLoss_o2 =\n",
    "        # dLoss_u2 =\n",
    "        # dLoss_W2 =\n",
    "        # dLoss_b2 =\n",
    "        # dLoss_o1 =\n",
    "        # dLoss_u1 =\n",
    "        # dLoss_W1 =\n",
    "        # dLoss_b1 =\n",
    "        # self.param[\"W2\"] = self.param[\"W2\"] - self.lr * dLoss_W2  # keep\n",
    "        # self.param[\"b2\"] = self.param[\"b2\"] - self.lr * dLoss_b2  # keep\n",
    "        # self.param[\"W1\"] = self.param[\"W1\"] - self.lr * dLoss_W1  # keep\n",
    "        # self.param[\"b1\"] = self.param[\"b1\"] - self.lr * dLoss_b1  # keep\n",
    "        #return dLoss_W2, dLoss_b2, dLoss_W1, dLoss_b1 #keep\n",
    "\n",
    "\n",
    "        #  Delete this line when you implement the function\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "    def gradient_decent(self, x, y, iter=60000):\n",
    "        '''\n",
    "        This function is an implementation of the gradient decent algorithm,\n",
    "        Its implemented for you.\n",
    "        '''\n",
    "        self.nInit()\n",
    "        for i in range(0, iter):\n",
    "            yh = self.forward(x)\n",
    "            loss = self.nloss(y, yh)\n",
    "            dLoss_W2, dLoss_b2, dLoss_W1, dLoss_b1 = self.backward(y, yh)\n",
    "            self.loss.append(loss)\n",
    "            if i % 2000 == 0: print(\"Loss after iteration %i: %f\" % (i, loss))\n",
    "        return\n",
    "\n",
    "    def predict(self, x):\n",
    "        '''\n",
    "        This function predicts new data points\n",
    "        Its implemented for you\n",
    "        '''\n",
    "        Yh = self.forward(x)\n",
    "        return np.round(Yh).squeeze()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7hMys6QATOMa"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Training the Neural Network, you donot need to modify this cell\n",
    "We are going to use Breast Cancer Wisconsin (Diagnostic) Data Set provided by sklearn\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_breast_cancer.html\n",
    "'''\n",
    "dataset = load_breast_cancer() # load the dataset\n",
    "x, y = dataset.data, dataset.target\n",
    "x = MinMaxScaler().fit_transform(x) #normalize data\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, random_state=1) #split data\n",
    "x_train, x_test, y_train, y_test = x_train.T, x_test.T, y_train.reshape(1,-1), y_test #condition data\n",
    "\n",
    "nn = dlnet(x_train,y_train,lr=0.1) # initalize neural net class\n",
    "nn.gradient_decent(x_train, y_train, iter = 66000) #train\n",
    "\n",
    "# create figure\n",
    "fig = plt.plot(np.array(nn.loss).squeeze())\n",
    "plt.title('Training')\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cZNBjuKZQDtY"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Testing Neural Network\n",
    "'''\n",
    "y_predicted = nn.predict(x_test) # predict \n",
    "\n",
    "#plot\n",
    "print(classification_report(y_test, y_predicted, target_names=dataset.target_names))\n",
    "plot_confusion_matrix(nn, x_test, y_test, cmap=plt.cm.Blues, display_labels=dataset.target_names)  \n",
    "plt.show()  \n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [],
   "name": "HW4_Summer2020_student_v2.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
