{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summer 2020 CX4641/CS7641 Homework 2\n",
    "\n",
    "## Instructor: Dr. Mahdi Roozbahani\n",
    "\n",
    "## Deadline: June 22nd, Monday, 11:59 pm\n",
    "\n",
    "* No unapproved extension of the deadline is allowed. Late submission will lead to 0 credit. \n",
    "\n",
    "* Discussion is encouraged on Piazza as part of the Q/A. However, all assignments should be done individually.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions for the assignment\n",
    "\n",
    "- In this assignment, we have programming and writing questions.\n",
    "\n",
    "- The Q4 is bonus for both undergraduate and graduate students.\n",
    "\n",
    "- To switch between cell for code and for markdown, see the menu -> Cell -> Cell Type\n",
    "    \n",
    "- You could directly type the Latex equations in the markdown cell.\n",
    "\n",
    "- Typing with Latex\\markdown is required for all the written questions. Handwritten answers would not be accepted. \n",
    "    \n",
    "- If a question requires a picture, you could use this syntax $\"<img src=\"\" style=\"width: 300px;\"/>\"$ to include them within your ipython notebook.\n",
    "\n",
    "## Using the autograder\n",
    "\n",
    "- You will find two assignments on Gradescope that correspond to HW2: \"HW2 - Programming\" and \"HW2 - Non-programming\".\n",
    "\n",
    "- You will submit your code for the autograder on \"HW2 - Programming\" in the following format:\n",
    "\n",
    "    * kmeans.py\n",
    "    * cleandata.py\n",
    "    * gmm.py\n",
    "    * semisupervised.py\n",
    "\n",
    "- All you will have to do is to copy your implementations of the classes onto the corresponding files: \"Kmeans\" > kmeans.py, \"GMM\" > gmm.py, \"CleanData\" > cleandata.py, \"SemiSupervised\" > semisupervised.py, \"ComparePerformance\" > semisupervised.py. We provided you different .py files and we added libraries in those files please DO NOT remove those lines and add your code after those lines. Note that these are the only allowed libraries that you can use for the homework.\n",
    "\n",
    "- You are allowed to make as many submissions until the deadline as you like. Additionally, note that the autograder tests each function separately, therefore it can serve as a useful tool to help you debug your code if you are not sure of what part of your implementation might have an issue.\n",
    "\n",
    "- **For the \"HW2 - Non-programming\" part, you will download your jupyter notbook as html and submit it on Gradescope. To download the notebook as html, click on \"File\" on the top left corner of this page and select \"Download as > HTML\". The non-programming part corresponds to Q2, Q3.3 (both your response and the generated images with your implementation) and Q4.2.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lBMpzE5S68ie"
   },
   "source": [
    "## 0 Set up\n",
    "This notebook is tested under [python 3.6.8](https://www.python.org/downloads/release/python-368/), and the corresponding packages can be downloaded from [miniconda](https://docs.conda.io/en/latest/miniconda.html). You may also want to get yourself familiar with several packages:\n",
    "\n",
    "- [jupyter notebook](https://jupyter-notebook.readthedocs.io/en/stable/)\n",
    "- [numpy](https://docs.scipy.org/doc/numpy-1.15.1/user/quickstart.html)\n",
    "- [matplotlib](https://matplotlib.org/users/pyplot_tutorial.html)\n",
    "\n",
    "Please implement the functions that have \"raise NotImplementedError\", and after you finish the coding, please delete or comment \"raise NotImplementedError\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "id": "7RIMxa4i7D3G",
    "outputId": "717999b3-9ac4-4e0a-f989-aa8cb0021b1e",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Version information\n",
      "python: 3.7.4 (default, Aug  9 2019, 18:34:13) [MSC v.1915 64 bit (AMD64)]\n",
      "matplotlib: 3.1.3\n",
      "numpy: 1.18.1\n"
     ]
    }
   ],
   "source": [
    "###############################\n",
    "### DO NOT CHANGE THIS CELL ###\n",
    "###############################\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "\n",
    "%matplotlib inline  \n",
    "\n",
    "import sys\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import axes3d\n",
    "from tqdm import tqdm\n",
    "\n",
    "print('Version information')\n",
    "\n",
    "print('python: {}'.format(sys.version))\n",
    "print('matplotlib: {}'.format(matplotlib.__version__))\n",
    "print('numpy: {}'.format(np.__version__))\n",
    "\n",
    "# Set random seed so output is all same\n",
    "np.random.seed(1)\n",
    "\n",
    "# Load image\n",
    "import imageio\n",
    "\n",
    "\n",
    "import scipy.stats as st"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. KMeans Clustering [5 + 30 + 10 + 5 pts]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "KMeans is trying to solve the following optimization problem:\n",
    "\n",
    "\\begin{align}\n",
    "\\arg \\min_S \\sum_{i=1}^K \\sum_{x_j \\in S_i} ||x_j - \\mu_i||^2\n",
    "\\end{align}\n",
    "where one needs to partition the N observations into K clusters: $S = \\{S_1, S_2, \\ldots, S_K\\}$ and each cluster has $\\mu_i$ as its center.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 pairwise distance [5pts]\n",
    "\n",
    "In this section, you are asked to implement pairwise_dist function.\n",
    "\n",
    "Given $X \\in \\mathbb{R}^{N x D}$ and $Y \\in \\mathbb{R}^{M x D}$, obtain the pairwise distance matrix $dist \\in \\mathbb{R}^{N x M}$ using the euclidean distance metric, where $dist_{i, j} = ||X_i - Y_j||_2$.  \n",
    "\n",
    "DO NOT USE FOR LOOP in your implementation -- they are slow and will make your code too slow to pass our grader.  Use array broadcasting instead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 KMeans Implementation [30pts]\n",
    "\n",
    "In this section, you are asked to implement _init_centers [5pts], _update_assignment [10pts], _update_centers [10pts] and _get_loss function [5pts].\n",
    "\n",
    "For the function signature, please see the corresponding doc strings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Find the optimal number of clusters [10 pts]\n",
    "\n",
    "In this section, you are asked to implement find_optimal_num_clusters function.\n",
    "\n",
    "You will now use the elbow method to find the optimal number of clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KMeans(object):\n",
    "    \n",
    "    def __init__(self): #No need to implement\n",
    "        pass\n",
    "    \n",
    "    def pairwise_dist(self, x, y): # [5 pts]\n",
    "        np.random.seed(1)\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: N x D numpy array\n",
    "            y: M x D numpy array\n",
    "        Return:\n",
    "                dist: N x M array, where dist2[i, j] is the euclidean distance between \n",
    "                x[i, :] and y[j, :]\n",
    "                \"\"\"\n",
    "    \n",
    "        dist = np.linalg.norm(x[:,None]-y,axis=2)       \n",
    "        return dist\n",
    "\n",
    "    def _init_centers(self, points, K, **kwargs): # [5 pts]\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            points: NxD numpy array, where N is # points and D is the dimensionality\n",
    "            K: number of clusters\n",
    "            kwargs: any additional arguments you want\n",
    "        Return:\n",
    "            centers: K x D numpy array, the centers. \n",
    "        \"\"\"\n",
    "        center_indices = np.random.choice(points.shape[0],size=K, replace=False)\n",
    "        centers = points[center_indices,:]\n",
    "        return centers\n",
    "\n",
    "    def _update_assignment(self, centers, points): # [10 pts]\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            centers: KxD numpy array, where K is the number of clusters, and D is the dimension\n",
    "            points: NxD numpy array, the observations\n",
    "        Return:\n",
    "            cluster_idx: numpy array of length N, the cluster assignment for each point\n",
    "            \n",
    "        Hint: You could call pairwise_dist() function.\n",
    "        \"\"\"\n",
    "        distances = self.pairwise_dist(points,centers)\n",
    "        cluster_idx = np.argmin(distances,axis=1)        \n",
    "        return cluster_idx\n",
    "\n",
    "    def _update_centers(self, old_centers, cluster_idx, points): # [10 pts]\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            old_centers: old centers KxD numpy array, where K is the number of clusters, and D is the dimension\n",
    "            cluster_idx: numpy array of length N, the cluster assignment for each point\n",
    "            points: NxD numpy array, the observations\n",
    "        Return:\n",
    "            centers: new centers, K x D numpy array, where K is the number of clusters, and D is the dimension.\n",
    "        \"\"\"\n",
    "        centers = np.zeros(old_centers.shape)\n",
    "        for i in range(len(old_centers)):\n",
    "            centers[i] = np.mean(points[cluster_idx == i],axis=0)            \n",
    "        return centers\n",
    "\n",
    "    def _get_loss(self, centers, cluster_idx, points): # [5 pts]\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            centers: KxD numpy array, where K is the number of clusters, and D is the dimension\n",
    "            cluster_idx: numpy array of length N, the cluster assignment for each point\n",
    "            points: NxD numpy array, the observations\n",
    "        Return:\n",
    "            loss: a single float number, which is the objective function of KMeans. \n",
    "        \"\"\"\n",
    "        loss = 0\n",
    "        for i in range(len(centers)):\n",
    "            cluster_pts = points[cluster_idx == i]\n",
    "            obj_f = np.sum((cluster_pts-centers[i])**2)\n",
    "            loss += obj_f\n",
    "        return loss\n",
    "        \n",
    "    def __call__(self, points, K, max_iters=100, abs_tol=1e-16, rel_tol=1e-16, verbose=False, **kwargs):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            points: NxD numpy array, where N is # points and D is the dimensionality\n",
    "            K: number of clusters\n",
    "            max_iters: maximum number of iterations (Hint: You could change it when debugging)\n",
    "            abs_tol: convergence criteria w.r.t absolute change of loss\n",
    "            rel_tol: convergence criteria w.r.t relative change of loss\n",
    "            verbose: boolean to set whether method should print loss (Hint: helpful for debugging)\n",
    "            kwargs: any additional arguments you want\n",
    "        Return:\n",
    "            cluster assignments: Nx1 int numpy array\n",
    "            cluster centers: K x D numpy array, the centers\n",
    "            loss: final loss value of the objective function of KMeans\n",
    "        \"\"\"\n",
    "        centers = self._init_centers(points, K, **kwargs)\n",
    "        for it in range(max_iters):\n",
    "            cluster_idx = self._update_assignment(centers, points)\n",
    "            centers = self._update_centers(centers, cluster_idx, points)\n",
    "            loss = self._get_loss(centers, cluster_idx, points)\n",
    "            K = centers.shape[0]\n",
    "            if it:\n",
    "                diff = np.abs(prev_loss - loss)\n",
    "                if diff < abs_tol and diff / prev_loss < rel_tol:\n",
    "                    break\n",
    "            prev_loss = loss\n",
    "            if verbose:\n",
    "                print('iter %d, loss: %.4f' % (it, loss))\n",
    "        return cluster_idx, centers, loss\n",
    "    \n",
    "    def find_optimal_num_clusters(self, data, max_K=15): # [10 pts]\n",
    "        np.random.seed(1)\n",
    "        \"\"\"Plots loss values for different number of clusters in K-Means\n",
    "        \n",
    "        Args:\n",
    "            data: input data array\n",
    "            max_K: number of clusters\n",
    "        Return:\n",
    "            losses: a list, which includes the loss values for different number of clusters in K-Means\n",
    "            Plot loss values against number of clusters\n",
    "        \"\"\"        \n",
    "        losses = []        \n",
    "        for c in range(1,max_K+1):\n",
    "            cluster_idx, centers, loss = self.__call__(data,c)\n",
    "            losses.append(loss)\n",
    "        plt.plot(np.arange(15)+1,losses)\n",
    "        plt.xlabel('Number of clusters')\n",
    "        plt.ylabel('Loss value')\n",
    "        plt.show()\n",
    "        return losses\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 COVID19 Clustering [5 pts]\n",
    "\n",
    "In this section, we are going to use our Kmeans algorithm to cluster the COVID19 dataset. The size for the dataset is $187\\times 3$, which includes all the number of confirmed cases and deathtoll for COVID19 till May 20th, 2020. \n",
    "The three columns are:\n",
    "* Countries\n",
    "* The number of confirmed cases\n",
    "* Death toll\n",
    "\n",
    "We are going to do the clustering task for just two columns which are the number of confirmed cases and death toll. The reason we have countries in our dataset is for you to associate the names of countries to each cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Expected Answer ***\n",
      "==x==\n",
      "[[ 1.62434536 -0.61175641]\n",
      " [-0.52817175 -1.07296862]]\n",
      "==y==\n",
      "[[ 0.86540763 -2.3015387 ]\n",
      " [ 1.74481176 -0.7612069 ]\n",
      " [ 0.3190391  -0.24937038]]\n",
      "==dist==\n",
      "[[1.85239052 0.19195729 1.35467638]\n",
      " [1.85780729 2.29426447 1.18155842]]\n",
      "\n",
      "*** My Answer ***\n",
      "==x==\n",
      "[[ 1.62434536 -0.61175641]\n",
      " [-0.52817175 -1.07296862]]\n",
      "==y==\n",
      "[[ 0.86540763 -2.3015387 ]\n",
      " [ 1.74481176 -0.7612069 ]\n",
      " [ 0.3190391  -0.24937038]]\n",
      "==dist==\n",
      "[[1.85239052 0.19195729 1.35467638]\n",
      " [1.85780729 2.29426447 1.18155842]]\n"
     ]
    }
   ],
   "source": [
    "# Helper function for checking the implementation of pairwise_distance fucntion. Please DO NOT change this function\n",
    "# TEST CASE\n",
    "x = np.random.randn(2, 2)\n",
    "y = np.random.randn(3, 2)\n",
    "\n",
    "print(\"*** Expected Answer ***\")\n",
    "print(\"\"\"==x==\n",
    "[[ 1.62434536 -0.61175641]\n",
    " [-0.52817175 -1.07296862]]\n",
    "==y==\n",
    "[[ 0.86540763 -2.3015387 ]\n",
    " [ 1.74481176 -0.7612069 ]\n",
    " [ 0.3190391  -0.24937038]]\n",
    "==dist==\n",
    "[[1.85239052 0.19195729 1.35467638]\n",
    " [1.85780729 2.29426447 1.18155842]]\"\"\")\n",
    "\n",
    "\n",
    "print(\"\\n*** My Answer ***\")\n",
    "print(\"==x==\")\n",
    "print(x)\n",
    "print(\"==y==\")\n",
    "print(y)\n",
    "print(\"==dist==\")\n",
    "print(KMeans().pairwise_dist(x, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Helper function for reading the .csv file.You don't need to change this function   \n",
    "def read_file():\n",
    "    data = np.zeros((187,2))\n",
    "    countries = []\n",
    "    cnt=0\n",
    "    with open(r'covid19_confirmed_deaths_052020.csv', 'r') as f:\n",
    "        for line in f:\n",
    "            country, confirmed,death = line.split(',')\n",
    "            data[cnt,:]=[confirmed,death]\n",
    "            countries.append(country)\n",
    "            cnt+=1\n",
    "    return data, countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function for visualizing cluster results. You don't have to modify it\n",
    "# If there are more than ten countries in the cluster, we are only going to show the first 10 countries as examples.\n",
    "def visualize (cluster_idx,centers, K,name_list):\n",
    "   \n",
    "    num_list = [np.sum(np.array(cluster_idx) == i) for i in range(0,K) ]\n",
    "\n",
    "    x =list(range(len(num_list)))\n",
    "    total_width, n = 0.8, 2\n",
    "    width = total_width / n\n",
    "    plt.figure(figsize=(10,5))\n",
    "    plt.title('Visualization for '+ str(K) + ' clusters', fontdict = {'fontsize' : 18})\n",
    "    plt.bar(x, num_list, width=width, label='number',tick_label = name_list, fc = 'orchid')\n",
    "\n",
    "    plt.legend()\n",
    "    for i in range(0, K):     \n",
    "        print('{0}: Average confirmed: {1:.2f}, Average Deathtoll: {2:.2f}.'.format(name_list[i], centers[i][0], centers[i][1])) \n",
    "        data = list(np.array(countries)[np.where(cluster_idx==i)])\n",
    "        print('Total number of countries in {0}: {1}'.format(name_list[i], len(data)))\n",
    "        if len(data) > 10:\n",
    "            data = data[:10]\n",
    "        print(('{}   '*len(data)).format(*data))\n",
    "        print('\\n')\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster 1: Average confirmed: 1551853.00, Average Deathtoll: 93439.00.\n",
      "Total number of countries in Cluster 1: 1\n",
      "US   \n",
      "\n",
      "\n",
      "Cluster 2: Average confirmed: 18495.95, Average Deathtoll: 1260.88.\n",
      "Total number of countries in Cluster 2: 186\n",
      "Afghanistan   Albania   Algeria   Andorra   Angola   Antigua and Barbuda   Argentina   Armenia   Australia   Austria   \n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster 1: Average confirmed: 1551853.00, Average Deathtoll: 93439.00.\n",
      "Total number of countries in Cluster 1: 1\n",
      "US   \n",
      "\n",
      "\n",
      "Cluster 2: Average confirmed: 3126.38, Average Deathtoll: 106.68.\n",
      "Total number of countries in Cluster 2: 157\n",
      "Afghanistan   Albania   Algeria   Andorra   Angola   Antigua and Barbuda   Argentina   Armenia   Australia   Austria   \n",
      "\n",
      "\n",
      "Cluster 3: Average confirmed: 38857.69, Average Deathtoll: 2196.06.\n",
      "Total number of countries in Cluster 3: 16\n",
      "Bangladesh   Belarus   Belgium   Chile   Ecuador   Ireland   Mexico   Netherlands   Pakistan   Portugal   \n",
      "\n",
      "\n",
      "Cluster 4: Average confirmed: 110274.17, Average Deathtoll: 4776.17.\n",
      "Total number of countries in Cluster 4: 6\n",
      "Canada   China   India   Iran   Peru   Turkey   \n",
      "\n",
      "\n",
      "Cluster 5: Average confirmed: 238005.43, Average Deathtoll: 21997.29.\n",
      "Total number of countries in Cluster 5: 7\n",
      "Brazil   France   Germany   Italy   Russia   Spain   United Kingdom   \n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlYAAAFDCAYAAAD4VkCqAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAgAElEQVR4nO3dfZxcZX338c8XgkYp8pT4RMCEGhFFBA2I2CryKFYE76qFeisKSq3WKi31sSh3W6xYb7Vqi42CwUp5KKhwW2xBFNEKaADFAKUgpLCCEkAUkSCR3/3HOQvDMsluds6ys8nn/XrNa3auc51zfjPXbOab65w5m6pCkiRJg9tguguQJElaVxisJEmSOmKwkiRJ6ojBSpIkqSMGK0mSpI4YrCRJkjpisJImKcnyJBdMdx29+tWU5IIky6enIkgyP0klOWa6amjr2CDJMUmuT7IqyTp3rZkke7Sv9eunuxZpfWWw0novyb+2H0Y7raFPktyQ5M4kj3kk65sJ2vB0zJpewyFwKPAB4BvA4cBrH4md9gTLfrdlj0QNXWsD3DFJNpvuWqRhM2u6C5CGwAnAK4E3AG9fTZ8XA/OBf6qqe9q27YCZMOuxL5Ap3sd8mtCyHPj+mGX/AzwGWDXFNYxnH+DnwBtreq6M/CXgi2Pa7pyGOrqwB814L2HmPgdpShisJDgXuAl4TZK/qKpf9+nzhvb+hNGGqrr3kShuUKt5Po/k/gtYOZ01tJ4I3Nl1qEqySVXdNYGuV1TVF7rc97ooyYbAo6vqV9NdizQZHgrUeq+q7qf5n/eWwMvHLk/yOOB/Acuq6ns97f3OZ9o9yVeT/CTJyiQ/TnJOkt16+ixZ3fk97eGhJWPa3pLk3HZbv05yS5IvJJk/kec39hyrJK9fw6GpSrJH22+TJH+T5JIktyW5N8l1ST6U5LG926M5vAbwuZ7tXNAu73uOVZJZSd6V5Kr2tbo9yZeSPGtMvwfWT/KyJN9r+9+S5O+SrPE/iKPnHdHMOj6lp74lPX1emOS8JD9Pck+Sy5IcvrrXMsm2Sc5Icgfwi3EH4cH1Z/e+dmujPRz9pnY8ftnefpjkr8ZZb3S89+iz7GHn3433Hm5ftw+03W/oeT2P6dnGpkmOa98v9yZZkeSUJNuupra9kxyd5Ec0IfzVE6lFGkbOWEmNzwF/STMzdcaYZQcDj6VntqqfJNsB5wE/Af4e+CnNLMkLgGcDF0+ytqPadT8B3AHsALwR2DPJs6rq9rXc3oU8/PyiAB8Engzc1rZt1e7nTOBfaA7lvQh4J7AzsF/P9j4IvBdYDHyrbf/pOHWcTPMBeh5wPM1r9VbgoiS/W1WXj+n/UuAtwKeBE4EDaV6bn7X7X52r2+f7PmAOcGTb/iOAJAfQHKb7CfB/gbtoxvyzSbatqveN2d5vAd8E/rPd5uPHeZ6j/hx4f7PLjNC8545di5nPfwZeA1wCHEtzCO7pNIex3z/BbazRBN/D/wQ8DngFzWs5+n65ot3GpsB3gG1oxulK4Ek0Y3dJkkVV9T9jdv0RYCPgMzRB9Zop/H2SplZVefPmrQrgfJrw8OQx7RcB9wJzxrQvBy7oefynNOdc7TrOfpbQHiHrs6yAJWPaNu7Tb6+27zvXVFPbdgGwfJyaPtxu7x09bY8CNurT96/HPk+ac24KeH2f/vPbZcf0tO3Ttp0GpKd9x3YMvtVn/buB+T3tAZYBt0xwfB/2OgAb0pwDdmfvuLfP/T+B3wALx2yjgL9Zi/fVNu176200M6KHA//Rbuc8YMMJbOPVbf9/BjYYs2yDnp8fNg7A69u2PcZ7TdbiPXxM229+n2V/D9wDPHtM+1NoQtOSPrVdAzx2TP8J1eLN27DdPBQoPegEmg/aB2Zzkjwd2A04u6puW92KrZ+39wcmmd1VUVV1d1vLBu0hljnAD9r9PW/Q7Sd5I/AXwKer6uM9+/11Vd3X9pmVZPN2319ruwyy71e098dW1QOHRavqCuArwO8kmTtmnS9X1fKevkVzCPKJSX5rknU8l3Zmpapu7tn2r4G/ozld4sA+631kojuoqhuraq+q+mRVnV1VJ1TVfjSzM3vTzI6N5zXt/VHVHLru3f79ffpP1kDv4SShqfVC4MdJ5ozeaILxxTRfphjr+Hr4OVVT8vskTTWDlfSgL9LMXLyhp+2w9v7ECax/Kk3oeC9wR5Kvt+cQPWWQopLs2Z6vdHdb34r2timw+YDb3ovmMNy5NDMqY5e/JckVNDN2d7T7vaBdPMi+FwD30xymG2tZT59e1/fpO3oYdMsB6oDmcNXq6th2TPuKqurim3DHtve/N4G+C2lm5sY7vDqoQd/Dc2nGYl8efJ/23vYBntBnvf+eglqkaWGwklpVtZLmXKLt2pNmR2evRmiCx3jr31tV+9DM5PwtzWGkvwL+K8krerv2W7/fSdhJdmn3/UTg3TSzJ/vSfEDdzgC/w0m2pzl/6hrgVVW1aszyPwP+AbgF+COaALAPzeEbBtk3k7v8w2863t5k1+vq22o30TynORPoGyZ/aY81rfeQ99xavIfXVCc0gWif1dz267Pew17TDmqRpoUnr0sPdQLNSbZvALagCTTHVtWaPtQfoqq+C3wXIMnWwOXA39CcIA3NzA9JtqiqO3pWHTszAvCHNIcn96+qG0Ybk2zMADNGSR4P/BvNTNTLqqrfN9teS3PO1v69h5uSvKRP37X90P8RzQfs9rQnPfd4Rnt/A1PvR+39M/ssG62j30xZF7alGduJzEJdQ3NI7AmTmLUafY9t0WfZAuC+sY0TeA+vbrxX0MyqPq6qvraaPmtlArVIQ8UZK6lHVV1Gc4HLPwD+hOYD5HMTWbc9j2SsEZoPm94PtdHDHnuP6fvnfdYfDXRjZ1beyyR/f9vzVc6i+abWgb3nLfXZd/Xuu51Ve3efvr9s7/t9ePfz5fb+Pe15OaPb34HmBO9vV9WKCW5rEJcBNwJvSPLEnjo2ojnvrGheq0lL8rDDlEk2oAkHAP9vAps5ub3/cLtu77bGm3Xr+35LcgjNt0B72yb6Hu473m0APxnYNckr+xXThvpxrUUt0lBxxkp6uBOAT9LMqFxQVT8ap/+ov0yyL83J1zfQBJIDaL4S/+GefqfQXB5gcXty/O3A/vQ/JPQlmq+0n5NkMfBrmsMpO/Lg19zX1v+hOSH/TOCpSZ46Zvl57azIGTSHYL6a5Is0X7H/Q/rMcABX0Vym4C1JfkUza3FrVX29XwFVdV6S02lO3N48yVd48HILK2m+ETblquo3Sf6E5nX+Xvsa30UTrHcDPlhV1w64m8+kuRbad2gO/80Bfp/mxPmzePjlPfrV+a9JTgNeByxMcjbNZSaeRvM+3WEN616T5GvAH7Uh7PvATjRfILiO5jIHoyb6Hh691MFxSU6mGbNlVbWM5hIULwBOb8f4Ypr37VNoLplxKQ8eTl6TidYiDZfp/lqiN2/DdqM5xHYPzWzFa9fQbzkPvdzCHjSXD1jern8HzTWH3kjPJQXavs+j+Tr/SpqAtBjYjP6XWziI5sPo7rbvqTTfZHvI/vvV1LZdwEO/Ur+k3c/qbnu0/TYE3kPz4XsvzWUJPkxz+O4hl09o+7+UZgZoZbv8grZ9/mr6zwLeRXMC++jJ8V8GnjWmX9/122XHsJqv/ffp+5DXYcyyF9Fc+uAXbf2X0/zpmwlvYw37Pbxd7yc0AeMumrDxFsZcOmGc7WxAEzwvozkn6S6aw6gfGPMefNhlL2hC67+2z++XwFfbcRz73tiDib+H30lzmPS+seNDc923o4Efttu5qx3nzwDP6+n3+t733JjtT7gWb96G6ZaqTv+6gyRJ0nrLc6wkSZI6YrCSJEnqiMFKkiSpIwYrSZKkjhisJEmSOjIU17GaM2dOzZ8/f7rLkCRJGtell156W1WN/UPxwJAEq/nz57N06dLpLkOSJGlcSf5ndcs8FChJktQRg5UkSVJHxg1WSU5McmuSZWPa35bkmiRXJvlwT/t7klzXLttvKoqWJEkaRhM5x2oJ8Cng86MNSV4MHAjsWFX3jv618iTPoPmjqs+k+avpX0vytKr6TdeFS5KkybnvvvsYGRlh5cqV013KUJs9ezbz5s1jo402Gr9za9xgVVUXJpk/pvmPgQ9V1b1tn1vb9gOBU9v2G5JcB+wKXDThiiRJ0pQaGRlhk002Yf78+SSZ7nKGUlVx++23MzIywoIFCya83mTPsXoa8LtJLknyzSS7tO1bATf19Btp2yRJ0pBYuXIlW265paFqDZKw5ZZbrvWs3mQvtzAL2BzYDdgFOD3JtkC/Eap+G0hyBHAEwDbbbDPJMiRJ0mQYqsY3mddosjNWI8AXq/Fd4H5gTtu+dU+/ecDN/TZQVYuralFVLZo7t+81tiRJktbKHnvsMa3XxpzsjNWXgT2BC5I8DXgUcBtwNvAvST5Kc/L6QuC7XRQqSZKmxrVHXt3p9hZ+bPtOt/dIWbVqFbNmDXbt9IlcbuEUmpPPt0sykuRw4ERg2/YSDKcCh7azV1cCpwNXAf8OvNVvBEqSpLGWL1/O9ttvz5ve9Cae+cxnsu+++3LPPfc8ZMbptttuY/RP3i1ZsoSDDjqIAw44gAULFvCpT32Kj370o+y8887stttu3HHHHQ9s+wtf+AK77747O+ywA9/9bjO/c/fdd3PYYYexyy67sPPOO3PWWWc9sN1XvepVHHDAAey7774DP6+JfCvwkNUs+t+r6X8scOwgRUmSpHXftddeyymnnMJnPvMZXv3qV3PmmWeusf+yZcu4/PLLWblyJU996lM57rjjuPzyyznyyCP5/Oc/zzve8Q6gCVHf+c53uPDCCznssMNYtmwZxx57LHvuuScnnngid955J7vuuit77703ABdddBFXXHEFW2yxxcDPaSj+VqC0trqetn6kzNTpcUmaCgsWLGCnnXYC4LnPfS7Lly9fY/8Xv/jFbLLJJmyyySZsuummHHDAAQA861nP4oorrnig3yGHNHNCL3zhC/nFL37BnXfeybnnnsvZZ5/NRz7yEaD5ZuSNN94IwD777NNJqAKDlSRJmiaPfvSjH/h5ww035J577mHWrFncf//9AA+71EFv/w022OCBxxtssAGrVq16YNnYb/Mloao488wz2W677R6y7JJLLmHjjTfu5gnh3wqUJElDZP78+Vx66aUAnHHGGZPaxmmnnQbAt7/9bTbddFM23XRT9ttvPz75yU9S1VwF6vLLL++m4DEMVpIkaWgcddRRHH/88ey+++7cdtttk9rG5ptvzu67786b3/xmTjjhBACOPvpo7rvvPnbccUd22GEHjj766C7LfkBGk9t0WrRoUU3nNSc083iOlSRN3tVXX8322/vv0UT0e62SXFpVi/r1d8ZKkiSpIwYrSZKkjhisJEmSOmKwkiRpPTQM51gPu8m8RgYrSZLWM7Nnz+b22283XK1BVXH77bcze/bstVrPC4RKkrSemTdvHiMjI6xYsWK6Sxlqs2fPZt68eWu1jsFKkqT1zEYbbcSCBQumu4x1kocCJUmSOmKwkiRJ6ojBSpIkqSMGK0mSpI4YrCRJkjpisJIkSeqIwUqSJKkjBitJkqSOGKwkSZI6YrCSJEnqiMFKkiSpIwYrSZKkjowbrJKcmOTWJMv6LDsqSSWZ0z5Okk8kuS7JFUmeMxVFS5IkDaOJzFgtAV4ytjHJ1sA+wI09zfsDC9vbEcDxg5coSZI0M4wbrKrqQuCOPos+BrwTqJ62A4HPV+NiYLMkT+qkUkmSpCE3qXOskrwc+HFV/WDMoq2Am3oej7RtkiRJ67xZa7tCkscC7wP27be4T1v1aSPJETSHC9lmm23WtgxJkqShM5kZq98GFgA/SLIcmAdcluSJNDNUW/f0nQfc3G8jVbW4qhZV1aK5c+dOogxJkqThstbBqqp+WFWPr6r5VTWfJkw9p6p+ApwNvK79duBuwM+r6pZuS5YkSRpOE7ncwinARcB2SUaSHL6G7ucA1wPXAZ8B3tJJlZIkSTPAuOdYVdUh4yyf3/NzAW8dvCxJkqSZxyuvS5IkdcRgJUmS1BGDlSRJUkcMVpIkSR0xWEmSJHXEYCVJktQRg5UkSVJHDFaSJEkdMVhJkiR1xGAlSZLUEYOVJElSRwxWkiRJHTFYSZIkdcRgJUmS1BGDlSRJUkcMVpIkSR0xWEmSJHXEYCVJktQRg5UkSVJHDFaSJEkdMVhJkiR1xGAlSZLUEYOVJElSRwxWkiRJHRk3WCU5McmtSZb1tP1dkv9KckWSLyXZrGfZe5Jcl+SaJPtNVeGSJEnDZiIzVkuAl4xpOw/Yoap2BP4beA9AkmcABwPPbNf5xyQbdlatJEnSEBs3WFXVhcAdY9rOrapV7cOLgXntzwcCp1bVvVV1A3AdsGuH9UqSJA2tLs6xOgz4avvzVsBNPctG2jZJkqR13kDBKsn7gFXAyaNNfbrVatY9IsnSJEtXrFgxSBmSJElDYdLBKsmhwMuA11TVaHgaAbbu6TYPuLnf+lW1uKoWVdWiuXPnTrYMSZKkoTGpYJXkJcC7gJdX1a96Fp0NHJzk0UkWAAuB7w5epiRJ0vCbNV6HJKcAewBzkowAH6D5FuCjgfOSAFxcVW+uqiuTnA5cRXOI8K1V9ZupKl6SJGmYjBusquqQPs0nrKH/scCxgxQlSZI0E3nldUmSpI4YrCRJkjpisJIkSeqIwUqSJKkjBitJkqSOGKwkSZI6YrCSJEnqiMFKkiSpIwYrSZKkjhisJEmSOmKwkiRJ6ojBSpIkqSMGK0mSpI4YrCRJkjpisJIkSeqIwUqSJKkjBitJkqSOGKwkSZI6YrCSJEnqiMFKkiSpIwYrSZKkjhisJEmSOmKwkiRJ6ojBSpIkqSPjBqskJya5NcmynrYtkpyX5Nr2fvO2PUk+keS6JFckec5UFi9JkjRMJjJjtQR4yZi2dwPnV9VC4Pz2McD+wML2dgRwfDdlSpIkDb9xg1VVXQjcMab5QOCk9ueTgIN62j9fjYuBzZI8qatiJUmShtlkz7F6QlXdAtDeP75t3wq4qaffSNsmSZK0zuv65PX0aau+HZMjkixNsnTFihUdlyFJkvTIm2yw+unoIb72/ta2fQTYuqffPODmfhuoqsVVtaiqFs2dO3eSZUiSJA2PyQars4FD258PBc7qaX9d++3A3YCfjx4ylCRJWtfNGq9DklOAPYA5SUaADwAfAk5PcjhwI/Cqtvs5wEuB64BfAW+YgpolSZKG0rjBqqoOWc2ivfr0LeCtgxYlSZI0E3nldUmSpI4YrCRJkjpisJIkSeqIwUqSJKkjBitJkqSOGKwkSZI6YrCSJEnqiMFKkiSpIwYrSZKkjhisJEmSOmKwkiRJ6ojBSpIkqSMGK0mSpI4YrCRJkjpisJIkSeqIwUqSJKkjBitJkqSOGKwkSZI6YrCSJEnqiMFKkiSpIwYrSZKkjhisJEmSOmKwkiRJ6ojBSpIkqSMDBaskRya5MsmyJKckmZ1kQZJLklyb5LQkj+qqWEmSpGE26WCVZCvgT4FFVbUDsCFwMHAc8LGqWgj8DDi8i0IlSZKG3aCHAmcBj0kyC3gscAuwJ3BGu/wk4KAB9yFJkjQjTDpYVdWPgY8AN9IEqp8DlwJ3VtWqttsIsNWgRUqSJM0EgxwK3Bw4EFgAPBnYGNi/T9dazfpHJFmaZOmKFSsmW4YkSdLQGORQ4N7ADVW1oqruA74I7A5s1h4aBJgH3Nxv5apaXFWLqmrR3LlzByhDkiRpOAwSrG4Edkvy2CQB9gKuAr4BvLLtcyhw1mAlSpIkzQyDnGN1Cc1J6pcBP2y3tRh4F/BnSa4DtgRO6KBOSZKkoTdr/C6rV1UfAD4wpvl6YNdBtitJkjQTeeV1SZKkjhisJEmSOmKwkiRJ6ojBSpIkqSMGK0mSpI4YrCRJkjpisJIkSeqIwUqSJKkjBitJkqSOGKwkSZI6YrCSJEnqiMFKkiSpIwYrSZKkjhisJEmSOmKwkiRJ6ojBSpIkqSMGK0mSpI4YrCRJkjpisJIkSeqIwUqSJKkjBitJkqSOGKwkSZI6YrCSJEnqiMFKkiSpIwMFqySbJTkjyX8luTrJ85NskeS8JNe295t3VawkSdIwG3TG6u+Bf6+qpwPPBq4G3g2cX1ULgfPbx5IkSeu8SQerJI8DXgicAFBVv66qO4EDgZPabicBBw1apCRJ0kwwyIzVtsAK4HNJLk/y2SQbA0+oqlsA2vvHd1CnJEnS0BskWM0CngMcX1U7A3ezFof9khyRZGmSpStWrBigDEmSpOEwSLAaAUaq6pL28Rk0QeunSZ4E0N7f2m/lqlpcVYuqatHcuXMHKEOSJGk4TDpYVdVPgJuSbNc27QVcBZwNHNq2HQqcNVCFkiRJM8SsAdd/G3BykkcB1wNvoAlrpyc5HLgReNWA+5AkSZoRBgpWVfV9YFGfRXsNsl1JkqSZyCuvS5IkdcRgJUmS1BGDlSRJUkcMVpIkSR0xWEmSJHXEYCVJktQRg5UkSVJHDFaSJEkdMVhJkiR1xGAlSZLUEYOVJElSRwxWkiRJHTFYSZIkdcRgJUmS1BGDlSRJUkcMVpIkSR0xWEmSJHXEYCVJktQRg5UkSVJHDFaSJEkdMVhJkiR1xGAlSZLUEYOVJElSRwxWkiRJHRk4WCXZMMnlSb7SPl6Q5JIk1yY5LcmjBi9TkiRp+HUxY/V24Oqex8cBH6uqhcDPgMM72IckSdLQGyhYJZkH/B7w2fZxgD2BM9ouJwEHDbIPSZKkmWLQGauPA+8E7m8fbwncWVWr2scjwFYD7kOSJGlGmHSwSvIy4NaqurS3uU/XWs36RyRZmmTpihUrJluGJEnS0BhkxuoFwMuTLAdOpTkE+HFgsySz2j7zgJv7rVxVi6tqUVUtmjt37gBlSJIkDYdJB6uqek9Vzauq+cDBwNer6jXAN4BXtt0OBc4auEpJkqQZYCquY/Uu4M+SXEdzztUJU7APSZKkoTNr/C7jq6oLgAvan68Hdu1iu5IkSTOJV16XJEnqiMFKkiSpIwYrSZKkjhisJEmSOmKwkiRJ6ojBSpIkqSMGK0mSpI4YrCRJkjpisJIkSeqIwUqSJKkjBitJkqSOGKwkSZI6YrCSJEnqiMFKkiSpIwYrSZKkjhisJEmSOmKwkiRJ6ojBSpIkqSMGK0mSpI4YrCRJkjpisJIkSeqIwUqSJKkjBitJkqSOGKwkSZI6MulglWTrJN9IcnWSK5O8vW3fIsl5Sa5t7zfvrlxJkqThNciM1Srgz6tqe2A34K1JngG8Gzi/qhYC57ePJUmS1nmTDlZVdUtVXdb+fBdwNbAVcCBwUtvtJOCgQYuUJEmaCTo5xyrJfGBn4BLgCVV1CzThC3h8F/uQJEkadgMHqyS/BZwJvKOqfrEW6x2RZGmSpStWrBi0DEmSpGk3ULBKshFNqDq5qr7YNv80yZPa5U8Cbu23blUtrqpFVbVo7ty5g5QhSZI0FAb5VmCAE4Crq+qjPYvOBg5tfz4UOGvy5UmSJM0cswZY9wXAa4EfJvl+2/Ze4EPA6UkOB24EXjVYiZIkSTPDpINVVX0byGoW7zXZ7UqSJM1UXnldkiSpI4McCpSkgV175NXTXcKkLPzY9tNdgqQh5IyVJElSRwxWkiRJHTFYSZIkdcRgJUmS1BGDlSRJUkcMVpIkSR0xWEmSJHXEYCVJktQRg5UkSVJHDFaSJEkdMVhJkiR1xGAlSZLUEYOVJElSRwxWkiRJHZk13QVIkqThc+2RV093CZOy8GPbT+v+nbGSJEnqiDNWkqROzNQZDpj+WQ6tO5yxkiRJ6ojBSpIkqSMGK0mSpI4YrCRJkjpisJIkSerIlAWrJC9Jck2S65K8e6r2I0mSNCymJFgl2RD4B2B/4BnAIUmeMRX7kiRJGhZTdR2rXYHrqup6gCSnAgcCV03R/iZkpl5jxeurSJI0M0zVocCtgJt6Ho+0bZIkSeusqZqxSp+2ekiH5AjgiPbhL5NcM0W1PFLmALdNyZY/PiVb1eo5lusOx3LdMXVjCY7nI2+m/24+ZXULpipYjQBb9zyeB9zc26GqFgOLp2j/j7gkS6tq0XTXocE5lusOx3Ld4ViuW9bl8ZyqQ4HfAxYmWZDkUcDBwNlTtC9JkqShMCUzVlW1KsmfAP8BbAicWFVXTsW+JEmShsVUHQqkqs4Bzpmq7Q+hdeawphzLdYhjue5wLNct6+x4pqrG7yVJkqRx+SdtJEmSOrLeBaskT0xyapIfJbkqyTlJnpZkfpJlk9zm65M8ecC6np7koiT3JjlqkG2tL4Z4LF+T5Ir29p0kzx5ke+uLIR7PA9ux/H6SpUl+Z5DtrQ+GdSx7trVLkt8keWUX21uXDetYJtkjyc/b38vvJ3n/INvr0pSdYzWMkgT4EnBSVR3ctu0EPIGHXtB0bb0eWMaYS0qMU8usqlrV03QH8KfAQQPUsd4Y8rG8AXhRVf0syf405xI8b4Ca1nlDPp7nA2dXVSXZETgdePoANa3ThnwsR//k2nE0X67SGgz7WALfqqqXDVDHlFjfZqxeDNxXVZ8ebaiq71fVt3o7tWn6Uz2Pv9Km4w2TLEmyLMkPkxzZ/o9nEXBym5ofk+S5Sb6Z5NIk/5HkSe12LkjywSTfBN7eu8+qurWqvgfcN4XPf10yzGP5nar6WfvwYprruGnNhnk8f1kPnoy6MWMudqyHGdqxbL0NOBO4dQqe+7pm2MdyKK1XM1bADsClA6y/E7BVVe0AkGSzqrozzaUljqqqpUk2Aj4JHFhVK5L8AXAscFi7jc2q6kUD1KDGTBnLw4GvDlDn+mKoxzPJK4C/BR4P/N4Ada4PhnYsk2wFvALYE9hlgBrXF0M7lq3nJ/kBzczXUcNyWaf1LVgN6npg2ySfBP4NOLdPn+1o3oznJYHmOl639Cw/baqL1IRM+VgmeTFNsPKcnKk3peNZVV8CvpTkhcBfA3t3VLcebirH8uPAu6rqN+16mlpTOZaXAU+pql8meSnwZWBhV4UPYn0LVlcCEzlZcRUPPUw6G6A9Z+bZwH7AW4FX82CqHhXgyqp6/mq2ffdaVazVGeqxTHMuzmeB/avq9gnUub4b6vEcVR8LxlIAAAGJSURBVFUXJvntJHOqaur+bt3MNsxjuQg4tf0AnwO8NMmqqvryBOpdHw3tWFbVL3p+PifJPw7L7+X6do7V14FHJ3nTaEOab4eMnWZcDuyUZIMkWwO7tn3nABtU1ZnA0cBz2v53AZu0P18DzE3y/HadjZI8c6qe0HpsaMcyyTbAF4HXVtV/T/YJrmeGeTyfmvaTOMlzgEcBhuXVG9qxrKoFVTW/quYDZwBvMVSt0dCOZZpvK47+Xu5Kk2eG4vdyvZqxar/V8wrg40neDaykeUO8Y0zX/6T5ZtcPab65cFnbvhXwuSSjgfQ97f0S4NNJ7gGeT5PwP5FkU5rX+OM0yX+1kjwRWAo8Drg/yTuAZ/Smcj1omMcSeD+wJfCP7e/9qnX1j412ZcjH8/eB1yW5D7gH+IOek9k1xpCPpdbCkI/lK4E/TrKK5vfy4GH5vfTK65IkSR1Z3w4FSpIkTRmDlSRJUkcMVpIkSR0xWEmSJHXEYCVJktQRg5UkSVJHDFaSJEkdMVhJkiR15P8D7mFeZlQ+m0EAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Helper function for running the algorithm for K=2 and K=5. You don't have to modify it\n",
    "name_list0 = ['Cluster 1','Cluster 2']\n",
    "name_list1 = ['Cluster 1','Cluster 2','Cluster 3','Cluster 4','Cluster 5']\n",
    "\n",
    "data, countries = read_file()\n",
    "cluster_idx2, centers2, loss2 = KMeans()(data, 2)\n",
    "visualize(cluster_idx2, centers2, 2, name_list0)\n",
    "\n",
    "cluster_idx5, centers5, loss5 = KMeans()(data, 5)\n",
    "visualize(cluster_idx5, centers5, 5, name_list1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Helper function for visualizing elbow method result. You don't have to modify it\n",
    "\n",
    "KMeans().find_optimal_num_clusters(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 EM algorithm [20 pts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Performing EM Algorithm [20 pts for CS 4641; 10 points for CS 7641]\n",
    "\n",
    "EM algorithm is a widely used approach to learning in the presence of unobserved variables. Consider the general framework of the EM algorithm, given a joint distribution $P(x,z|\\theta)$ over observed variables $x$, hidden variable $z$ and its distribution $q(z)$, and governing parameter $\\theta$, the goal is to maximize the likelihbood function $P(x|\\theta)$ and given the following expression:\n",
    "    \n",
    "$$\\log P(x|\\theta) = \\log (\\sum_{z}P(x, z|\\theta)) = \\log (\\sum_{z}q(z)\\frac{P(x, z|\\theta)}{q(z)}) \\geq \\sum_{z}q(z) \\log \\frac{P(x, z|\\theta)}{q(z)} = \\sum_{z}q(z) \\log \\frac{P(z|x, \\theta)P(x|\\theta)}{q(z)}$$\n",
    "\n",
    "The inequality is based on the Jensen's Theorem.\n",
    "\n",
    "2.1.1. Please provide a brief description on how to perform the $E$ step in the above equation.\n",
    "\n",
    "2.1.2. Please provide a brief description on how to perform the $M$ step in the above equation.\n",
    "\n",
    "Suppose\n",
    "\n",
    "$$F(q,\\theta) = \\sum_{z}q(z) \\log \\frac{P(x, z|\\theta)}{q(z)}$$\n",
    "\n",
    "During the lecture, the expression $\\sum_{z}q(z) \\log \\frac{P(x, z|\\theta)}{q(z)}$ was explained through the sum of entropy and log-likelihood; the same approach can also be explained using KL-divergence. \n",
    "\n",
    "2.1.3. Please derive that from the above equation (**Hint**: use product rule for joint probability) and explain what will happen to the KL term in the $E$ step.\n",
    "\n",
    "### Answers:\n",
    "\n",
    "2.1.1\n",
    "For $i^{th}$ iteration of the E-step, we use the current values of the parameters $\\theta^{i}$ to estimate the posterior distribution of the hidden variable $z$ as $P(z|x,\\theta^{i})$. Thus, the distribution of the latent variable $z$ becomes:\n",
    "$$q^{i} = P(z|x,\\theta^{i})$$\n",
    "\n",
    "2.1.2\n",
    "For the M-step, we maximize the expected log-likelihood function by modifying the parameters, as:\n",
    "$$\\theta^{i+1} = argmax_{\\theta} \\log P(x|\\theta)$$\n",
    "\n",
    "2.1.3\n",
    "We begin with the expression of \n",
    "$$F(q,\\theta) = \\sum_{z}q(z) \\log \\left(\\frac{P(x, z|\\theta)}{q(z)}\\right)$$\n",
    "\n",
    "Applying the product rule of joint probability, we get:\n",
    "\n",
    "$$F(q,\\theta) = \\sum_{z}q(z) \\log \\left(\\frac{P(z|x,\\theta)P(x|\\theta)}{q(z)}\\right)$$\n",
    "$$ = \\sum_{z}q(z)log(P(x|\\theta))+\\sum_{z}q(z)log\\left(\\frac{P(z|x,\\theta)}{q(z)}\\right)$$\n",
    "$$\\Rightarrow F(q,\\theta) = l(\\theta) + KL[q(z)||P(z|x,\\theta)]$$\n",
    "\n",
    "During the E-step, given that we set $q^{i} = P(z|x,\\theta^{i})$, the second KL divergence term in the above equation disappears (given that the distributions are the same afterward). As such, after the E-step, we get:\n",
    "$$F(q,\\theta) = l(\\theta)$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 EM Algorithm in Coin Toss problem [10 pts for CS 7641; 10 points Bonus for CS 4641]\n",
    "\n",
    "Suppose we have a bunch of coins $C$ consisting three kinds of coins. Mathematically, it obeys a mixed Bernoulli distribution:\n",
    "$$X\\sim F = \\pi_{1} F_{1}+\\pi_{2} F_{2}+(1-\\pi_{1}-\\pi_{2}) F_{3}$$\n",
    "where $\\pi \\in [0,1]$, and $F_{1}=Ber(p_{1})$, $F_{2}=Ber(p_{2})$, $F_{3}=Ber(p_{3})$. That is to say, each coin belongs to $F_{1}$, $F_{2}$ or $F_{3}$. Here $Ber(p)$ means the coin gives 1 (head) with probability $p$ and gives 0 (tail) with probability $1-p$.\n",
    "We initialized parameters $p_{1}=\\frac{1}{2}$, $p_{2}=\\frac{5}{6}$, $p_{3}=\\frac{1}{3}$, $\\pi_{1}=\\frac{1}{4}$, $\\pi_{2}=\\frac{1}{2}$. Now, we draw 3 coins $X_{1}$, $X_{2}$, $X_{3}$ independently from $C$ and have 6 independent trials for each of them. The result shows:\n",
    "\n",
    "$$\n",
    "\\begin{array}{|c|c|c|c|}\\hline Coins & X_{1} & X_{2} & X_{3}   \\\\ \\hline Trial 1 & 0 & 1 & 1   \\\\ \\hline Trial 2 & 0 & 1 & 1   \\\\ \\hline Trial 3 & 1 & 0 & 1   \\\\  \\hline Trial 4 & 0 & 1 & 1   \\\\ \\hline Trial 5 & 1 & 0 & 1   \\\\ \\hline Trial 6 & 1 & 0 & 0   \\\\\\hline\\end{array}\n",
    "$$\n",
    "\n",
    "\n",
    "2.2.1. Use EM algorithm for one step, we update $F = F(p_{1}=\\frac{1}{2}$, $p_{2}=\\frac{5}{6}$, $p_{3}=\\frac{1}{3}$, $\\pi_{1}=\\frac{1}{4}$, $\\pi_{2}=\\frac{1}{2})$ to $F^{'}(p_{1}^{'},p_{2}^{'},p_{3}^{'},\\pi_{1}^{'},\\pi_{2}^{'})$. Write down your EM algorithm and show the value of $p_{1}^{'},p_{2}^{'},p_{3}^{'},\\pi_{1}^{'},\\pi_{2}^{'}$. (Round the answer to three decimal places.)\n",
    "\n",
    "(**Hint**: $\\theta^{new}=argmax_{\\theta}\\sum_{Z}p(Z|X,\\theta^{old})\\ln p(X,Z|\\theta)$)\n",
    "\n",
    "2.2.2. Can you explain the reason why we are getting the value of $p_{1}^{'},p_{2}^{'},p_{3}^{'},\\pi_{1}^{'},\\pi_{2}^{'}$ in 2.2.1? What will the values be if we implement more EM steps? \n",
    "\n",
    "(**Hint**: For example, why the values are increasing/ decreasing? Will the values be stable if we implement more steps? No need to calculate the real number)\n",
    "\n",
    "### Answers:\n",
    "2.2.1 First, we express the likelihoods of $X_{1}$,$X_{2}$ and $X_{3}$ as:\n",
    "$$L(X_{1}) = p^{3}(1-p)^{3}$$\n",
    "$$L(X_{2}) = p^{3}(1-p)^{3}$$\n",
    "$$L(X_{3}) = p^{5}(1-p)$$\n",
    "In the E-step, we first evaluate the responsibilities for each of the coins as:\n",
    "$$\\tau_{Fi} = \\frac{\\pi_{i}L(X_{i})}{\\sum_{j=1}^{3}\\pi_{j}L(X_{i})}$$\n",
    "So, for coin X1, we get:\n",
    "$$\\tau_{F1,X1} = \\frac{\\pi_{1}p_{1}^{3}(1-p_{1})^{3}}{\\pi_{1}p_{1}^{3}(1-p_{1})^{3}+\\pi_{2}p_{2}^{3}(1-p_{2})^{3}+\\pi_{3}p_{3}^{3}(1-p_{3})^{3}} = 0.4889$$\n",
    "\n",
    "$$\\tau_{F2,X1} = \\frac{\\pi_{2}p_{2}^{3}(1-p_{2})^{3}}{\\pi_{1}p_{1}^{3}(1-p_{1})^{3}+\\pi_{2}p_{2}^{3}(1-p_{2})^{3}+\\pi_{3}p_{3}^{3}(1-p_{3})^{3}} = 0.1677$$\n",
    "\n",
    "$$\\tau_{F3,X1} = \\frac{\\pi_{3}p_{3}^{3}(1-p_{3})^{3}}{\\pi_{1}p_{1}^{3}(1-p_{1})^{3}+\\pi_{2}p_{2}^{3}(1-p_{2})^{3}+\\pi_{3}p_{3}^{3}(1-p_{3})^{3}} = 0.3433$$\n",
    "\n",
    "For coin X2, since the likelihood expression is the same as for coin X1, we get the same answers:\n",
    "$$\\tau_{F1,X2} = 0.4889; \\tau_{F2,X2} = 0.1677; \\tau_{F3,X2} = 0.3433$$\n",
    "\n",
    "For coin X3, we get these results:\n",
    "$$\\tau_{F1,X3} = \\frac{\\pi_{1}p_{1}^{5}(1-p_{1})}{\\pi_{1}p_{1}^{5}(1-p_{1})+\\pi_{2}p_{2}^{5}(1-p_{2})+\\pi_{3}p_{3}^{5}(1-p_{3})} = 0.1026$$\n",
    "\n",
    "$$\\tau_{F2,X3} = \\frac{\\pi_{3}p_{3}^{5}(1-p_{3})}{\\pi_{1}p_{1}^{5}(1-p_{1})+\\pi_{2}p_{2}^{5}(1-p_{2})+\\pi_{3}p_{3}^{5}(1-p_{3})} = 0.8794$$\n",
    "\n",
    "$$\\tau_{F3,X3} = \\frac{\\pi_{3}p_{3}^{5}(1-p_{3})}{\\pi_{1}p_{1}^{5}(1-p_{1})+\\pi_{2}p_{2}^{5}(1-p_{2})+\\pi_{3}p_{3}^{5}(1-p_{3})} = 0.0180$$\n",
    "\n",
    "Now, we can formulate $Q(\\theta)$ as:\n",
    "\n",
    "$$Q = \\tau_{F1,X1}log(\\pi_{1}L(X_{1}))+\\tau_{F2,X1}log(\\pi_{2}L(X_{2}))+\\tau_{F3,X1}log(\\pi_{3}L(X_{3})) + \\tau_{F1,X2}log(\\pi_{1}L(X_{1}))+\\tau_{F2,X2}log(\\pi_{2}L(X_{2}))+\\tau_{F3,X2}log(\\pi_{3}L(X_{3}))+\\tau_{F1,X3}log(\\pi_{1}L(X_{1}))+\\tau_{F2,X3}log(\\pi_{2}L(X_{2}))+\\tau_{F3,X3}log(\\pi_{3}L(X_{3}))$$\n",
    "\n",
    "So, we find $p_{1}'$ by maximizing:\n",
    "$$Q_{p1} = \\tau_{F1,X1}log(\\pi_{1}L(X_{1}))+\\tau_{F2,X1}log(\\pi_{2}L(X_{2}))+\\tau_{F3,X1}log(\\pi_{3}L(X_{3}))$$\n",
    "$$ = 0.489log[\\pi_{1}(p_{1}^{3}(1-p_{1})^{3}] + 0.168log[\\pi_{2}(p_{1}^{3}(1-p_{1})^{3}] + 0.343log[\\pi_{3}(p_{1}^{5}(1-p_{1})]$$\n",
    "\n",
    "$$\\Rightarrow \\frac{\\partial Q_{p1}}{\\partial p_{1}} = \\frac{6p_{1}-3.686}{(p_{1}-1)p_{1})} = 0$$\n",
    "Solving the above, we get $p_{1}' = 0.614$.\n",
    "\n",
    "Similarly, we get the following expression to maximize for $p_{2}'$:\n",
    "$$\\tau_{F1,X2}log(\\pi_{1}L(X_{1}))+\\tau_{F2,X2}log(\\pi_{2}L(X_{2}))+\\tau_{F3,X2}log(\\pi_{3}L(X_{3}))$$\n",
    "$$ = 0.489log[\\pi_{1}(p_{2}^{3}(1-p_{2})^{3}] + 0.168log[\\pi_{2}(p_{1}^{3}(1-p_{2})^{3}] + 0.343log[\\pi_{3}(p_{2}^{5}(1-p_{2})]$$\n",
    "\n",
    "We get a similar answer (since the Q expressions are the same) as $p_{2}' = 0.614$.\n",
    "\n",
    "For $p_{3}'$, we maximize:\n",
    "$$Q_{p3} = \\tau_{F1,X3}log(\\pi_{1}L(X_{1}))+\\tau_{F2,X3}log(\\pi_{2}L(X_{2}))+\\tau_{F3,X3}log(\\pi_{3}L(X_{3}))$$\n",
    "$$ = 0.103log[\\pi_{1}(p_{3}^{3}(1-p_{3})^{3}]+0.880log[\\pi_{2}(p_{3}^{3}(1-p_{3})^{3}]+0.018log[\\pi_{3}p_{3}^{5}(1-p_{3})]$$\n",
    "\n",
    "$$\\Rightarrow \\frac{\\partial Q_{p3}}{\\partial p_{3}} = \\frac{6.006p_{3}-3.039}{(p_{3}-1)p_{3}} = 0$$\n",
    "Solving the above, we get $p_{3}' = 0.506$.\n",
    "\n",
    "Now, to solve for $\\pi_{1}$,$\\pi_{2}$ and $\\pi_{3}$ we apply the constraint that $0<\\pi_{1},\\pi_{2}<1$ and $\\pi_{3} = 1-\\pi_{1}-\\pi_{2}$\n",
    "\n",
    "So, we have to maximize\n",
    "$$Q_{\\pi} = \\tau_{F1,X1}log(\\pi_{1}) + \\tau_{F2,X1}log(\\pi_{2}) + \\tau_{F3,X1}log(1-\\pi_{1}-\\pi_{2}) + \\tau_{F1,X2}log(\\pi_{1}) + \\tau_{F2,X2}log(\\pi_{2}) + \\tau_{F2,X3}log(1-\\pi_{1}-\\pi_{2}) + \\tau_{F1,X3}log(\\pi_{1}) + \\tau_{F2,X3}log(\\pi_{2}) + \\tau_{F3,X3}log(1-\\pi_{1}-\\pi_{2})$$\n",
    "\n",
    "$$= 0.489log(\\pi_{1}) + 0.168log(\\pi_{2}) + 0.343log(1-\\pi_{1}-\\pi_{2}) + 0.489log(\\pi_{1}) + 0.168log(\\pi_{2}) + 0.343log(1-\\pi_{1}-\\pi_{2}) + 0.103log(\\pi_{1}) + 0.880log(\\pi_{2}) + 0.018log(1-\\pi_{1}-\\pi_{2})$$\n",
    "\n",
    "2.2.2.\n",
    "If we implement more EM steps, the algorithm would be expected to converge to the true mean values of $F_{1}$,$F_{2}$ and $F_{3}$. Based on the increase observed for $p_{1}$, $p_{2}$ and the decrease for $p_{3}$, the true values appear to follow these trends. Further, the $pi$ values will eventually approach $\\frac{1}{3}$, because of approximately equal weightage to each component after the iterations are complete."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4ixFqMoB-gbY"
   },
   "source": [
    "## 3. GMM implementation [40+10 pts]\n",
    "\n",
    "GMM uses MLE to optimize its parameters. It approximates the distribution of data using a set of gaussian distributions. \n",
    "\n",
    "Given $N$ samples $X = [x_1, x_2, \\ldots, x_N]$, we are asked to find $K$ diagonal gaussian distributions to model the data $X$: \n",
    "\n",
    "\\begin{align}\n",
    "    \\max_{\\{\\mu_k, \\sigma_k\\}_{k=1}^K} \\sum_{i=1}^N \\log \\big( \\sum_{k=1}^{K} \\pi(k) \\mathcal{N}(x_i | \\mu_k, \\sigma_k)\\big)\n",
    "\\end{align}\n",
    "\n",
    "- For undergaduate student: you may assume the covariance matrix is diag matrix, which means the features are independent. (i.e. the red intensity of a pixel is independent from its blue intensity, etc). \n",
    "- For graduate student: please assume full covariance matrix.\n",
    "\n",
    "**Hints**\n",
    "\n",
    "1. Here $\\pi(\\cdot)$ is the prior of the latent variable. It is also called the mixture coefficient. To make it simple, we assume $\\pi(k) = \\frac{1}{K}, \\forall k = 1, 2, \\ldots, K$.\n",
    "\n",
    "2. As we create our model, we will need to use a multivariate Gaussian since our pixels are 3-dimensional vectors corresponding to red, green, and blue color intensities.  It means that for each image, you need to convert it into a N*3 matrix, where N is the number of pixels, and 3 is the number of features.\n",
    "\n",
    "The following example from a machine learning textbook may be helpful:\n",
    "\n",
    "3. In this question, each pixel has three features, which are R, G, and B.\n",
    "\n",
    "4. At EM steps, gamma means $\\tau\\left(z_{k}\\right)$ at our slide of GMM, which is called the responsibility. If we have K components, each data point (pixel) will have K responsibility values. $\\tau\\left(z_{k}\\right)$ matrix size is $n\\times 1$. For this homework, you will work with $\\tau\\left(z\\right)$ which has a size of $n\\times k$ which means that you have all the responsibility values in one matrix.\n",
    "\n",
    "5. For E steps, we already get the log-likelihood at ll_joint() function. For the fomula at our slide: \n",
    "$$\n",
    "\\tau\\left(z_{k}\\right)=\\frac{\\pi_{k} N\\left(x | \\mu_{k}, \\Sigma_{k}\\right)}{\\sum_{j=1}^{K} \\pi_{j} N\\left(x | \\mu_{j}, \\Sigma_{j}\\right)},\n",
    "$$\n",
    "    \n",
    "ll_joint equals to the $N$ here. Thus, You should be able to finish E steps with just a few lines of code by using ll_joint() and softmax() defined above. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hJQe_EO8uB5D"
   },
   "source": [
    "## 3.1 Helper functions\n",
    "\n",
    "To facilitate some of the operations in the GMM implementation, we would like you to implement the following two helper functions. In these functions, \"logit\" refers to an input array of size $N x D$.\n",
    "\n",
    "### softmax\n",
    "\n",
    "Given $logit \\in \\mathbb{R}^{N x D}$, calculate $prob \\in \\mathbb{R}^{N x D}$, where $prob_{i, j} = \\frac{\\exp(logit_{i, j})}{\\sum_{d=1}^D exp(logit_{i, d})}$.\n",
    "\n",
    "Note that it is possible that $logit_{i, j}$ is very large, making $\\exp(\\cdot)$ of it to explode. To make sure it is numerical stable, you may need to subtract the maximum for each row of $logits$.  As in calculating pairwise distances, DO NOT USE A FOR LOOP.\n",
    "\n",
    "### logsumexp\n",
    "\n",
    "Given $logit \\in \\mathbb{R}^{N x D}$, calculate $s \\in \\mathbb{R}^N$, where $s_i = \\log \\big( \\sum_{j=1}^D \\exp(logit_{i, j}) \\big)$. Again, pay attention to the numerical problem. You may want to use similar trick as in the softmax function.  DO NOT USE A FOR LOOP."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oj3URdu1uB5K"
   },
   "source": [
    "## 3.2 GMM Implementations [40pts]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SkcdDpIy9Iz2"
   },
   "outputs": [],
   "source": [
    "class GMM(object):\n",
    "    def __init__(self): # No need to implement\n",
    "        pass\n",
    "    \n",
    "    def softmax(self,logits): # [5pts]\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            logits: N x D numpy array\n",
    "        Return:\n",
    "            prob: N x D numpy array\n",
    "        \"\"\"\n",
    "        logits -= np.max(logits,axis=1)[:,None]\n",
    "        prob = np.exp(logits)/(np.sum(np.exp(logits),axis=1))[:,None]\n",
    "        return prob\n",
    "        \n",
    "    def logsumexp(self,logits): # [5pts]\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            logits: N x D numpy array\n",
    "        Return:\n",
    "            s: N x 1 array where s[i,0] = logsumexp(logits[i,:])\n",
    "        \"\"\"\n",
    "        maxvals = np.max(logits,axis=1)[:,None]\n",
    "        logits -= maxvals\n",
    "        s = np.log(np.sum(np.exp(logits),axis=1))+maxvals.ravel()\n",
    "        return s\n",
    "    \n",
    "    def _init_components(self, points, K, **kwargs): # [5pts]\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            points: NxD numpy array, the observations\n",
    "            K: number of components\n",
    "            kwargs: any other args you want\n",
    "        Return:\n",
    "            pi: numpy array of length K, prior\n",
    "            mu: KxD numpy array, the center for each gaussian. \n",
    "            sigma: KxDxD numpy array, the diagonal standard deviation of each gaussian. You will have KxDxD numpy\n",
    "            array for full covariance matrix case\n",
    "            \n",
    "        \"\"\"    \n",
    "        D = points.shape[1]\n",
    "        mu = points[np.random.choice(points.shape[0],K,replace=False),:]        \n",
    "        cov_mat = np.cov(points, rowvar=False)+1e-4\n",
    "        sigma = np.repeat(cov_mat[np.newaxis,...],K,axis=0)        \n",
    "        pi = np.full(shape=K,fill_value = 1/K)\n",
    "        return pi,mu,sigma\n",
    "\n",
    "\n",
    "    def _ll_joint(self, points, pi, mu, sigma, **kwargs): # [10pts]\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            points: NxD numpy array, the observations\n",
    "            pi: np array of length K, the prior of each component\n",
    "            mu: KxD numpy array, the center for each gaussian. \n",
    "            sigma: KxDxD numpy array, the diagonal standard deviation of each gaussian. You will have KxDxD numpy\n",
    "            array for full covariance matrix case\n",
    "        Return:\n",
    "            ll(log-likelihood): NxK array, where ll(i, j) = log pi(j) + log NormalPDF(points_i | mu[j], sigma[j])\n",
    "            \n",
    "        Hint for undergraduate: Assume that each dimension of our multivariate gaussian are independent.  \n",
    "              This allows you to write treat it as a product of univariate gaussians.\n",
    "        \"\"\"\n",
    "        N,D = points.shape\n",
    "        K = mu.shape[0]\n",
    "        \n",
    "        ll = []\n",
    "        for i in range(K):\n",
    "            prior = pi[i]\n",
    "            sigma[i] += 1e-3 * np.identity(D)\n",
    "            norm_pdf_const = 1/np.sqrt(((2*np.pi)**D)*np.linalg.det(sigma[i]))\n",
    "            pts_norm = (points-mu[i])\n",
    "            lognorm_pdf = np.log(norm_pdf_const*np.exp(-0.5*np.einsum('ij,jk,ki->i',pts_norm,np.linalg.inv(sigma[i]),pts_norm.T)))\n",
    "            #lognorm_pdf = np.log(st.multivariate_normal(mean=mu[i], cov=sigma[i]).pdf(points))\n",
    "            ll_cluster = np.log(prior)+lognorm_pdf            \n",
    "            ll.append(ll_cluster)\n",
    "        ll = np.array(ll).T\n",
    "\n",
    "\n",
    "            \n",
    "        return ll\n",
    "\n",
    "    def _E_step(self, points, pi, mu, sigma, **kwargs): # [5pts]\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            points: NxD numpy array, the observations\n",
    "            pi: np array of length K, the prior of each component\n",
    "            mu: KxD numpy array, the center for each gaussian. \n",
    "            sigma: KxDxD numpy array, the diagonal standard deviation of each gaussian.You will have KxDxD numpy\n",
    "            array for full covariance matrix case\n",
    "        Return:\n",
    "            gamma(tau): NxK array, the posterior distribution (a.k.a, the soft cluster assignment) for each observation.\n",
    "            \n",
    "        Hint: You should be able to do this with just a few lines of code by using _ll_joint() and softmax() defined above. \n",
    "        \"\"\"\n",
    "        gamma = self.softmax(self._ll_joint(points, pi, mu, sigma))\n",
    "        return(gamma)\n",
    "        \n",
    "    def _M_step(self, points, gamma, **kwargs): # [10pts]\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            points: NxD numpy array, the observations\n",
    "            gamma(tau): NxK array, the posterior distribution (a.k.a, the soft cluster assignment) for each observation.\n",
    "        Return:\n",
    "            pi: np array of length K, the prior of each component\n",
    "            mu: KxD numpy array, the center for each gaussian. \n",
    "            sigma: KxDxD numpy array, the diagonal variances of each gaussian. You will have KxDxD numpy\n",
    "            array for full covariance matrix case\n",
    "            \n",
    "        Hint:  There are formulas in the slide.\n",
    "        \"\"\"        \n",
    "        \n",
    "        N,D = points.shape\n",
    "        K = gamma.shape[1]\n",
    "        mu = []\n",
    "        sigma = []\n",
    "        pi = []\n",
    "        for i in range(K):\n",
    "            mu_cl = np.sum(points*gamma[:,i].reshape(N,1),axis=0)/np.sum(gamma[:,i])\n",
    "            mu.append(mu_cl)            \n",
    "            sigma_cl = np.dot((gamma[:, i].reshape(N,1)*(points-mu_cl)).T, (points-mu_cl))/np.sum(gamma[:,i])+1e-5 * np.identity(D)\n",
    "            sigma.append(sigma_cl)            \n",
    "            pi_cl = np.sum(gamma[:, i])/np.sum(gamma)\n",
    "            pi.append(pi_cl)\n",
    "            \n",
    "        mu = np.array(mu)\n",
    "        sigma = np.array(sigma)\n",
    "        pi = np.array(pi)\n",
    "        \n",
    "        return pi, mu, sigma\n",
    "        \n",
    "    def __call__(self, points, K, max_iters=100, abs_tol=1e-16, rel_tol=1e-16, **kwargs):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            points: NxD numpy array, where N is # points and D is the dimensionality\n",
    "            K: number of clusters\n",
    "            max_iters: maximum number of iterations\n",
    "            abs_tol: convergence criteria w.r.t absolute change of loss\n",
    "            rel_tol: convergence criteria w.r.t relative change of loss\n",
    "            kwargs: any additional arguments you want\n",
    "        Return:\n",
    "            gamma(tau): NxK array, the posterior distribution (a.k.a, the soft cluster assignment) for each observation.\n",
    "            (pi, mu, sigma): (1xK np array, KxD numpy array, KxDxD numpy array)       \n",
    "        Hint: You do not need to change it. For each iteration, we process E and M steps, then \n",
    "        \"\"\"\n",
    "        pi, mu, sigma = self._init_components(points, K, **kwargs)\n",
    "        pbar = tqdm(range(max_iters))\n",
    "        for it in pbar:\n",
    "            # E-step\n",
    "            gamma = self._E_step(points, pi, mu, sigma)\n",
    "            \n",
    "            # M-step\n",
    "            pi, mu, sigma = self._M_step(points, gamma)\n",
    "            \n",
    "            # calculate the negative log-likelihood of observation\n",
    "            joint_ll = self._ll_joint(points, pi, mu, sigma)\n",
    "            loss = -np.sum(self.logsumexp(joint_ll))\n",
    "            if it:\n",
    "                diff = np.abs(prev_loss - loss)\n",
    "                if diff < abs_tol and diff / prev_loss < rel_tol:\n",
    "                    break\n",
    "            prev_loss = loss\n",
    "            pbar.set_description('iter %d, loss: %.4f' % (it, loss))\n",
    "        return gamma, (pi, mu, sigma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3TKYVhEwGKCu"
   },
   "source": [
    "## 3.3 Japanese art and pixel clustering [10pts]\n",
    "\n",
    "Ukiyo-e is a Japanese art genre predominant from the 17th through 19th centuries. In order to produce the intricate prints that came to represent the genre, artists carved wood blocks with the patterns for each color in a design. Paint would be applied to the block and later transfered to the print to form the image.\n",
    "In this section, you will use your GMM algorithm implementation to do pixel clustering and estimate how many wood blocks were likely used to produce a single print. (Hint: you can justify your answer based on visual inspection of the resulting images or on a different metric of your choosing)\n",
    "#### You do NOT need to submit your code for this question to the autograder. Instead you should include whatever images/information you find relevant in the report.\n",
    "\n",
    "### Answer:\n",
    "For the pixel clustering, images 0 and 3 were used. Here are the original images:\n",
    "Image 0:\n",
    "<img src=\"files/image0_original.jpg\">\n",
    "\n",
    "Image 3:\n",
    "<img src=\"files/image3_original.jpg\">\n",
    "\n",
    "Here are the results:\n",
    "\n",
    "Image 0:\n",
    "<img src=\"files/GMM_6-10-image0.png\">\n",
    "\n",
    "Image 3:\n",
    "\n",
    "<img src=\"files/GMM_6-10-image3.png\">\n",
    "<img src=\"files/GMM_10-15-image3.png\">\n",
    "\n",
    "From the above images, it appears that the contrast is best captured by using K=10 clusters for Image 0, and K=15 clusters for Image 3. Consequentially, K also indicates the most likely number of blocks used to create these paintings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CiRrwAbVKSQL"
   },
   "outputs": [],
   "source": [
    "# helper function for performing pixel clustering. You don't have to modify it\n",
    "def cluster_pixels_gmm(image, K):\n",
    "    \"\"\"Clusters pixels in the input image\n",
    "    \n",
    "    Args:\n",
    "        image: input image of shape(H, W, 3)\n",
    "        K: number of components\n",
    "    Return:\n",
    "        clustered_img: image of shape(H, W, 3) after pixel clustering\n",
    "    \"\"\"\n",
    "    im_height, im_width, im_channel = image.shape\n",
    "    flat_img = np.reshape(image, [-1, im_channel]).astype(np.float32)\n",
    "    gamma, (pi, mu, sigma) = GMM()(flat_img, K=K, max_iters=100)\n",
    "    cluster_ids = np.argmax(gamma, axis=1)\n",
    "    centers = mu\n",
    "\n",
    "    gmm_img = np.reshape(centers[cluster_ids], (im_height, im_width, im_channel))\n",
    "    \n",
    "    return gmm_img\n",
    "\n",
    "# helper function for plotting images. You don't have to modify it\n",
    "def plot_images(img_list, title_list, figsize=(20, 10)):    \n",
    "    assert len(img_list) == len(title_list)\n",
    "    f, axarr = plt.subplots(1,len(title_list),figsize=figsize,squeeze=False)\n",
    "    for i in range(len(img_list)):\n",
    "        img = img_list[i]/255.0\n",
    "        axarr[0,i].imshow(img)\n",
    "        axarr[0,i].set_title(title_list[i])\n",
    "        axarr[0,i].axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 375,
     "referenced_widgets": [
      "02ced44bb79744d9ad1eec51ce730db4",
      "6770b3f708a4412b8c0c20cc6ac92b75"
     ]
    },
    "colab_type": "code",
    "id": "kgVYo3cGGXcs",
    "outputId": "02a2c7fc-8293-4038-cff9-c305044778f3",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# pick 2 of the images in this list:\n",
    "url0 = 'https://upload.wikimedia.org/wikipedia/commons/thumb/1/10/Kuniyoshi_Utagawa%2C_Suikoden_Series_4.jpg/320px-Kuniyoshi_Utagawa%2C_Suikoden_Series_4.jpg'\n",
    "url1 = 'https://upload.wikimedia.org/wikipedia/commons/thumb/a/a9/Shibai_Ukie_by_Masanobu_Okumura.jpg/640px-Shibai_Ukie_by_Masanobu_Okumura.jpg'\n",
    "url2 = 'https://upload.wikimedia.org/wikipedia/commons/thumb/f/fd/Flickr_-_%E2%80%A6trialsanderrors_-_Utamaro%2C_Kushi_%28Comb%29%2C_ca._1785.jpg/388px-Flickr_-_%E2%80%A6trialsanderrors_-_Utamaro%2C_Kushi_%28Comb%29%2C_ca._1785.jpg'\n",
    "url3 = 'https://upload.wikimedia.org/wikipedia/commons/thumb/c/c2/Geisha_Playing_the_Hand-Game_Kitsune-ken_%28%E7%8B%90%E6%8B%B3%29%2C_a_Japanese_rock-paper-scissors_variant%2C_by_Kikukawa_Eizan_%28%E8%8F%8A%E5%B7%9D%E8%8B%B1%E5%B1%B1%29.jpg/640px-Geisha_Playing_the_Hand-Game_Kitsune-ken_%28%E7%8B%90%E6%8B%B3%29%2C_a_Japanese_rock-paper-scissors_variant%2C_by_Kikukawa_Eizan_%28%E8%8F%8A%E5%B7%9D%E8%8B%B1%E5%B1%B1%29.jpg'\n",
    "\n",
    "# example of loading image from url0\n",
    "image0 = imageio.imread(imageio.core.urlopen(url0).read())\n",
    "image3 = imageio.imread(imageio.core.urlopen(url3).read())\n",
    "\n",
    "# this is for you to implement\n",
    "def find_n_woodblocks(image, min_clusters=5, max_clusters=10):\n",
    "    \"\"\"Uses the \n",
    "    \n",
    "    Args:\n",
    "        image: input image of shape(H, W, 3)\n",
    "        K: number of components\n",
    "    Return:\n",
    "        plot: comparison between original image and image pixel clustering (you can use the helper function)\n",
    "        optional: any other information/metric/plot you think is necessary.\n",
    "    \"\"\"\n",
    "    \n",
    "    clustered_images = []\n",
    "    title_list = []\n",
    "    for i in range(min_clusters,max_clusters+1): #max_clusters+1):\n",
    "        res_clustering = cluster_pixels_gmm(image,i)\n",
    "        clustered_images.append(res_clustering)\n",
    "        title_list.append(str(i)+\" clusters in GMM\")\n",
    "        \n",
    "    plot_images(clustered_images, title_list, figsize=(20, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "res_clustering_0 = find_n_woodblocks(image0,min_clusters=5,max_clusters=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_clustering_3 = find_n_woodblocks(image3,min_clusters=5,max_clusters=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "T3IAXZNgGaHl"
   },
   "source": [
    "## 4 (Bonus for All) Messy, messy data and semi-supervised learning [30 pts]\n",
    "(Preamble: This part of the assignment was designed to expose you to interesting topics we did not cover in class, while allowing you to do minimal work by reusing most of your previous implementations with some modifications.)\n",
    "\n",
    "Two students at Georgia Tech want to improve the safety of composite Lithium-ion batteries by leveraging data obtained from quality control tests and machine learning. They ordered several battery specimens $-$ rated as safe or unsafe $-$ from various manufacturers. They proceeded to measure the chemical stability, mechanical resistance and charging rate of each specimen.\n",
    "\n",
    "When the campus shutdown was announced in the Spring 2020, the students rushed to the lab to try and collect the hard disks where the data had been stored. After settling back in their hometowns, they compiled the dataset and formatted it such that each row corresponds to the characterization results of a specimen, organized as follows:\n",
    "\n",
    "Chemical stability, mechanical resistance, charging rate, [safe/unsafe]\n",
    "\n",
    "They soon realized they have two major problems:\n",
    "* They only have the safe/unsafe labels for characterization tests performed on batteries from one manufacturer (20% of the data), while the labels are missing for all specimens by other manufacturers.\n",
    "* Due to a number of corrupt files, several of the labeled tests (30%) are missing some characterization data, which they labeled as NaN on their dataset.\n",
    "\n",
    "The students are aware that the few data points with complete information do not reflect the overall variance of the data. They realize they cannot exclude neither the remaining unlabeled data nor the messy labeled data.\n",
    "\n",
    "Your job is to assist the students in cleaning their data and implementing a semi-supervised learning framework to help them create a general classifier.\n",
    "\n",
    "To help you with this task the students shared four datasets:\n",
    "* Labeled_materials_complete.txt: containing the complete material characterization data and corresponding labels (safe = 1 and unsafe = 0);\n",
    "* Labeled_materials_incomplete.txt: containing partial material characterization data and corresponding labels (safe = 1 and unsafe = 0);\n",
    "* Unlabeled_materials.txt: containing only complete material characterization results;\n",
    "* Independent_materials.txt: a labeled dataset the students obtained from a previous student in the laboratory, which you can use to test your model after training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Data cleaning with k-NN [10pts]\n",
    "The first step in this task is to clean the Labeled_materials_incomplete dataset by filling in the missing values with probable ones derived from complete data. A useful approach to this type of problem is using a k-nearest neighbors (k-NN) algorithm. For this application, the method consists of replacing the missing value of a given point with the mean of the closest k-neighbors to that point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CleanData(object):\n",
    "    def __init__(self): # No need to implement\n",
    "        pass\n",
    "    \n",
    "    def pairwise_dist(self, x, y): # [0pts] - copy from kmeans\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: N x D numpy array\n",
    "            y: M x D numpy array\n",
    "        Return:\n",
    "            dist: N x M array, where dist2[i, j] is the euclidean distance between \n",
    "            x[i, :] and y[j, :]\n",
    "        \"\"\"\n",
    "        dist = np.linalg.norm(x[:,None]-y,axis=2)       \n",
    "        return dist\n",
    "    \n",
    "    def __call__(self, incomplete_points,  complete_points, K, **kwargs): # [10pts]\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            incomplete_points: N_incomplete x (D+1) numpy array, the incomplete labeled observations\n",
    "            complete_points: N_complete x (D+1) numpy array, the complete labeled observations\n",
    "            K: integer, corresponding to the number of nearest neighbors you want to base your calculation on\n",
    "            kwargs: any other args you want\n",
    "        Return:\n",
    "            clean_points: (N_incomplete + N_complete) x (D+1) numpy array of length K, containing both complete points and recently filled points\n",
    "            \n",
    "        Hints: (1) You want to find the k-nearest neighbors within each class separately;\n",
    "               (2) There are missing values in all of the features. It might be more convenient to address each feature at a time.\n",
    "        \"\"\"\n",
    "        c_safe = complete_points[np.where(complete_points[:,-1] == 1)]\n",
    "        ic_safe = incomplete_points[np.where(incomplete_points[:,-1] == 1)]\n",
    "        \n",
    "        c_unsafe = complete_points[np.where(complete_points[:,-1] == 0)]\n",
    "        ic_unsafe = incomplete_points[np.where(incomplete_points[:,-1] == 0)]\n",
    "        \n",
    "        for ic_pt in ic_safe:\n",
    "            bad_ind = np.argwhere(np.isnan(ic_pt)).ravel()\n",
    "            remaining_pts = np.delete(c_safe,bad_ind,1)\n",
    "            pt_wo_nan = np.delete(ic_pt,bad_ind,0).reshape(1,-1)\n",
    "            distances = self.pairwise_dist(pt_wo_nan,remaining_pts)            \n",
    "            nn_pts_ind = distances.argsort()[:K]\n",
    "            nn_pts = np.take(c_safe,nn_pts_ind,axis=0)[0]\n",
    "            avg_vals = np.mean(nn_pts,axis=0)[bad_ind]            \n",
    "            ic_pt[bad_ind] = avg_vals\n",
    "            \n",
    "        for ic_pt in ic_unsafe:\n",
    "            bad_ind = np.argwhere(np.isnan(ic_pt)).ravel()\n",
    "            remaining_pts = np.delete(c_unsafe,bad_ind,1)\n",
    "            pt_wo_nan = np.delete(ic_pt,bad_ind,0).reshape(1,-1)\n",
    "            distances = self.pairwise_dist(pt_wo_nan,remaining_pts)            \n",
    "            nn_pts_ind = distances.argsort()[:K]\n",
    "            nn_pts = np.take(c_unsafe,nn_pts_ind,axis=0)[0]\n",
    "            avg_vals = np.mean(nn_pts,axis=0)[bad_ind]            \n",
    "            ic_pt[bad_ind] = avg_vals\n",
    "            \n",
    "        clean_points = np.vstack((c_safe,c_unsafe,ic_safe,ic_unsafe))        \n",
    "            \n",
    "        return clean_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_data = np.array([[1.,2.,3.,1],[7.,8.,9.,0],[16.,17.,18.,1],[22.,23.,24.,0]])\n",
    "incomplete_data = np.array([[1.,np.nan,3,1],[7.,np.nan,9.,0],[np.nan,17.,18.,1],[np.nan,23.,24.,0]])\n",
    "\n",
    "clean_data = CleanData()(incomplete_data, complete_data, 2)\n",
    "print(\"*** Expected Answer - k = 2 ***\")\n",
    "print(\"\"\"==complete data==\n",
    "[[ 1.  2.  3.  1.]\n",
    " [ 7.  8.  9.  0.]\n",
    " [16. 17. 18.  1.]\n",
    " [22. 23. 24.  0.]]\n",
    "==incomplete data==\n",
    "[[ 1. nan  3.  1.]\n",
    " [ 7. nan  9.  0.]\n",
    " [nan 17. 18.  1.]\n",
    " [nan 23. 24.  0.]]\n",
    "==clean_data==\n",
    "[[ 1.   2.   3.   1. ]\n",
    " [ 7.   8.   9.   0. ]\n",
    " [16.  17.  18.   1. ]\n",
    " [22.  23.  24.   0. ]\n",
    " [14.5 23.  24.   0. ]\n",
    " [ 7.  15.5  9.   0. ]\n",
    " [ 8.5 17.  18.   1. ]\n",
    " [ 1.   9.5  3.   1. ]]\"\"\")\n",
    "\n",
    "print(\"\\n*** My Answer - k = 2***\")\n",
    "print(clean_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wJ_esg6lGzyg"
   },
   "source": [
    "### 4.2 Getting acquainted with semi-supervised learning approaches. [5pts]\n",
    "\n",
    "You will implement a version of the algorithm presented in Table 1 of the paper [\"Text Classification from Labeled and Unlabeled Documents using EM\"](http://www.kamalnigam.com/papers/emcat-mlj99.pdf) by Nigam et al. (2000). While you are recommended to read the whole paper this assignment focuses on items 1$-$5.2 and 6.1. Write a brief summary of three interesting highlights of the paper (50-word maximum).\n",
    "\n",
    "### Answers\n",
    "Three interesting aspects of the paper are:\n",
    "\n",
    "- EM (Expectation Maximization) provides a methodical probabilistic approach to classification, when the assumptions of a generative model hold.\n",
    "- The accuracy of text classifiers can be drastically improved by adding limited training data (correctly labelled) to a much larger test dataset.\n",
    "- Assigning a weight contribution to different clusters of training data can improve classification performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KTK4FZpejU5Z"
   },
   "source": [
    "### 4.3 Implementing the EM algorithm. [10 pts]\n",
    "In your implementation of the EM algorithm proposed by Nigam et al. (2000) on Table 1, you will use a Gaussian Naive Bayes (GNB) classifier as opposed to a naive Bayes (NB) classifier. (Hint: Using a GNB in place of an NB will enable you to reuse most of the implementation you developed for GMM in this assignment. In fact, you can successfully solve the problem by simply modifying the call method.)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "z6tw7JcVjK6N"
   },
   "outputs": [],
   "source": [
    "class SemiSupervised(object):\n",
    "    def __init__(self): # No need to implement\n",
    "        pass\n",
    "    \n",
    "    def softmax(self,logits): # [0 pts] - can use same as for GMM\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            logits: N x D numpy array\n",
    "        Return:\n",
    "            logits: N x D numpy array\n",
    "        \"\"\"\n",
    "        logits -= np.max(logits,axis=1)[:,None]\n",
    "        prob = np.exp(logits)/(np.sum(np.exp(logits),axis=1))[:,None]\n",
    "        return prob\n",
    "\n",
    "    def logsumexp(self,logits): # [0 pts] - can use same as for GMM\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            logits: N x D numpy array\n",
    "        Return:\n",
    "            s: N x 1 array where s[i,0] = logsumexp(logits[i,:])\n",
    "        \"\"\"\n",
    "        maxvals = np.max(logits,axis=1)[:,None]\n",
    "        logits -= maxvals\n",
    "        s = np.log(np.sum(np.exp(logits),axis=1))+maxvals.ravel()\n",
    "        return s\n",
    "    \n",
    "    def _init_components(self, points, K, **kwargs): # [5 pts] - modify from GMM\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            points: Nx(D+1) numpy array, the observations\n",
    "            K: number of components\n",
    "            kwargs: any other args you want\n",
    "        Return:\n",
    "            pi: numpy array of length K, prior\n",
    "            mu: KxD numpy array, the center for each gaussian. \n",
    "            sigma: KxDxD numpy array, the diagonal standard deviation of each gaussian.\n",
    "            \n",
    "        Hint: The paper describes how you should initialize your algorithm.\n",
    "        \"\"\"\n",
    "        \n",
    "        \n",
    "        mu = points[np.random.choice(points.shape[0],K,replace=False),:]        \n",
    "        pi = np.full(shape=K,fill_value = 1/K)\n",
    "        cov_mat = np.diag(np.diag(np.cov(points, rowvar=False)+1e-4))\n",
    "        sigma = np.repeat(cov_mat[np.newaxis,...],K,axis=0)\n",
    "        \n",
    "        return pi,mu,sigma\n",
    "        \n",
    "\n",
    "    def _ll_joint(self, points, pi, mu, sigma, **kwargs): # [0 pts] - can use same as for GMM\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            points: NxD numpy array, the observations\n",
    "            pi: np array of length K, the prior of each component\n",
    "            mu: KxD numpy array, the center for each gaussian. \n",
    "            sigma: KxDxD numpy array, the diagonal standard deviation of each gaussian.\n",
    "        Return:\n",
    "            ll(log-likelihood): NxK array, where ll(i, j) = log pi(j) + log NormalPDF(points_i | mu[j], sigma[j])\n",
    "            \n",
    "        Hint: Assume that the three properties of the lithium-ion batteries (multivariate gaussian) are independent.  \n",
    "              This allows you to treat it as a product of univariate gaussians.\n",
    "        \"\"\"\n",
    "        N,D = points.shape\n",
    "        K = mu.shape[0]\n",
    "        \n",
    "        ll = []\n",
    "        \n",
    "        for i in range(K):\n",
    "            prior = pi[i]\n",
    "            sigma[i]  = np.diag(np.diag(sigma[i]))\n",
    "            norm_pdf_const = 1/np.sqrt(((2*np.pi)**D)*np.linalg.det(sigma[i]))\n",
    "            pts_norm = (points-mu[i])\n",
    "            lognorm_pdf = np.log(norm_pdf_const*np.exp(-0.5*np.einsum('ij,jk,ki->i',pts_norm,np.linalg.inv(sigma[i]),pts_norm.T)))\n",
    "            #lognorm_pdf = np.log(st.multivariate_normal(mean=mu[i], cov=sigma[i]).pdf(points))\n",
    "            ll_cluster = np.log(prior)+lognorm_pdf            \n",
    "            ll.append(ll_cluster)\n",
    "        ll = np.array(ll).T\n",
    "        return ll\n",
    "\n",
    "    def _E_step(self, points, pi, mu, sigma, **kwargs): # [0 pts] - can use same as for GMM\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            points: NxD numpy array, the observations\n",
    "            pi: np array of length K, the prior of each component\n",
    "            mu: KxD numpy array, the center for each gaussian. \n",
    "            sigma: KxDxD numpy array, the diagonal standard deviation of each gaussian.\n",
    "        Return:\n",
    "            gamma: NxK array, the posterior distribution (a.k.a, the soft cluster assignment) for each observation.\n",
    "            \n",
    "        Hint: You should be able to do this with just a few lines of code by using _ll_joint() and softmax() defined above. \n",
    "        \"\"\"\n",
    "        gamma = self.softmax(self._ll_joint(points, pi, mu, sigma))\n",
    "        return(gamma)\n",
    "\n",
    "    def _M_step(self, points, gamma, **kwargs): # [0 pts] - can use same as for GMM\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            points: NxD numpy array, the observations\n",
    "            gamma: NxK array, the posterior distribution (a.k.a, the soft cluster assignment) for each observation.\n",
    "        Return:\n",
    "            pi: np array of length K, the prior of each component\n",
    "            mu: KxD numpy array, the center for each gaussian. \n",
    "            sigma: KxDxD numpy array, the diagonal variances of each gaussian. \n",
    "            \n",
    "        Hint:  There are formulas in the slide.\n",
    "        \"\"\"\n",
    "        N,D = points.shape\n",
    "        K = gamma.shape[1]\n",
    "        mu = []\n",
    "        sigma = []\n",
    "        pi = []\n",
    "        for i in range(K):\n",
    "            mu_cl = np.sum(points*gamma[:,i].reshape(N,1),axis=0)/np.sum(gamma[:,i])\n",
    "            mu.append(mu_cl)            \n",
    "            sigma_cl = np.dot((gamma[:, i].reshape(N,1)*(points-mu_cl)).T, (points-mu_cl))/np.sum(gamma[:,i])+1e-5 * np.identity(D)\n",
    "            sigma.append(sigma_cl)            \n",
    "            pi_cl = np.sum(gamma[:, i])/np.sum(gamma)\n",
    "            pi.append(pi_cl)\n",
    "            \n",
    "        mu = np.array(mu)\n",
    "        sigma = np.array(sigma)\n",
    "        pi = np.array(pi)\n",
    "        \n",
    "        return pi, mu, sigma\n",
    "\n",
    "    def __call__(self, points, K, max_iters=100, abs_tol=1e-16, rel_tol=1e-16, **kwargs): # [5 pts] - modify from GMM\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            points: NxD numpy array, where N is # points and D is the dimensionality\n",
    "            K: number of clusters\n",
    "            max_iters: maximum number of iterations\n",
    "            abs_tol: convergence criteria w.r.t absolute change of loss\n",
    "            rel_tol: convergence criteria w.r.t relative change of loss\n",
    "            kwargs: any additional arguments you want\n",
    "        Return:\n",
    "            (pi, mu, sigma): (1xK np array, KxD numpy array, KxD numpy array), mu and sigma.\n",
    "         \n",
    "        \"\"\"\n",
    "        #Have to use labelled data only for initializing components, and only unlabelled for e-step and m-step\n",
    "        points_l = points[np.where(points[:,-1] != -1)][:,:-1]\n",
    "        points_ul = points[np.where(points[:,-1] == -1)][:,:-1]\n",
    "        \n",
    "        pi, mu, sigma = self._init_components(points_l, K, **kwargs)\n",
    "        pbar = tqdm(range(max_iters))\n",
    "        for it in pbar:\n",
    "            # E-step\n",
    "            gamma = self._E_step(points_ul, pi, mu, sigma)            \n",
    "            # M-step\n",
    "            pi, mu, sigma = self._M_step(points_ul, gamma)\n",
    "            \n",
    "            # calculate the negative log-likelihood of observation\n",
    "            joint_ll = self._ll_joint(points_ul, pi, mu, sigma)\n",
    "            loss = -np.sum(self.logsumexp(joint_ll))\n",
    "            if it:\n",
    "                diff = np.abs(prev_loss - loss)\n",
    "                #if diff < abs_tol and diff / prev_loss < rel_tol:\n",
    "                    #break\n",
    "            prev_loss = loss\n",
    "            pbar.set_description('iter %d, loss: %.4f' % (it, loss))\n",
    "        return (pi, mu, sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "incomplete_mat = np.genfromtxt('Labeled_materials_incomplete.txt',delimiter=',')\n",
    "complete_mat = np.genfromtxt('Labeled_materials_complete.txt',delimiter=',')\n",
    "clean_labelled = CleanData()\n",
    "labelled_mat = clean_labelled(incomplete_mat,complete_mat,K=2)\n",
    "unlabelled_mat = np.genfromtxt('Unlabeled_materials.txt',delimiter=',')\n",
    "flag = np.ones((unlabelled_mat.shape[0],1))*-1\n",
    "unlabelled_mat = np.append(unlabelled_mat,flag,axis=1)\n",
    "points = np.vstack((labelled_mat,unlabelled_mat))\n",
    "\n",
    "ss = SemiSupervised()\n",
    "result_ss = ss(points,K=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Qlb7C2UEjNGU"
   },
   "source": [
    "### 4.4 Demonstrating the performance of the algorithm. [5pts]\n",
    "Compare the classification error based on the Gaussian Naive Bayes (GNB) classifier you implemented following the Nigam et al. (2000) approach to the performance of a GNB classifier trained using only labeled data. Since you have not covered supervised learning in class, you are allowed to use the scikit learn library for training the GNB classifier based only on labeled data: https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "class ComparePerformance(object):\n",
    "    \n",
    "    def __init__(self): #No need to implement\n",
    "        pass\n",
    "    \n",
    "    def accuracy_semi_supervised(self,points, independent, 2):\n",
    "        \"\"\"\n",
    "        Args: \n",
    "            points: Nx(D+1) numpy array, where N is the number of points in the training set, D is the dimensionality, the last column\n",
    "            represents the labels (when available) or a flag that allows you to separate the unlabeled data. \n",
    "            independent: Nx(D+1) numpy array, where N is # points and D is the dimensionality and the last column are the correct labels\n",
    "        Return:\n",
    "            accuracy: floating number\n",
    "        \"\"\"\n",
    "        \n",
    "        raise NotImplementedError\n",
    "\n",
    "    def accuracy_GNB_onlycomplete(self,points, independent, 2):\n",
    "        \"\"\"\n",
    "        Args: \n",
    "            points: Nx(D+1) numpy array, where N is the number of only initially complete labeled points in the training set, D is the dimensionality, the last column\n",
    "            represents the labels.\n",
    "            independent: Nx(D+1) numpy array, where N is # points and D is the dimensionality and the last column are the correct labels\n",
    "        Return:\n",
    "            accuracy: floating number\n",
    "        \"\"\"\n",
    "        classifier = GaussianNB()\n",
    "        classifier.fit()\n",
    "        \n",
    "        raise NotImplementedError          \n",
    "\n",
    "    def accuracy_GNB_cleandata(self,points, independent, 2):\n",
    "        \"\"\"\n",
    "        Args: \n",
    "            points: Nx(D+1) numpy array, where N is the number of clean labeled points in the training set, D is the dimensionality, the last column\n",
    "            represents the labels.\n",
    "            independent: Nx(D+1) numpy array, where N is # points and D is the dimensionality and the last column are the correct labels\n",
    "        Return:\n",
    "            accuracy: floating number\n",
    "        \"\"\"\n",
    "        raise NotImplementedError"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "spring20_HW2_template.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
