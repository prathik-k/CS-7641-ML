{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summer 2020 CX4641/CS7641 Homework 3\n",
    "\n",
    "## Instructor: Dr. Mahdi Roozbahani\n",
    "\n",
    "## Deadline: July 10th, Friday, 11:59 pm\n",
    "\n",
    "* No unapproved extension of the deadline is allowed. Late submission will lead to 0 credit. \n",
    "\n",
    "* Discussion is encouraged on Piazza as part of the Q/A. However, all assignments should be done individually.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions for the assignment\n",
    "\n",
    "- In this assignment, we have programming and writing questions.\n",
    "- To switch between cell for code and for markdown, see the menu -> Cell -> Cell Type\n",
    "- You could directly type the Latex equations in the markdown cell.\n",
    "- Typing with Latex\\markdown is required for all the written questions. Handwritten answers would not be accepted. \n",
    "- If a question requires a picture, you could use this syntax $\"<img src=\"\" style=\"width: 300px;\"/>\"$ to include them within your ipython notebook.\n",
    "- Questions marked with <span style=\"color:blue\">**[P]**</span> are programming only and should be submitted to the autograder. Questions marked with <span style=\"color:green\">**[W]**</span> may required that you code a small function or generate plots, but should **NOT** be submitted to the autograder. It should be submitted on the writing portion of the assignment on gradescope\n",
    "- The outline of the assignment is as follows:\n",
    "    * Q1 [20 pts] > Naive Bayes classification. <span style=\"color:green\">**[W]** items 1.1 </span>| <span style=\"color:blue\">**[P]** items 1.2 </span>\n",
    "    * Q2 [30 pts] > Image compression with PCA <span style=\"color:green\">**[W]** 2.2 and 2.3 </span>| <span style=\"color:blue\">**[P]** items 2.1 </span>\n",
    "    * Q3 [Undergrad: 55+20 (bonus) pts| Grad: 75 pts] > Regression and regularization <span style=\"color:green\">**[W]** items 3.2, 3.3, 3.4 and 3.5 </span>| <span style=\"color:blue\">**[P]** items 3.1 </span>\n",
    "    * Q4 [20 pts] > Understanding PCA <span style=\"color:green\">**[W]** items 4.1 and 4.2 </span>| <span style=\"color:blue\">**[P]** None </span>\n",
    "    * Q5 [Bonus for all][30 pts] > Manifold learning with Isomap <span style=\"color:green\">**[W]** items 5.2 </span>| <span style=\"color:blue\">**[P]** items 5.1 </span>\n",
    "\n",
    "\n",
    "\n",
    "## Using the autograder\n",
    "\n",
    "- You will find two assignments on Gradescope that correspond to HW3: \"HW3 - Programming\" and \"HW3 - Non-programming\".\n",
    "- You will submit your code for the autograder on \"HW3 - Programming\" in the following format:\n",
    "\n",
    "    * nb.py\n",
    "    * imgcompression.py\n",
    "    * regression.py\n",
    "    * isomap.py\n",
    "\n",
    "- All you will have to do is to copy your implementations of the classes \"NaiveBayes\", \"ImgCompression\", \"Regression\", \"Isomap\" onto the respective files. We provided you different .py files and we added libraries in those files please DO NOT remove those lines and add your code after those lines. Note that these are the only allowed libraries that you can use for the homework.\n",
    "\n",
    "- You are allowed to make as many submissions until the deadline as you like. Additionally, note that the autograder tests each function separately, therefore it can serve as a useful tool to help you debug your code if you are not sure of what part of your implementation might have an issue.\n",
    "\n",
    "- **For the \"HW3 - Non-programming\" part, you will download your jupyter notbook as HTML, print it as a PDF from your browser and submit it on Gradescope. To download the notebook as html, click on \"File\" on the top left corner of this page and select \"Download as > HTML\". The non-programming part corresponds to Q2, Q3.3 (both your response and the generated images with your implementation) and Q4.2.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.feature_extraction import text\n",
    "from sklearn.datasets import load_boston, load_diabetes, load_digits, load_breast_cancer, load_iris, load_wine\n",
    "from scipy.sparse import csr_matrix\n",
    "from scipy.sparse.csgraph import floyd_warshall\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VseMdaxZ8MSh"
   },
   "source": [
    "## 1. Naive Bayes Classification [20pts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VLGB3ah5v9aN"
   },
   "source": [
    "In Bayesian classification, we're interested in finding the probability of a label given some observed feature vector $x = [x_{1},.., x_{d}]$, which we can write as $P(y~|~{ x_{1},.., x_{d}})$.\n",
    "Bayes's theorem tells us how to express this in terms of quantities we can compute more directly:\n",
    "\n",
    "\\begin{eqnarray} \n",
    "P(y~|~{ x_{1},.., x_{d}}) = \\frac{P({ x_{1},.., x_{d}}~|~y)P(y)}{P({ x_{1},.., x_{d}})} \n",
    "\\end{eqnarray}\n",
    "\n",
    "\n",
    "The main assumption in Naive Bayes is that, given the label, the observed features are conditionally independent i.e. \n",
    "\n",
    "$$\n",
    "P({ x_{1},.., x_{d}}~|~y) = P({x_{1}}~|~y) \\times .. \\times P({x_{d}}~|~y)    \n",
    "$$\n",
    "\n",
    "Therefore, we can rewrite Bayes rule as\n",
    "\n",
    "\\begin{eqnarray}\n",
    "P(y~|~{ x_{1},.., x_{d}}) = \\frac{P({x_{1}}~|~y) \\times .. \\times P({x_{d}}~|~y)P(y)}{P({ x_{1},.., x_{d}})}\n",
    "\\end{eqnarray}\n",
    "\n",
    "### Training Naive Bayes\n",
    "One way to train a Naive Bayes classifier is done using frequentist approach to calculate probability, which is simply going over the training data and calculating the frequency of different observations in the training set given different labels. For example,  \n",
    "\\begin{eqnarray}\n",
    "P({x_{1}=i}~|~y=j) &=& \\frac{P({x_{1}=i}, y=j)}{P(y=j)} =  \\frac{\\text{Number of times in training data } x_{1}=i \\text{ and } y=j }{\\text{Total number of times in training data } y=j}\n",
    "\\end{eqnarray}\n",
    "\n",
    "### Testing Naive Bayes\n",
    "\n",
    "During the testing phase, we try to estimate the probability of a label given an observed feature vector. We combine the probabilities computed from training data to estimate the probability of a given label. For example, if we are trying to decide between two labels $y_{1}$ and $y_{2}$, then we compute the ratio of the posterior probabilities for each label:\n",
    "\n",
    "\\begin{eqnarray}\n",
    "\\frac{P(y_{1}~|~ x_{1},.., x_{d})}{P(y_2~|~x_{1},.., x_{d})} = \\frac{P(x_{1},.., x_{d}~|~y_{1})}{P(x_{1},.., x_{d}~|~y_{2})}\\frac{P(y_1)}{P(y_2)}= \\frac{P({x_{1}}~|~y_{1}) \\times .. \\times P({x_{d}}~|~y_{1})P(y_{1})}{P({x_{1}}~|~y_{2}) \\times .. \\times P({x_{d}}~|~y_{2})P(y_{2})}\n",
    "\\end{eqnarray}\n",
    "\n",
    "All we need now is to compute $P(x_{1}|y_{i}),.., P(x_{d}~|~y_i)$ and $P(y_{i})$ for each label by pluging in the numbers we got during training. The label with the higher posterior probabilities is the one that is selected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Cp4y8Xws8XU-"
   },
   "source": [
    "### 1.1 Supermarket [5pts] <span style=\"color:green\">**[W]**</span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "beb0jOdBGsg2"
   },
   "source": [
    "A local supermarket specializing in breakfast cereals decides to analyze the buying patterns of its customers. They make a small survey asking 6 randomly chosen people their age (older or younger than 60 years) and which of the breakfast cereals (Cornflakes, Frosties, Sugar Puffs, Branflakes) they like. Each respondent provides a vector with entries 1 or 0 corresponding to whether they like or dislike the cereal. Thus a respondent with (1101) would like Cornflakes, Frosties and Branflakes, but not Sugar Puffs. The older than 60 years respondents provide the following data (1000), (1001), (1111), (0001). The younger than 60 years old respondents responded (0110), (1110). A new customer comes into the supermarket and says she only likes Frosties and Sugar Puffs. Using naive Bayes, Is the new customer older than 60?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zkrPLBXr8suX"
   },
   "source": [
    "### 1.2 The Federalist Papers [15pts] <span style=\"color:blue\">**[P]**</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NmnXJLaMYJXw"
   },
   "source": [
    " [The Federalist Papers](https://en.wikipedia.org/wiki/The_Federalist_Papers) were a series of essays written in 1787–1788 meant to persuade the citizens of the State of New York to ratify the Constitution and which were published anonymously under the pseudonym “Publius”. In later years the authors were revealed as Alexander Hamilton, John Jay, and James Madison. However, there is some disagreement as to who wrote which essays. Hamilton wrote a list of which essays he had authored only days before being killed in a duel with then Vice President Aaron Burr. Madison wrote his own list many years later, which is in conflict with Hamilton’s list on 12 of the essays. Since by this point the two (who were once close friends) had become bitter rivals, historians have long been unsure as to the reliability of both lists. We will try to settle this dispute using a simple Naive Bayes classifier. \n",
    "\n",
    "The code which is provided loads the documents and builds a [“bag of words” representation](https://en.wikipedia.org/wiki/Bag-of-words_model)  of each document. Your task is to complete the missing portions of the code and to determine your best guess as to who wrote each of the 12 disputed essays. \n",
    "(Hint: H and M are the labels that stand for Hamilton and Madison, while the label D stands for disputed for the papers we are trying to label in our data. Our job here is to define whether D essays belong to H or M using Naive Bayes. Note that the label D for disputed, is completely unrelated to the feature dimension D which is an integer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 165
    },
    "colab_type": "code",
    "id": "DE3Q5iy0vqxj",
    "outputId": "7796f284-b17a-4b99-ff19-2f83b94ed704",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "from sklearn.feature_extraction import text\n",
    "\n",
    "class NaiveBayes(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        # load Documents\n",
    "        x = open('fedpapers_split.txt').read()\n",
    "        papers = json.loads(x)\n",
    "\n",
    "        # split Documents\n",
    "        papersH = papers[0] # papers by Hamilton \n",
    "        papersM = papers[1] # papers by Madison\n",
    "        papersD = papers[2] # disputed papers\n",
    "\n",
    "        # Number of Documents for H, M and D\n",
    "        nH = len(papersH)\n",
    "        nM = len(papersM)\n",
    "        nD = len(papersD)\n",
    "        \n",
    "        '''To ignore certain common words in English that might skew your model, we add them to the stop words \n",
    "        list below. You may want to experiment by choosing your own list of stop words, but be sure to keep \n",
    "        'HAMILTON' and 'MADISON' in this list at a minimum, as their names appear in the text of the papers \n",
    "        and leaving them in could lead to unpredictable results '''\n",
    "\n",
    "        stop_words = text.ENGLISH_STOP_WORDS.union({'HAMILTON','MADISON'})\n",
    "        #stop_words = {'HAMILTON','MADISON'}\n",
    "        # Form bag of words model using words used at least 10 times\n",
    "        vectorizer = text.CountVectorizer(stop_words=stop_words,min_df=10)\n",
    "        X = vectorizer.fit_transform(papersH+papersM+papersD).toarray()\n",
    "\n",
    "        '''To visualize the full list of words remaining after filtering out stop words and words used less \n",
    "        than min_df times uncomment the following line'''\n",
    "        #print(vectorizer.vocabulary_)\n",
    "\n",
    "        # split word counts into separate matrices\n",
    "        self.XH, self.XM, self.XD = X[:nH,:], X[nH:nH+nM,:], X[nH+nM:,:]\n",
    "        \n",
    "\n",
    "    def _likelihood_ratio(self, XH, XM): # [5pts]\n",
    "        '''\n",
    "        Args:\n",
    "            XH: nH x D where nH is the number of documents that we have for Hamilton,\n",
    "                while D is the number of features (we use the word count as the feature)\n",
    "            XM: nM x D where nM is the number of documents that we have for Madison,\n",
    "                while D is the number of features (we use the word count as the feature)\n",
    "        Return:\n",
    "            fratio: 1 x D vector of the likelihood ratio of different words (Hamilton/Madison)\n",
    "        '''\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def _priors_ratio(self, XH, XM): # [5pts]\n",
    "        '''\n",
    "        Args:\n",
    "            XH: nH x D where nH is the number of documents that we have for Hamilton,\n",
    "                while D is the number of features (we use the word count as the feature)\n",
    "            XM: nM x D where nM is the number of documents that we have for Madison,\n",
    "                while D is the number of features (we use the word count as the feature)\n",
    "        Return:\n",
    "            pr: prior ratio of (Hamilton/Madison)\n",
    "        '''\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def classify_disputed(self, fratio, pratio, XD): # [5pts]\n",
    "        '''\n",
    "        Args:\n",
    "            fratio: 1 x D vector of ratio of likelihoods of different words\n",
    "            pratio: 1 x 1 number\n",
    "            XD: 12 x D bag of words representation of the 12 disputed documents (D = 1307 which are the number of features for each document)\n",
    "        Return:\n",
    "             1 x 12 list, each entry is H to indicate Hamilton or M to indicate Madison for the corrsponding document\n",
    "        '''        \n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    NB = NaiveBayes()\n",
    "    fratio = NB._likelihood_ratio(NB.XH, NB.XM)\n",
    "    pratio = NB._priors_ratio(NB.XH, NB.XM)\n",
    "    resolved = NB.classify_disputed(fratio, pratio, NB.XD)\n",
    "    \n",
    "    print(resolved)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hdZ6tPq2ifTp",
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "## 2. Image compression with PCA [30 pts]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5oTTYpV9Ur0j",
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "### Load images data and plot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-22T17:18:37.872437Z",
     "start_time": "2019-06-22T17:18:35.554046Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 561
    },
    "colab_type": "code",
    "id": "dTCTuygnUr0j",
    "outputId": "16389e80-c3eb-4312-9c39-e27edd6a36bb",
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# load Image\n",
    "image = plt.imread(\"hw3_img.jpg\")/255.\n",
    "#plot image\n",
    "fig = plt.figure(figsize=(10,10))\n",
    "plt.imshow(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-22T17:18:39.594607Z",
     "start_time": "2019-06-22T17:18:37.927119Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 488
    },
    "colab_type": "code",
    "id": "ULd7_WqGUr0m",
    "outputId": "91da9771-ff7d-4184-9b46-80367cbc0f6a",
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "def rgb2gray(rgb):   \n",
    "    return np.dot(rgb[...,:3], [0.299, 0.587, 0.114])\n",
    "\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "# plot several images\n",
    "plt.imshow(rgb2gray(image), cmap=plt.cm.bone)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xaIhkaKyTyJb",
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "### 2.1 Image compression [20pts]  <span style=\"color:blue\">**[P]**</span>\n",
    "\n",
    "The SVD allows us to compress an image by throwing away the least important information.  The greater the singular values -> the greater the variance -> most information from the corresponding singular vector. SVD each matrix and get rid of the small singular values to compress the image. The loss of inforamtion is negligible as the difference is very difficult to be spotted. [Principal Component Analysis](https://en.wikipedia.org/wiki/Principal_component_analysis) (PCA) follows the same process to elimate the small variance eigenvalues and their vectors.  With PCA, we center the data first by subtracting the mean. Each singular value tells us how much of the variance of a matrix (e.g. image) is captured in each component.  For example, the variance captured by the first component is $$\\frac{\\sigma_1}{\\sum_{i=1}^n \\sigma_i}$$ where $\\sigma_i$ is the $i^{th}$ singular value. You need to finish the following functions to do SVD and then reconstruct the image by components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-22T17:18:39.629743Z",
     "start_time": "2019-06-22T17:18:39.599162Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "h_XXstqJTpJf",
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "class ImgCompression(object):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def svd(self, X): # [5pts]\n",
    "        \"\"\"\n",
    "        Do SVD. You could use numpy SVD.\n",
    "        Your function should be able to handle black and white\n",
    "        images (N x D arrays) as well as color images (N x D x 3 arrays)\n",
    "        In the image compression, we assume that each colum of the image is a feature. Image is the matrix X.\n",
    "        Args:\n",
    "            X: N x D array corresponding to an image (N x D x 3 if color image)\n",
    "        Return:\n",
    "            U: N x N (N x N x 3, for color images)\n",
    "            S: min(N, D) x 1 (min(N, D) x 3, for color images)\n",
    "            V: D x D (D x D x 3, for color images)\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "    def rebuild_svd(self, U, S, V, k): # [5pts]\n",
    "        \"\"\"\n",
    "        Rebuild SVD by k componments.\n",
    "        Args:\n",
    "            U: N x N (N x N x 3, for color images)\n",
    "            S: min(N, D) x 1 (min(N, D) x 3, for color images)\n",
    "            V: D x D (D x D x 3, for color images)\n",
    "            k: int corresponding to number of components\n",
    "        Return:\n",
    "            Xrebuild: N x D array of reconstructed image (N x D x 3 if color image)\n",
    "\n",
    "        Hint: numpy.matmul may be helpful for reconstructing color images\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "    def compression_ratio(self, X, k): # [5pts]\n",
    "        \"\"\"\n",
    "        Compute compression of an image: (num stored values in compressed)/(num stored values in original)\n",
    "        Args:\n",
    "            X: N x D array corresponding to an image (N x D x 3 if color image)\n",
    "            k: int corresponding to number of components\n",
    "        Return:\n",
    "            compression_ratio: float of proportion of storage used by compressed image\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "    def recovered_variance_proportion(self, S, k): # [5pts]\n",
    "        \"\"\"\n",
    "        Compute the proportion of the variance in the original matrix recovered by a rank-k approximation\n",
    "\n",
    "        Args:\n",
    "           S: min(N, D) x 1 (min(N, D) x 3 for color images) of singular values for the image\n",
    "           k: int, rank of approximation\n",
    "        Return:\n",
    "           recovered_var: int (array of 3 ints for color image) corresponding to proportion of recovered variance\n",
    "        \"\"\"\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8LRdI_NdUr0q",
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "### 2.2 Black and white [5 pts] <span style=\"color:green\">**[W]**</span>\n",
    "Use your implementation to generate a set of images compressed to different degrees. Include the images in your non-programming submission to the assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-22T17:18:45.705973Z",
     "start_time": "2019-06-22T17:18:40.298098Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 985
    },
    "colab_type": "code",
    "id": "broYG5UuTtey",
    "outputId": "a91852f9-eeed-4367-969a-b28d36bdcd2f",
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#helper do not need to change\n",
    "imcompression = ImgCompression()\n",
    "bw_image = rgb2gray(image)\n",
    "U, S, V = imcompression.svd(bw_image)\n",
    "component_num = [1,2,5,10,20,40,80,160,256]\n",
    "\n",
    "fig = plt.figure(figsize=(18, 18))\n",
    "\n",
    "# plot several images\n",
    "i=0\n",
    "for k in component_num:\n",
    "    img_rebuild = imcompression.rebuild_svd(U, S, V, k)\n",
    "    c = np.around(imcompression.compression_ratio(bw_image, k), 4)\n",
    "    r = np.around(imcompression.recovered_variance_proportion(S, k), 3)\n",
    "    ax = fig.add_subplot(3, 3, i + 1, xticks=[], yticks=[])\n",
    "    ax.imshow(img_rebuild, cmap=plt.cm.bone)\n",
    "    ax.set_title(f\"{k} Components\")\n",
    "    ax.set_xlabel(f\"Compression: {c},\\nRecovered Variance: {r}\")\n",
    "    i = i+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Color image [5 pts] <span style=\"color:green\">**[W]**</span>\n",
    "Use your implementation to generate a set of images compressed to different degrees. Include the images in your non-programming submission to the assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-22T17:19:05.459426Z",
     "start_time": "2019-06-22T17:18:45.710417Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "67zNhKjfUr0t",
    "outputId": "41d3c974-3ee6-4591-f9a2-a5fecf599ea2",
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "#helper do not need to change\n",
    "imcompression = ImgCompression()\n",
    "U, S, V = imcompression.svd(image)\n",
    "component_num = [1,2,5,10,20,40,80,160,256]\n",
    "\n",
    "fig = plt.figure(figsize=(18, 18))\n",
    "\n",
    "# plot several images\n",
    "i=0\n",
    "for k in component_num:\n",
    "    img_rebuild = imcompression.rebuild_svd(U, S, V, k)\n",
    "    c = np.around(imcompression.compression_ratio(image, k), 4)\n",
    "    r = np.around(imcompression.recovered_variance_proportion(S, k), 3)\n",
    "    ax = fig.add_subplot(3, 3, i + 1, xticks=[], yticks=[])\n",
    "    ax.imshow(img_rebuild)\n",
    "    ax.set_title(f\"{k} Components\")\n",
    "    ax.set_xlabel(f\"Compression: {np.around(c,4)},\\nRecovered Variance:  R: {r[0]}  G: {r[1]}  B: {r[2]}\")\n",
    "    i = i+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nWExwLuZcaYI"
   },
   "source": [
    "## 3 Polynomial regression and regularization [Undergrad: 55 pts + 20 Bonus pts | Grad: 75pts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8Em_UwQmelrt"
   },
   "source": [
    "### 3.1 Regression and regularization implementations <span style=\"color:blue\">**[P]**</span>\n",
    "We have three methods to fit linear and ridge regression models: 1) close form; 2) gradient descent (GD); 3) Stochastic gradient descent (SGD). For undergraduate students, you are required to implement the closed form for linear regression and for ridge regression, the others 4 methods are bonus parts. For graduate students, you are required to implement all of them. We use the term weight in the following code. Weights and parameters ($\\theta$) have the same meaning here. We used parameters ($\\theta$) in the lecture slides."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9p68Xgn0fA8W"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Regression(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def rmse(self, pred, label): # [5pts]\n",
    "        '''\n",
    "        This is the root mean square error.\n",
    "        Args:\n",
    "            pred: numpy array of length N x 1, the prediction of labels\n",
    "            label: numpy array of length N x 1, the ground truth of labels\n",
    "        Return:\n",
    "            a float value\n",
    "        '''\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def construct_polynomial_feats(self, x, degree): # [5pts]\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: numpy array of length N, the 1-D observations\n",
    "            degree: the max polynomial degree\n",
    "        Return:\n",
    "            feat: numpy array of shape Nx(degree+1), remember to include \n",
    "            the bias term. feat is in the format of:\n",
    "            [[1.0, x1, x1^2, x1^3, ....,],\n",
    "             [1.0, x2, x2^2, x2^3, ....,],\n",
    "             ......\n",
    "            ]\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "    def predict(self, xtest, weight): # [5pts]\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            xtest: NxD numpy array, where N is number \n",
    "                   of instances and D is the dimensionality of each \n",
    "                   instance\n",
    "            weight: Dx1 numpy array, the weights of linear regression model\n",
    "        Return:\n",
    "            prediction: Nx1 numpy array, the predicted labels\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    # =================\n",
    "    # LINEAR REGRESSION\n",
    "    # Hints: in the fit function, use close form solution of the linear regression to get weights. \n",
    "    # For inverse, you can use numpy linear algebra function  \n",
    "    # For the predict, you need to use linear combination of data points and their weights (y = theta0*1+theta1*X1+...)\n",
    "\n",
    "    def linear_fit_closed(self, xtrain, ytrain): # [5pts]\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            xtrain: N x D numpy array, where N is number of instances and D is the dimensionality of each instance\n",
    "            ytrain: N x 1 numpy array, the true labels\n",
    "        Return:\n",
    "            weight: Dx1 numpy array, the weights of linear regression model\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def linear_fit_GD(self, xtrain, ytrain, epochs=5, learning_rate=0.001): # [5pts]\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            xtrain: NxD numpy array, where N is number \n",
    "                    of instances and D is the dimensionality of each \n",
    "                    instance\n",
    "            ytrain: Nx1 numpy array, the true labels\n",
    "        Return:\n",
    "            weight: Dx1 numpy array, the weights of linear regression model\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def linear_fit_SGD(self, xtrain, ytrain, epochs=100, learning_rate=0.001): # [5pts]\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            xtrain: NxD numpy array, where N is number \n",
    "                    of instances and D is the dimensionality of each \n",
    "                    instance\n",
    "            ytrain: Nx1 numpy array, the true labels\n",
    "        Return:\n",
    "            weight: Dx1 numpy array, the weights of linear regression model\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    # =================\n",
    "    # RIDGE REGRESSION\n",
    "        \n",
    "    def ridge_fit_closed(self, xtrain, ytrain, c_lambda): # [5pts]\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            xtrain: N x D numpy array, where N is number of instances and D is the dimensionality of each instance\n",
    "            ytrain: N x 1 numpy array, the true labels\n",
    "            c_lambda: floating number\n",
    "        Return:\n",
    "            weight: Dx1 numpy array, the weights of ridge regression model\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "        \n",
    "    def ridge_fit_GD(self, xtrain, ytrain, c_lambda, epochs=500, learning_rate=1e-7): # [5pts]\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            xtrain: NxD numpy array, where N is number \n",
    "                    of instances and D is the dimensionality of each \n",
    "                    instance\n",
    "            ytrain: Nx1 numpy array, the true labels\n",
    "            c_lambda: floating number\n",
    "        Return:\n",
    "            weight: Dx1 numpy array, the weights of linear regression model\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def ridge_fit_SGD(self, xtrain, ytrain, c_lambda, epochs=100, learning_rate=0.001): # [5pts]\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            xtrain: NxD numpy array, where N is number \n",
    "                    of instances and D is the dimensionality of each \n",
    "                    instance\n",
    "            ytrain: Nx1 numpy array, the true labels\n",
    "        Return:\n",
    "            weight: Dx1 numpy array, the weights of linear regression model\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "    \n",
    "\n",
    "    def ridge_cross_validation(self, X, y, kfold=10, c_lambda=100): # [8 pts]\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            X: NxD numpy array, where N is number \n",
    "                    of instances and D is the dimensionality of each \n",
    "                    instance\n",
    "            y: Nx1 numpy array, the true labels\n",
    "            kfold: integer, size of the fold for the data split\n",
    "            c_lambda: floating number\n",
    "        Return:\n",
    "            mean_error: the mean of the RMSE for each fold\n",
    "        \"\"\"\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dBStSX_8f8mE"
   },
   "source": [
    "### 3.2 About RMSE [5pts] <span style=\"color:green\">**[W]**</span>\n",
    "Do you know whether this RMSE is good or not? If you don't know, we could normalize our labels between 0 and 1. After normalization, what does it mean when RMSE = 1? \n",
    "\n",
    "**Hint**: think of the way that you can enforce your RMSE = 1. Note that you can not change the actual labels to make RMSE = 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CMIH8qA-o4Dx"
   },
   "source": [
    "### 3.3 Testing: general functions and linear regression [5 pts] <span style=\"color:green\">**[W]**</span>\n",
    "\n",
    "Let's first construct a dataset for polynomial regression.\n",
    "\n",
    "In this case, we construct the polynomial features up to degree 5, where the groundtruth function is just a linear function (i.e., only require polynomial features up to degree 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#helper do not need to change\n",
    "def plot_curve(x, y, curve_type='.', color='b', lw=2):\n",
    "    plt.plot(x, y, curve_type, color=color, linewidth=lw)\n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel('y')\n",
    "    plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XvtxaQiro8gp"
   },
   "outputs": [],
   "source": [
    "#helper, do not need to change\n",
    "POLY_DEGREE = 5\n",
    "NUM_OBS = 1000\n",
    "\n",
    "rng = np.random.RandomState(seed=4)\n",
    "\n",
    "true_weight = -rng.rand(POLY_DEGREE + 1, 1)\n",
    "true_weight[2:, :] = 0\n",
    "x_all = np.linspace(-5, 5, NUM_OBS)\n",
    "reg = Regression()\n",
    "x_all_feat = reg.construct_polynomial_feats(x_all, POLY_DEGREE)\n",
    "y_all = np.dot(x_all_feat, true_weight) + rng.randn(x_all_feat.shape[0], 1) # in the second term, we add noise to data\n",
    "# Note that here we try to produce y_all as our training data\n",
    "plot_curve(x_all, y_all) # Data with noise that we are going to predict\n",
    "plot_curve(x_all, np.dot(x_all_feat, true_weight), curve_type='-', color='r', lw=4) # the groundtruth information\n",
    "\n",
    "indices = rng.permutation(NUM_OBS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nM8mZQTjo_6G"
   },
   "source": [
    "In the figure above, the red curve is the true fuction we want to learn, while the blue dots are the noisy observations. The observations are generated by  $Y=X\\theta+σ$ , where  σ∼N(0,1)  are i.i.d. generated noise.\n",
    "\n",
    "Now let's split the data into two parts, namely the training set and test set. The red dots are for training, while the blue dots are for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tp8DdM0ao-1m"
   },
   "outputs": [],
   "source": [
    "train_indices = indices[:NUM_OBS//2]\n",
    "test_indices = indices[NUM_OBS//2:]\n",
    "\n",
    "plot_curve(x_all[train_indices], y_all[train_indices], color='r')\n",
    "plot_curve(x_all[test_indices], y_all[test_indices], color='b')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GS2LrQNgpH-2"
   },
   "source": [
    "Now let's first train using the entire training set, and see how we performs on the test set and how the learned function look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pUj3xvMvpJxH"
   },
   "outputs": [],
   "source": [
    "#helper, do not need to change\n",
    "weight = reg.linear_fit_closed(x_all_feat[train_indices], y_all[train_indices])\n",
    "y_test_pred = reg.predict(x_all_feat[test_indices], weight)\n",
    "test_rmse = reg.rmse(y_test_pred, y_all[test_indices])\n",
    "print('test rmse: %.4f' % test_rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "h8Qi59G7pMC4"
   },
   "outputs": [],
   "source": [
    "weight = reg.linear_fit_GD(x_all_feat[train_indices], y_all[train_indices], epochs=5000, learning_rate=1e-7)\n",
    "y_test_pred = reg.predict(x_all_feat[test_indices], weight)\n",
    "test_rmse = reg.rmse(y_test_pred, y_all[test_indices])\n",
    "print('test rmse: %.4f' % test_rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VAnWJW8ZpS_F"
   },
   "source": [
    "And what if we just use the first 10 observations to train? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NNVl8HfwpU6n"
   },
   "outputs": [],
   "source": [
    "sub_train = train_indices[:10]\n",
    "weight = reg.linear_fit_closed(x_all_feat[sub_train], y_all[sub_train])\n",
    "y_test_pred = reg.predict(x_all_feat[test_indices], weight)\n",
    "test_rmse = reg.rmse(y_test_pred, y_all[test_indices])\n",
    "print('test rmse: %.4f' % test_rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dUWutkjgpYPN"
   },
   "source": [
    "Did you see a worse performance? Let's take a closer look at what we have learned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_tX0xGt1HFXc"
   },
   "outputs": [],
   "source": [
    "#helper, do not need to change\n",
    "y_pred = reg.predict(x_all_feat, weight)\n",
    "plot_curve(x_all, y_pred, curve_type='-', color='b', lw=4)\n",
    "plt.scatter(x_all[sub_train], y_all[sub_train], s=100, c='r', marker='x')\n",
    "\n",
    "y_test_pred = reg.predict(x_all_feat[test_indices], weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Qclyu8ZLHMpM"
   },
   "source": [
    "### 3.4 Testing: ridge regression [5 pts]\n",
    "\n",
    "Now let's try ridge regression. Similarly, undergraduate students need to implement the closed form, and graduate students need to implement all the three methods. We will call the prediction function from linear regression part."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xXwNWhnPHSZJ"
   },
   "source": [
    "Again, let's see what we have learned. You only need to run the cell corresponding to your specific implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Krvj9UaNHU0I"
   },
   "outputs": [],
   "source": [
    "sub_train = train_indices[:10]\n",
    "print(x_all_feat[sub_train].shape)\n",
    "print(y_all[sub_train].shape)\n",
    "weight = reg.ridge_fit_closed(x_all_feat[sub_train], y_all[sub_train], c_lambda=1000)\n",
    "\n",
    "y_pred = reg.predict(x_all_feat, weight)\n",
    "plot_curve(x_all, y_pred)\n",
    "plt.scatter(x_all[sub_train], y_all[sub_train], s=100, c='r', marker='x')\n",
    "\n",
    "y_test_pred = reg.predict(x_all_feat[test_indices], weight)\n",
    "test_rmse = reg.rmse(y_test_pred, y_all[test_indices])\n",
    "print('test rmse: %.4f' % test_rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8H1u-0RRHXkX"
   },
   "outputs": [],
   "source": [
    "sub_train = train_indices[:10]\n",
    "weight = reg.ridge_fit_GD(x_all_feat[sub_train], y_all[sub_train], c_lambda=1000, learning_rate=1e-7)\n",
    "\n",
    "y_pred = reg.predict(x_all_feat, weight)\n",
    "plot_curve(x_all, y_pred)\n",
    "plt.scatter(x_all[sub_train], y_all[sub_train], s=100, c='r', marker='x')\n",
    "\n",
    "y_test_pred = reg.predict(x_all_feat[test_indices], weight)\n",
    "test_rmse = reg.rmse(y_test_pred, y_all[test_indices])\n",
    "print('test rmse: %.4f' % test_rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SLY0_dU1HZfn"
   },
   "outputs": [],
   "source": [
    "sub_train = train_indices[:10]\n",
    "weight = reg.ridge_fit_SGD(x_all_feat[sub_train], y_all[sub_train], c_lambda=1000, learning_rate=1e-7)\n",
    "\n",
    "y_pred = reg.predict(x_all_feat, weight)\n",
    "plot_curve(x_all, y_pred)\n",
    "plt.scatter(x_all[sub_train], y_all[sub_train], s=100, c='r', marker='x')\n",
    "\n",
    "y_test_pred = reg.predict(x_all_feat[test_indices], weight)\n",
    "test_rmse = reg.rmse(y_test_pred, y_all[test_indices])\n",
    "print('test rmse: %.4f' % test_rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "B6gwxJkVHdB8"
   },
   "source": [
    "### 3.5 Cross validation [7 pts] <span style=\"color:green\">**[W]**</span>\n",
    "Let's use Cross Validation to find the best value for c_lambda in ridge regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nA1aW31XHRXj"
   },
   "outputs": [],
   "source": [
    "# We provided 6 possible values for lambda, and you will use them in cross validation.\n",
    "# For cross validation, use 10-fold method and only use it for your training data (you already have the train_indices to get training data).\n",
    "# For the training data, split them in 10 folds which means that use 10 percent of training data for test and 90 percent for training.\n",
    "# At the end for each lambda, you have caluclated 10 rmse and get the mean value of that.\n",
    "# That's it. Pick up the lambda with the lowest mean value of rmse. \n",
    "# Hint: np.concatenate is your friend.\n",
    "best_lambda = None\n",
    "best_error = None\n",
    "kfold = 10\n",
    "lambda_list = [0, 0.1, 1, 5, 10, 100, 1000]\n",
    "for lm in lambda_list:\n",
    "    err = reg.ridge_cross_validation(x_all_feat[train_indices], y_all[train_indices], kfold, lm)\n",
    "    print('lambda: %.2f' % lm, 'error: %.6f'% err)\n",
    "    if best_error is None or err < best_error:\n",
    "        best_error = err\n",
    "        best_lambda = lm\n",
    "\n",
    "print('best_lambda: %.2f' % best_lambda)\n",
    "weight = reg.ridge_fit_closed(x_all_feat[train_indices], y_all[train_indices], c_lambda=10)\n",
    "y_test_pred = reg.predict(x_all_feat[test_indices], weight)\n",
    "test_rmse = reg.rmse(y_test_pred, y_all[test_indices])\n",
    "print('test rmse: %.4f' % test_rmse)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rfzZUT1sHy6h"
   },
   "source": [
    "## 4 Understanding PCA [20 pts] <span style=\"color:green\">**[W]**</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kEPFAlz7HzMY"
   },
   "source": [
    "In this problem, we will investigate how PCA can be used to improve features for regression and classification tasks and how the data itself affects the behavior of PCA. Implement PCA in the below cell. Note that you will not submit your code for this problem to the autograder.\n",
    "\n",
    "### 4.1 Intrinsic Dimensionality [10 pts] <span style=\"color:green\">**[W]**</span>\n",
    "Assume a dataset is composed of N datapoints, each of which has D features with D < N.  The *dimension* of our data would be $D$. It is possible, however, that many of these dimensions contain redundant information.  The *intrinsic dimensionality* is the number of dimensions we need to reconstruct our data with high fidelity.  For our purposes, we will define the intrinsic dimension as the number of principal components needed to reconstruct 99% of the variation within our data.  \n",
    "\n",
    "We define a set of features as linearly independent if we cannot construct one out of a linear combination of the others. The number of linearly independent features is the number of nonzero principal components (where we define 0 is anything less than $10^{-11}$ due to floating point error). Zero principal components mean that we can not find any weights to linearly combine features in order to create an indenpendent feature. Thus, our algorithm will assign 0 to these weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4DSSKufTH3G9"
   },
   "outputs": [],
   "source": [
    "def pca(X):\n",
    "    \"\"\"\n",
    "    Decompose dataset into principal components. \n",
    "    You may use your SVD function from the previous part in your implementation.\n",
    "\n",
    "    Args: \n",
    "        X: N x D array corresponding to a dataset, in which N is the number of points and D is the number of features\n",
    "    Return:\n",
    "        U: N x N \n",
    "        S: min(N, D) x 1 \n",
    "        V: D x D\n",
    "    \"\"\"\n",
    "    raise NotImplementedError\n",
    "\n",
    "def intrinsic_dimension(S, recovered_variance=.99):\n",
    "    \"\"\"\n",
    "    Find the number of principal components necessary to recover given proportion of variance\n",
    "\n",
    "    Args: \n",
    "        S: 1-d array corresponding to the singular values of a dataset\n",
    "\n",
    "        recovered_varaiance: float in [0,1].  Minimum amount of variance \n",
    "            to recover from given principal components\n",
    "    Return:\n",
    "        dim: int, the number of principal components necessary to recover \n",
    "            the given proportion of the variance\n",
    "    \"\"\"\n",
    "    raise NotImplementedError\n",
    "\n",
    "def num_linearly_ind_features(S, eps=1e-11):\n",
    "    \"\"\"\n",
    "    Find the number of linearly independent features in dataset\n",
    "\n",
    "    Args: \n",
    "        S: 1-d array corresponding to the singular values of a dataset\n",
    "    Return:\n",
    "        dim: int, the number of linearly independent dimensions in our data\n",
    "    \"\"\"\n",
    "    raise NotImplementedError\n",
    "\n",
    "def apply_PCA(X, retained_variance=0.99):\n",
    "    \"\"\"\n",
    "    Apply the functions you just implemented\n",
    "    Args: \n",
    "        X: N x D array corresponding to a dataset, in which N is the number of points and D is the number of features\n",
    "        retained variance: floating number\n",
    "    Return:\n",
    "        (X_new, num_linearly_ind_features, intrinsic_dimension): The X projection on the new feature space, the number of linearly independent dimensions in our data, the intrinsic dimension\n",
    "    \"\"\"\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BufvP3BzH5Ti"
   },
   "source": [
    "PCA is used to transform multivariate data tables into smaller sets so as to observe the hidden trends and variations in the data. Here you will visualize two datasets (iris and wine) using PCA. Use the above implementation of PCA and reduce the datasets such that they contain only two features. Make 2-D scatter plots of the data points using these features. Make sure to differentiate the data points according to their true labels. The datasets have already been loaded for you. In addition, return the retained variance obtained from the reduced features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GRilYfoeH7as"
   },
   "outputs": [],
   "source": [
    "def visualize(X,y):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "    xtrain: NxD numpy array, where N is number \n",
    "          of instances and D is the dimensionality of each \n",
    "          instance\n",
    "    ytrain: numpy array (N,), the true labels\n",
    "  \n",
    "    Return:\n",
    "    retained variance: scalar\n",
    "    \n",
    "    Plot:\n",
    "    scatter plot with the datapoints projected onto new feature space\n",
    "    \"\"\"\n",
    "    raise NotImplementedError\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "30eydfzLH-g0"
   },
   "outputs": [],
   "source": [
    "# use PCA for visualization of iris and wine data\n",
    "data=load_iris(return_X_y=True)\n",
    "X=data[0]\n",
    "y=data[1]\n",
    "print(y.shape)\n",
    "retained_variance_for_iris=visualize(X,y)\n",
    "print(\"Retained variance for iris dataset \",retained_variance_for_iris)\n",
    "\n",
    "\n",
    "data=load_wine(return_X_y=True)\n",
    "X=data[0]\n",
    "y=data[1]\n",
    "retained_variance_for_wine=visualize(X,y)\n",
    "print(\"Retained variance for wine dataset \",retained_variance_for_wine)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mqt4ISUDIAVt"
   },
   "source": [
    "Now you will use PCA on an actual real-world dataset. Implement feature reduction on the dataset using PCA with 99% retained variance. Use it to obtain the reduced features. On the reduced dataset, use linear regression and calculate the rmse values on test data. Your are encouraged to experiment with hyperparameters like the learning rate, number of epochs and regularization strength. Compare these results with those obtained by implementing regression on the unreduced dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mZPUKNJ8LGCB"
   },
   "outputs": [],
   "source": [
    "# helper\n",
    "def apply_regression(X_train,y_train,X_test):\n",
    "    weight = reg.ridge_fit_closed(X_train, y_train, c_lambda=1600)\n",
    "    y_pred = reg.predict(X_test, weight)\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "urfrl0YGLH0C"
   },
   "outputs": [],
   "source": [
    "#load the dataset \n",
    "data=np.load(\"data.npy\",allow_pickle=True)\n",
    "# separate the data\n",
    "X=data[:,:-1]\n",
    "y=data[:,-1]\n",
    "X_PCA, ind_features, intrinsic_dimensions = apply_PCA(X)\n",
    "print(\"data shape with PCA \",X_PCA.shape)\n",
    "print(\"Number of independent features \",ind_features)\n",
    "print(\"Number of intrinsic components \",intrinsic_dimensions)\n",
    "\n",
    "#get training and testing data \n",
    "X_train=X_PCA[:int(0.8*len(data)),:]\n",
    "y_train=y[:int(0.8*len(data))].reshape(-1,1)\n",
    "X_test=X_PCA[int(0.8*len(data)):]\n",
    "y_test=y[int(0.8*len(data)):].reshape(-1,1)\n",
    "\n",
    "# use Ridge Regression for getting predicted labels\n",
    "y_pred=apply_regression(X_train,y_train,X_test)\n",
    "\n",
    "#calculate RMSE \n",
    "rmse_score = reg.rmse(y_pred, y_test)\n",
    "print(\"rmse score with PCA\",rmse_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hgqaj5t2LKQW"
   },
   "outputs": [],
   "source": [
    "# rRidge regression without PCA\n",
    "X_train=X[:int(0.8*len(data)),:]\n",
    "y_train=y[:int(0.8*len(data))].reshape(-1,1)\n",
    "X_test=X[int(0.8*len(data)):]\n",
    "y_test=y[int(0.8*len(data)):].reshape(-1,1)\n",
    "\n",
    "#use Ridge Regression for getting predicted labels\n",
    "y_pred=apply_regression(X_train,y_train,X_test)\n",
    "\n",
    "#calculate RMSE \n",
    "rmse_score = reg.rmse(y_pred, y_test)\n",
    "print(\"rmse score without PCA\",rmse_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "k_3IWpnBLQgq"
   },
   "source": [
    "### 4.2 Feature Scaling [10 pts] <span style=\"color:green\">**[W]**</span>\n",
    "\n",
    "Principal component analysis is not agnostic to the scale of your features.  Measuring a feature with different units can change your principal components.\n",
    "\n",
    "For this problem, randomly choose one column in each of the above datasets and multiply it by 1000.  For each of the datasets, answer the following:\n",
    "1. How does this change the distribution of variance among the first 10 components?  \n",
    "2. How does this change the first principal component of the data?\n",
    "3. How does this affect the number of linearly independent components and intrinsic dimensionality?  Why?\n",
    "\n",
    "It may be helpful to plot the variance captured by each component in a scree plot (see function below) and to make a bar plot of the absolute value of each feature in the first principal component. Hint: Exploring the imported datasets (i.e. Boston, Diabetes, Digits, etc.) may help you understand the answers to these questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FsyQC7CsLUQs"
   },
   "outputs": [],
   "source": [
    "# helper fuctions\n",
    "def randomly_perturb_data(data, multiplier=1000):\n",
    "    \"\"\"\n",
    "    Multiply a random column in data by multiplier\n",
    "    \n",
    "    Inputs:\n",
    "        data: N*D numpy array of features\n",
    "        multiplier: multiplier by which to perturb a random column in data\n",
    "        \n",
    "    Returns:\n",
    "        perturbed_data: Data with random column multiplied by multiplier\n",
    "    \"\"\"\n",
    "    i = np.random.randint(data.shape[1])\n",
    "    copy = data.copy()\n",
    "    copy[:,i] *= multiplier\n",
    "    return copy\n",
    "   \n",
    "    \n",
    "def scree_plot(S, n_components=10):\n",
    "    \"\"\"\n",
    "    Plot proportion of variance contained in each individual component\n",
    "    \"\"\"\n",
    "    plt.plot(range(1, n_components+1), (S/S.sum())[:n_components])\n",
    "    plt.ylabel(\"Proportion of Variance\")\n",
    "    plt.xlabel(\"Component Number\")\n",
    "    plt.title(\"PCA Scree Plot\")\n",
    "    plt.show()\n",
    "    \n",
    "def plot_component_vector(V):\n",
    "    v = V[:,0]\n",
    "    plt.bar(range(1, len(v)+1), np.abs(v))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RRC0Z7uoLWRT"
   },
   "source": [
    "<b>Write your analysis here</b>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wWO9JhXJLa7w"
   },
   "source": [
    "## 5 Manifold learning [Bonus for everyone][30 pts]\n",
    "\n",
    "While PCA is wonderful tool for dimensionality reduction it does not work very well when dealing with non-linear relationships between features. Manifold learning is a class of algorithms that can be used to reduce dimensions in complex high-dimensional datasets. While a number of methods have been proposed to perform this type of operation, we will focus on Isomap. Isomap has been shown to be sensitive to data noise amongst other issues, however it has been shown to perform reasonably well for real world data. The algorithm consists of two main steps: first computing a manifold distance matrix, followed by classical mutidimensional scaling. You will be creating your implementation of Isomap. In order to do so, you must read the original paper [\"A Global Geometric Framework for Nonlinear Dimensionality Reduction\"](http://web.mit.edu/cocosci/Papers/sci_reprint.pdf) by Tenenbaum et al. (2000), which outlines the method. You are also encouraged to read this [general survey of manifold learning](https://cseweb.ucsd.edu/~lcayton/resexam.pdf) by Cayton (2005), where the original algorithm is further explained in a more detailed yet simplified fashion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Implementation [23 pts] <span style=\"color:blue\">**[P]**</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "F2ReXSB-LfES"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "from scipy.sparse.csgraph import floyd_warshall\n",
    "\n",
    "class Isomap(object):\n",
    "    def __init__(self): # No need to implement\n",
    "        pass\n",
    "    \n",
    "    def pairwise_dist(self, x, y): # [3 pts]\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: N x D numpy array\n",
    "            y: M x D numpy array\n",
    "        Return:\n",
    "                dist: N x M array, where dist2[i, j] is the euclidean distance between \n",
    "                x[i, :] and y[j, :]\n",
    "        \"\"\"\n",
    "    \n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def manifold_distance_matrix(self, x, K): # [10 pts]\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: N x D numpy array\n",
    "        Return:\n",
    "            dist_matrix: N x N numpy array, where dist_matrix[i, j] is the euclidean distance between points if j is in the neighborhood N(i)\n",
    "            or comp_adj = shortest path distance if j is not in the neighborhood N(i).\n",
    "        Hint: After creating your k-nearest weighted neighbors adjacency matrix, you can convert it to a sparse graph\n",
    "        object csr_matrix (https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.csr_matrix.html) and utilize\n",
    "        the pre-built Floyd-Warshall algorithm (https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.csgraph.floyd_warshall.html)\n",
    "        to compute the manifold distance matrix.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    \n",
    "    def multidimensional_scaling(self, dist_matrix, d): # [10 pts]\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            dist_matrix: N x N numpy array, the manifold distance matrix\n",
    "            d: integer, size of the new reduced feature space \n",
    "        Return:\n",
    "            S: N x d numpy array, X embedding into new feature space.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    # you do not need to change this\n",
    "    def __call__(self, data, K, d):\n",
    "        # get the manifold distance matrix\n",
    "        W = self.manifold_distance_matrix(data, K)\n",
    "        # compute the multidimensional scaling embedding\n",
    "        emb_X = self.multidimensional_scaling(W, d)\n",
    "        return emb_X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Examples for different datasets [7pts] <span style=\"color:green\">**[W]**</span>\n",
    "Apply your implementation of Isomap for some of the datasets (e.g. MNIST and Iris). Discuss how the new embedding compares to PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Rvd5VIzWTXCk"
   },
   "outputs": [],
   "source": [
    "# example MNIST data\n",
    "mnist = load_digits()\n",
    "proj = Isomap()(mnist.data, 10, 2)\n",
    "plt.scatter(proj[:, 0], proj[:, 1], c=mnist.target, cmap=plt.cm.get_cmap('jet', 10))\n",
    "plt.colorbar(ticks=range(10))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "HW3_complete_summer20.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
